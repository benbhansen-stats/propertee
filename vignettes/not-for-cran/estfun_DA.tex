\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}%for \llbracket
\usepackage{mathtools}% for DeclarePairedDelimiter
\usepackage{soul}
\usepackage{colonequals}
\newcommand{\defeq}{\ensuremath{\colonequals}}
\newcommand{\eqdef}{\ensuremath{\equalscolon}}

\newcommand{\EE}{\operatorname{E}}
\DeclarePairedDelimiter{\indicator}{\llbracket}{\rrbracket}
\newcommand{\owt}[1][{[a_i]}]{\ensuremath{\check{w}_{i#1}}}
\newcommand{\absorbInterceptsEF}{\upsilon}
\newcommand{\AbsorbInterceptsEF}{\Upsilon}
\newcommand{\absorbModeratorEF}{\grave{\upsilon}}
\newcommand{\AbsorbModeratorEF}{\grave{\Upsilon}}

\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{plain}

\usepackage{hyperref}


\author{BBH}
\title{\texttt{sandwich::estfun()} method for DA objects}
\begin{document}
\nobibliography*
\maketitle
\section{Separate and combined estimating functions for covariance and comparison fits}

\subsection{Setting}

Let $\mathcal{C}$ and $\mathcal{Q}$ denote the covariance and
quasiexperimental samples, respectively, expressed at the level of
unit of assignment: if assignment is at the level of the classroom,
then these are collections of classes not collections of students, nor
of student-year observations.  In turn, identify each unit of assignment
with the set of elemental observations they contain: a classroom is
identified with the 
set of students, or student-years, associated with that classroom;
if there is no clustering, then we think of $\mathcal{C}$ and
$\mathcal{Q}$ as consisting of singleton sets, each containing a
single element.  (Additional structure: any clusters $c, c' \in \mathcal{C}
\cup \mathcal{Q}$ that are distinct, $c  \neq c'$, are nonoverlapping, $c \cap c' = \emptyset$.) This permits us to write $\mathcal{Q}$ for the
quasiexperimental sample as expressed in terms of clusters, and $\bigcup
\mathcal{Q}$ for the collection of all elements belonging to the
quasiexperimental sample.

There are random estimating functions $\Phi(\mathbf{Z}; \beta ) =
[\phi(\vec{Z}_{i};
\beta) : i]'$, an $n$ (number of elements) by $p$ matrix,  and 
       $\grave{\Psi}(\mathbf{Z}; \rho, \beta) = [\psi(\vec{Z}_{i};
       \rho, \beta): i]'$, \textit{also} an $n$ by $k$ matrix, with 
       $\phi(\vec{Z}_{i}; \beta)$ equal to the product of the covariance
       regression estimating functions with $\indicator{i \in \bigcup \mathcal{C}}$
       and $\grave{\Psi}(\cdot; \cdot)$ the estimating functions defining the comparison regression.  That is, for now:
\begin{equation}
         \label{eq:5}
         \grave{\psi}(\vec{Z}_{i}; \rho, \beta) =\\
         \left( \begin{array}{c}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[0]}] [Y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \rho_{0}]\indicator{A_{i}=0}\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[1]}] [Y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \rho_{1}]\indicator{A_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[k]}] [Y_{i}[k] - g(\vec{x}_{i}\beta)-
                  \rho_{k}]\indicator{A_{i}=k}\\
                \end{array}\right)
\end{equation}
where
\begin{itemize}
\item $Y_{i}[j]$ represents $i$'s potential outcome following
  assignment to condition $i$;
\item $A_{i}$ is the treatment assignment of $i$'s cluster $[i]$;
\item $\vec{Z}_{i} = (Y_{i}[0],\ldots, Y_{i}[k], \vec{x}_{i}, m_{i}, w_{i}, A_{i})$, where $\vec{x}_{i}$, $m_{i}$ and $w_{i}$ are covariates, an optional moderator variable and a
  frequency or importance weight associating with $i$ (1 if no such weight was
  given); 
\item $\owt[{[j]}]$ is either $w_i$ or, 
   if requested by the user, $w_{i}$'s product with an
  odds- or inverse probability-based weight attaching to $i$'s
  assignment unit $[i]$ under assignment $j$.
\end{itemize}
(I say ``for now'' because \eqref{eq:14} will present a
$\psi(\cdot)$ with equivalent solutions that is better aligned with
\texttt{lm()}'s default treatment effect parameterizations, and
\eqref{eq:9} in \S~\ref{sec:accomm-interc-via} below will generalize $\psi()$'s definition
to add block-specific intercepts.)
The interest parameter is $\tau \defeq \rho - \rho_{0}$. In the model-based interpretation, occurrences of ``$A_{i}$'' in \eqref{eq:5} may be replaced with ``$a_{i}$,'' while under design-based we can write ``$y_{i}[j]$'' instead of ``$Y_{i}[j]$''; see Section~\ref{sec:des-vs-mod-based} below.

\subsection{Goal}
We aim to define \texttt{estfun.DirectAdjusted()} associating
with a direct adjustment model a matrix of evaluated estimating function
contributions of form
\begin{equation*}
  \tilde{\Psi} = c_{2}\Psi - c_{1}\Phi A_{11}^{-1}A_{21}',
\end{equation*}
and a corresponding \texttt{bread.DirectAdjusted()} which together will enable \texttt{sandwich::vcov()} to provide valid standard errors. 
Note that this is $n$ by
$k+1$, $n$ being the size of the combined $\mathcal{C}$ and
$\mathcal{Q}$ sample, with columns corresponding to $\rho$.%
(Assuming we're estimating a single marginal treatment effect, as
opposed to moderation effect[s], either without blocking or with
neither explicit nor implicit block fixed effects; otherwise the
number of columns could be $k$\footnote{For a main effect
DA model fitted with absorption of the block intercept or
intercepts; see \S~\ref{sec:accomm-interc-via} below.} or something
larger than $k+1$\footnote{In the case of subgroup effects and/or unabsorbed block fixed effects.},
depending. To obviate the ambiguity we'll say
\begin{equation*}
  \tilde{k} \defeq \mathtt{ncol}(\tilde{\Psi}).)
\end{equation*}

Values of the constants $c_{1}$ and $c_{2}$ are to be determined.
I conjecture these decisions can be made in such away that 
\begin{enumerate}
\item Using this as a basis for a meat matrix while using
  $A_{22}^{-1}$  ``bread matrix'' lands us in the same place as the
  inversion formula as expressed with A's and B's on p.373 of Carroll
  et al. 2006.
\item Suppose we've got multiple DA models that have been fit over the
  same CA model, and we wish to compute joint covariances for their
  distinct parameters
  (\href{https://github.com/benbhansen-stats/propertee/issues/79}{\#79
    on GitHub}). We get appropriate joint bread matrices by
  diagonally combining bread matrices of the DA models, and we get
  appropriate meat matrices by cbind-ing the estfun's.
\item For ``natural'' choices of $c_{1}$ and $c_{2}$, in the
  presence of clustering the corresponding bread matrices are
  reasonable and interpretable, at least in cases of HC0 and HC1
  degree of freedom adjustments. 
\end{enumerate}

\subsection{$\psi(\vec{Z}_{i};
  \alpha, \tau, \beta)$ with intercept and $\tau$'s}
\label{sec:psit-rho_0-tau}

The estimating equation $\psi(\vec{z}; \rho, \beta)=0$,
$\psi(\cdot)$ as in \eqref{eq:5} above, has the same solutions as $\sum_{i}\psi(\vec{z}_{i};
  \alpha, \tau, \beta)=0$, with $\psi(\cdot; \cdot)$ as in \eqref{eq:14} immediately below.  Writing $\tau_{j}=\rho_{j}-\rho_{0}$ and introducing the intercept parameter ``$\alpha$,''  for the time being one-dimensional with $\alpha_{0}=\rho_{0}$, while abbreviating ``$\owt{}$'' as ``$\owt[]$,'' 
\begin{equation} \label{eq:14}
         \psi(\vec{z}_{i};
  \alpha, \tau, \beta) =
         \left( \begin{array}{l}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[] \big[y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{a_{i}=j}\big]\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[1]}] [y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{1}]\indicator{a_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[k]}] [y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_{0} - \tau_{k}]\indicator{a_{i}=k}\\
                \end{array}\right), 
\end{equation}
the omission of a ``$\indicator{a_{i}=0}$'' factor from the topmost
expression being intentional and deliberate. It doesn't affect the solution
$(\alpha_{0}, \tau_{1}, \ldots, \tau_{k})$ because this solution also satisfies
\begin{equation} \label{eq:16}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[][y_{i}[a_i] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{j}]\indicator{a_{i}=j}, \text{ each } j=1, \ldots, k;
\end{equation}
and therefore
\begin{equation*}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i}[a_i] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\sum_{j=1}^{k}\tau_{j}\indicator{a_{i}=j}\big]\indicator{a_{i}
                  \neq 0}.
\end{equation*}
The complement of condition $\indicator{a\neq 0}$ being
$\indicator{a=0}$, under which
$\sum_{j=1}^{k}\tau_{j}\indicator{a_{i}=j}=0$, it follows from \eqref{eq:16} that
\begin{align*}
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[{[0]}]\big[y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}\big]\indicator{a_{i}
             =0}\\
           & \Updownarrow \\
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i}[a_i] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{a_{i}=j}\big].
\end{align*}
One advantage of this representation is that it better represents what
\texttt{lm()} ordinarily does, when given a factor treatment
variable.  Another is that it isolates the intercept parameter, which
we have no use for and will be done away with via absorption (\S~\ref{sec:accomm-interc-via} below). 

\subsection{The covariance we're trying to
  produce}\label{sec:covar-were-trying}

Let $\hat\beta$, $\hat{\alpha}$ and $\hat\tau$ solve
\[\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{z}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\psi(\vec{z}_{j}; \tau,
\alpha, \beta )  =0
  \]
  in $\beta$ and $\tau$, whereas $\beta_{(n)}$, $\alpha_{(n)}$ and $\tau_{(n)}$ solve or nearly solve\footnote{%
In actuality we only need the estimator sequence to be consistent for the deterministic sequences $(\beta_{(n)}:n)$, $(\alpha_{(n)}:n)$ and $(\tau_{(n)}:n)$, i.e. $|\hat\beta - \beta_{(n)}|_{2}$, $|\hat\alpha - \alpha_{(n)}|_{2}$ and $|\hat\tau - \tau_{(n)}|_{2}$ are all $o_{P}(1)$.%
}
\[\sum_{i\in \bigcup \mathcal{C}}\EE\phi(\vec{Z}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\EE\psi(\vec{Z}_{j}; \tau,
\alpha, \beta )  =0.
  \]
In these expectations and those that follow, $\vec{x}_{i}$ and $w_{i}$
are taken as fixed but $A_{i}$ and/or potential outcomes $(Y_{i}[a]:
a=0, \ldots, k)$ may be random according as we follow a design- or
model-based framework; see \S~\ref{sec:des-vs-mod-based}.
  Write
  \begin{align*}
%    \label{eq:13}
    A(\alpha, \beta, \tau) &= \left(
      \begin{array}{cc}
        n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\EE [\nabla_{\beta}\phi(\vec{Z}_{i};
        \beta )]&0\\
        n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\beta}\psi(\vec{Z}_{j};
        \tau, \alpha, \beta )]  & n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\alpha, \tau}\psi(\vec{Z}_{j};
        \tau, \alpha, \beta )]
      \end{array}
    \right)\\
    &\eqdef \left(
      \begin{array}{cc}
        A_{11}(\beta)& 0\\
        A_{21}(\beta) & A_{22}(\alpha, \tau)
      \end{array}
\right),
  \end{align*}
where the structure of $\psi(\cdot)$ as given in \eqref{eq:14} ensures
that $A_{21}(\alpha, \beta, \tau)$ and $A_{22}(\alpha, \beta, \tau)$ are functions only of $\beta$ and of $(\alpha, \tau)$ respectively. Define   
  \begin{align*}
   \tilde{\beta}_{(n)}
    \defeq& \beta_{(n)} + A_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{Z}_{i}; \beta_{(n)})
  \quad \text{and}\\
    \left(\begin{array}{c}\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)
    \defeq& \left(\begin{array}{c}\alpha_{(n)}\\
                    \tau_{(n)} \end{array}\right)
    + A^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})\left(
    \begin{array}{c}
      n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{Z}_{i}; \beta_{(n)})\\
      n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{Z}_{j};
                 \tau_{(n)},\beta_{(n)} )
    \end{array}
\right)\\
    \\
               &=\left(\begin{array}{c}\alpha_{(n)}\\
                         \tau_{(n)} \end{array}\right) +
    A_{22}^{-1}(\alpha_{(n)}, \tau_{(n)})\times \\
    & \left[
                 n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{Z}_{j};
                 \tau_{(n)},\beta_{(n)} ) - A_{21}(\beta_{(n)}) A_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{Z}_{i}; \beta_{(n)})\right],
\end{align*}
known as \textit{linearizations} (of $\hat\beta$ and $(\hat\alpha, \hat\tau)$).


Observe that these expressions define the same $\tilde{\beta}_{(n)}$ and $\tilde{\tau}_{(n)}$
  irrespective of the choice of nonzero constants $n_{\mathcal{C}}$
  and $n$.  To promote alignment with existing \texttt{sandwich}
  functions%
  \footnote{%
    This $n_{\mathcal{C}}$ is the constant that \texttt{sandwich} functions
will select for the covariance model, given its sample size.  It's
incumbent on us to supply \texttt{sandwich::estfun()} and/or
\texttt{sandwich::bread()} methods for \texttt{DirectAdjusted} objects
with compatible implicit
scaling constants. \texttt{sandwich::bread()} and
\texttt{sandwich::breadCL()} take the scaling constant to be the
reciprocal of the number of rows in whatever
\texttt{sandwich::estfun()} returns; it will fall to us to ensure our
$\hat{A}_{21}$ and $\hat{A}_{22}$ observe compatible scalings.}%
, we set $n_{\mathcal{C}}$ and $n$ to be the number of elements
(not clusters) in the covariance and in the combined samples,
respectively.

Assume that: the linearization errors $|\hat\beta -\tilde{\beta}_{(n)}|_{2}$, 
$|\hat\tau -\tilde{\tau}_{(n)}|_{2}$ and $|\hat\alpha - \tilde{\alpha}_{(n)}|_{2}$ tend in probability to 0;
$A_{11}(\beta_{(n)})$ and $A_{22}(\beta_{(n)}, \tau_{(n)})$ are nonsingular
with eigenvalues bounded away from 0; and $A_{11}(\cdot)$, $A_{21}(\cdot)$ and $A_{22}(\cdot, \cdot)$ are suitably
regular\footnote{E.g, they admit of Lipschitz constants: see \S~9.8,
  \bibentry{keener2010TheorStats}, while bearing in mind issues raised
  by \bibentry{feng2014exact}.}. 
Then we have the covariance approximation characteristic of M-estimation,
\begin{align*} \operatorname{Cov}([\hat\beta',\hat\alpha', \hat\tau']) \approx&
  \operatorname{Cov}([\tilde{\beta}_{(n)}',\hat{\alpha}_{(n)}',\tilde{\tau}_{(n)}'])\\
  &=
    A^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)}) \times \\
  &\hspace{1.5em} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{Z}_{i}; \beta_{(n)} )\\
       n^{-1}\sum_{j=1}^{n}\psi(\vec{Z}_{j}; \tau_{(n)}, \alpha_{(n)}, \beta_{(n)} )
     \end{array}
    \right)
  A^{-t}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})
\end{align*}
(recalling that we have so defined $\psi(\cdot)$ that $\psi(\vec{Z}_{j}; \tau, \alpha, \beta) = 0$ when
$j\not\in \bigcup \mathcal{Q}$).


To help mark our nonstandard scaling constants, 
meat matrices will be denoted with the letter ``M,'' not ``B.''
We have
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{Z}_{i}; \beta )\\
       n^{-1}\sum_{j=1}^{n}\psi(\vec{Z}_{j}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-2}M_{11}& (n_{\mathcal{C}}n)^{-1}M_{12}\\
    (n_{\mathcal{C}}n)^{-1}M_{21} & n^{-2}M_{22}
  \end{array}
\right),
\]
where
\[
  \begin{array}{cc}
    M_{11}  = \operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\vec{Z}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  \operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\vec{Z}_{i};
                                                                  \beta
                                                                  ), \sum_{j}\psi'(\vec{Z}_{j}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = \operatorname{Cov}[\sum_{j}\psi(\vec{Z}_{j};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
Note that $n_{\mathcal{C}}^{-1}M_{11}$ is a natural meat matrix, while $n^{-1}M_{22}$'s and
$(n_{\mathcal{C}}n)^{-1}M_{12}$/$(n_{\mathcal{C}}n)^{-1}M_{21}$'s scaling constants --- $n^{-1}$ rather than $(\# \bigcup
\mathcal{Q})^{-1}$ (or just $(\# \mathcal{Q})^{-1}$) in $n^{-1}M_{22}$'s
case and $(n_{\mathcal{C}}n)^{-1}$ not $[n (\# \bigcup
\mathcal{Q})]^{-1}$ in $(n_{\mathcal{C}}n)^{-1}M_{12}$/$(n_{\mathcal{C}}n)^{-1}M_{21}$'s --- are progressively more weird.

With these definitions,
\begin{align}
  \operatorname{Cov}(\hat\tau) \approx& A_{22}^{-1}\{n^{-2} M_{22} -
                                 (n_{\mathcal{C}}n)^{-1}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-2}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}\nonumber
  \\
                               &= n^{-1}A_{22}^{-1}\{
                                 n^{-1}M_{22} -
                                 n_{\mathcal{C}}^{-1}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_{\mathcal{C}}^{2}}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
                                . \label{eq:6}
\end{align}
An advantage of the expression with leading factor $n^{-1}$ is that
that way it parallels the calculation of
\texttt{sandwich::sandwich()} and, I think, other \texttt{sandwich}
functions. (Assuming that we write
\texttt{estfun.DirectAdjusted()}  to return a matrix of row extent
$n$.) 

For model-based estimation with units of observation and of assignment being
the same, $M$'s are estimated as follows. 
\begin{equation}\label{eq:7}
  \begin{array}{cc}
  \hat{M}_{11} =\Phi'\Phi & \hat{M}_{12} = \Phi'\Psi\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= \Psi'\Psi\\
    \end{array} 
  \end{equation}
  In the more general case with potentially clustering of observation
units, by units of assigment and maybe more, we can express HC0
type estimators with matrix rather than summation notation by using
$F$ for the \{0,1\}-valued $n \times m$ matrix with an indicator
column for each cluster:
\begin{equation} \label{eq:8}
  \begin{array}{cc}
  \hat{M}_{11} =(F'\Phi)'(F'\Phi) & \hat{M}_{12} = (F'\Phi)'(F'\Psi)\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= (F'\Psi)'(F'\Psi)\\
    \end{array}
  \end{equation}

\subsection{Realization of goal}
Leaving the clusterwise aggregation to \texttt{sandwich}, as
appropriate, it seems that if we define
\texttt{sandwich::estfun.DirectAdjusted()} to return the $n \times \tilde{k}$ matrix
\begin{equation} \label{eq:15}
  \Psi - \Phi\hat{A}_{11}^{-t}\hat{A}_{21}', 
\end{equation}
while continuing to allow \texttt{sandwich::bread()} as applied to
\texttt{DirectAdjusted} objects to return only
the lower $A_{22}^{-1}$ matrix, then the effect of this will be for \texttt{sandwich::sandwich()}
and \texttt{sandwich::vcovCL()} to return \eqref{eq:6} with
$\hat{M}$'s,  as
given by \eqref{eq:7} or \eqref{eq:8} respectively, substituted for
$M$'s, and with $\hat{A}$'s for $A$'s.

(In \eqref{eq:15}
we use
\begin{align*}
  \hat{A}_{11}(\beta) &\defeq  n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\nabla_{\beta}\phi(\vec{z}_{i};
        \beta ),\\
  \hat{A}_{21}(\beta) &\defeq n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\nabla_{\beta}\psi(\vec{z}_{j};
        \tau, \alpha, \beta ),\\
  \hat{A}_{11}&\defeq \hat{A}_{11}(\hat\beta)\, \text{and}\, \hat{A}_{21}\defeq \hat{A}_{21}(\hat\beta).
\end{align*}
Under the model based
formulation, $\hat{A}_{21}(\cdot) \equiv {A}_{21}(\cdot)$; when the
first-stage model is a glm, also $\hat{A}_{11}(\cdot) \equiv
{A}_{11}(\cdot)$.  However, in design based formulations neither
equivalence holds in general.)

Alternately put,
defining $\tilde{k}$ by 1 random matrices 
\begin{equation}\label{eq:10}
    \psi_\text{DA}(\vec{Z}; \tau, \alpha, 
    \beta) = \psi (\vec{Z}; \tau,\alpha, 
    \beta) -
    \hat{A}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{Z}_{i};
    \beta), 
\end{equation}
we have in the model-based formulation 
\begin{multline*}
  \operatorname{Cov}\left(n^{-1}\sum_{i=1}^{n} \psi_\text{DA}(\vec{Z}_{i}; \tau_{(n)},\alpha_{(n)},
    \beta_{(n)})\right) = \\
  n^{-1}M_{22} -
                                 n_{\mathcal{C}}^{-1}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_{\mathcal{C}}^{2}}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t},
                               \end{multline*}
regardless of what adjustments for
clustering or heteroskedasticity that we might seek to apply.


In the design-based formulation there
would likely be approximate equality here, as in that setting
$\hat{A}_{21}(\beta_{(n)})$ and $\hat{A}_{11}(\beta_{(n)})$ needn't
coincide with ${A}_{21}(\beta_{(n)})$ and ${A}_{11}(\beta_{(n)})$, respectively.  In both design- and model-based settings, absorption (e.g. of intercepts) introduces additional wrinkles.

This generalizes readily beyond comparison regressions of the type encoded in \eqref{eq:14}. Should $\tilde{\psi}(\cdot; \cdot)$ encode such a regression with a subgroup or continuous moderator variable, or even with the offset $g(\vec{x}\beta)$ replaced with independent variables $\gamma_{0}g(\vec{x}\beta) + \sum_{j=1}^{k}\gamma_{j} g(\vec{x}\beta)\indicator{a_{i}=j}$, then we only have to generate a corresponding $\tilde{A}_{21}(\cdot)$ to use
\begin{equation*}
      \tilde{\psi}_\text{DA}(\vec{Z}; \tau, \alpha, 
    \beta) = \tilde{\psi} (\vec{Z}; \tau,\alpha, 
    \beta) -
    \hat{\tilde{A}}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{Z}_{i};
    \beta)
\end{equation*}
in combination with $\tilde{A}_{22}^{-1}$ as bread matrix. As long as the comparison regression value inherits from \texttt{lm} (or \texttt{glm}), \texttt{sandwich::bread()} as applied to it will with little or no modification return
$\tilde{A}_{22}^{-1}$; \texttt{sandwich::vcov()} and \texttt{sandwich::vcovCL()} should do the right thing. 

\subsection{Aside: ``Model-based'' vs. ``Design-based''}\label{sec:des-vs-mod-based}

In the model-based perspective, expected value and covariance are
conditional on
\[ \mathcal{M} = \sigma\left(\left[(X_{i}, W_{i}, M_{i}, A_{i}: i \in c): c \in
      \mathcal{C}\cup \mathcal{Q} \right]\right)\]


In the design-based perspective, $\mathcal{Q}$ is partitioned into
blocks $b$, and expected value and covariance are
conditional on
\[\mathcal{Z} = \sigma(\{(X_{i}, W_{i}, M_{i}, Y_{i}[1], \ldots Y_{i}[k]):
  i \in c; c\}, \{\sum_{c
    \in b}\, \![A_{c}=j\!] : j \in \{0, \ldots, k\}; b\} )\]
where $a_{c}$ denotes the common treatment assignment of all $i\in
c$. (So estimating equations ${\Psi}_{i}$s are independent across but not within
clusters $c \ni i$.)

To incorporate $c\in \mathcal{C}\setminus\mathcal{Q}$ into this
definition, let each such cluster $c$ constitute a block unto itself,
$\{c\}$. This has the consequence that observations $i
\in \bigcup\left(\mathcal{C}\setminus\mathcal{Q}\right)$ do not
contribute to $M_{11}$, $M_{12}$ or $M_{21}$.

\section{Accommodating intercept(s) via absorption}\label{sec:accomm-interc-via}
As $\alpha$ is a nuisance parameter, let's use the
Frisch-Waugh-Lovell (FWL) Theorem%
\footnote{Or rather, we invoke the variant of the FWL theorem calling
  for regression of the unchanged independent variable on residualized
or partialed out dependent variables.  See e.g. Section 8.4.1,
``Regressing the partialled-out X on the full Y,'' of
\href{https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html}{Robinson's
  blog writeup of Wooldridge's 10 biggest principles of econometrics},
which in turn references Angrist \& Pitschke (2009).} to get rid of it.  In the process we'll
introduce at least $k$ new nuisance parameters, but we can think of these as
being estimated not simultaneously but in a prior regression fit.
We can fold them into the estimator stack in the same way that a prior
covariance model is folded in.

First, let's use $[\alpha_{1}, \ldots, \alpha_{s}]$
as block-specific intercept parameters. Then 
estimating function \eqref{eq:14} becomes

\begin{multline}
  \label{eq:9}
  \psi(\vec{z}; \tau, \alpha, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}} \owt[]\times \\  
  \left(
    \begin{array}{c}
           [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} -
      \tau_{a_{i}}]\indicator{b_{i}=1}\\
      \vdots\\ 
          {} [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{s} -
      \tau_{a_{i}}]\indicator{b_{i}=s}\\      
         {}  [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \sum_{r=1}^s\alpha_r\indicator{b_i=r}-\tau_{1}]\indicator{a_{i}=1}\\
                  \vdots \\
          {} [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                 \sum_{r=1}^s\alpha_r\indicator{b_i=r} - \tau_{k}]\indicator{a_{i}=k}\\
    \end{array}
\right).
\end{multline}

Recall that $\tau_{0} = \rho_{0}-\rho_{0}=0$.  If \texttt{absorb=FALSE}, then we revert to \eqref{eq:14}, i.e.
\begin{equation*}
         \psi(\vec{z}_{i};
  \alpha, \tau, \beta) = \indicator{i \in \bigcup \mathcal{Q}}\owt[]
         \left( \begin{array}{l}
           y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_0 - \tau_{a_{i}}\\
           {}[y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_0-\tau_{1}]\cdot \indicator{a_{i}=1}\\
                  \vdots \\
           {}[y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_0 - \tau_{k}]\cdot \indicator{a_{i}=k}\\%
                \end{array}\right).
\end{equation*}
In this case we rely on inverse probability weighting (via $\owt[]$ factors) to ensure causal interpretability in light of the design;  $\hat{\alpha}_0$ will correspond to an $\owt[]$-weighted control group mean. In contrast to the situation with a grand-mean intercept but no block effects, \eqref{eq:9}'s block-specific intercepts $\alpha_s$ do not ordinarily correspond to block means under control.\footnote{%
The argument of \S~\ref{sec:psit-rho_0-tau} no longer applies
to show the system of equations
$0=\sum_{i}\psi(\vec{z}_{i}; \tau, \alpha, \beta)$
equivalent to $0=\sum_{i}\grave{\psi}(\vec{z}_{i}; \tau,
\alpha, \beta)$, where $\grave{\psi}(\vec{z}; \tau,
\alpha, \beta)$ has uppermost $s$ entries
\begin{equation*}
    \left(
    \begin{array}{c}
           [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} ]\indicator{b_{i}=1} \indicator{a_{i}=0}\\
      \vdots\\ 
          {} [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{s}]\indicator{b_{i}=s} \indicator{a_{i}=0}\\      
    \end{array}
    \right),
  \end{equation*}
in parallel with \eqref{eq:5}, but bottom $k$ entries the same as ${\psi}(\vec{z}_{i}; \tau,
\alpha, \beta)$'s. That is, it may not be the case that the $\alpha$-solutions of
$0=\sum_{i}\psi(\vec{z}_{i}; \tau, \alpha, \beta)$ are
block-specific control group means.  Accordingly its $\tau$-solutions
do not necessarily bear interpretation as contrasts of Hajek estimators.
}

The FWL theorem is used to switch out the system of equations
$0=\sum_{i}\psi(\vec{z}_{i}; \tau, \alpha, \beta)$
for the pair of systems $0 = \sum_{i}\absorbInterceptsEF(\vec{z}_{i}; \mathbf{p})$ and
$0= \sum_{i}\acute{\psi}(\vec{z}_{i}; \tau, \mathbf{p},
\beta)$,  where
\begin{align}
  \absorbInterceptsEF(\vec{z}; \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{ccc}
      % (\indicator{a_{i}=0}-p_{01})\indicator{b_{i}=1}
      % &\ldots
      % &
      %   (\indicator{a_{i}=0}-p_{0s})\indicator{b_{i}=s}
      % \\
      (\indicator{a_{i}=1}-p_{11})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{a_{i}=1}-p_{1s})\indicator{b_{i}=s}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{a_{i}=k}-p_{k1})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{a_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  \right)%
  \right) ;\nonumber \\
  \acute{\psi}(\vec{z}; \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber \\
&\left(
  \begin{array}{c}                                               
    % \big[y_{i}[a_{i}]
    % -
    % g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{a_{i}}\big]
    % (
    % \indicator{a_{i}=0} - \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r})\\
    \big[y_{i}[a_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{a_{i}}\big]
    (
    \indicator{a_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[a_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{0}-
    \tau_{a_{i}}\big]
    (\indicator{a_{i}=k}
    - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\                                             \end{array}
\right) . \label{eq:upsilondef}
\end{align}

In the model based formulation, $[A_{i}: i]$ is held fixed, so
solutions $\hat{\mathbf{p}}$ of
$0 = \sum_{i}\absorbInterceptsEF(\vec{z}_{i}; \mathbf{p})$ have
covariance 0.  Writing $\acute{\Psi} =
[\acute{\psi}(\vec{z}_{i}; \mathbf{p}, \beta, \tau) : i]'$, one
obtains valid\footnote{%
The residuals contributing to $\acute{\Psi}$ via \eqref{eq:upsilondef} do not remove stratum
effects, as conventional covariance estimators for this scenario
seem to do.  This aspect would inflate these standard errors
relative to those. Nonetheless, I maintain that
\texttt{estfun.DirectAdjusted()} as described here are sound as a
basis for variance-covariance estimation.}
  sandwich estimates by substitution of $\acute{\Psi}$ for
$\Psi$ in \eqref{eq:15}.  That is, for a
\texttt{DirectAdjusted} object \texttt{DA} we should have
\begin{equation*}
\mathtt{estfun.DirectAdjusted(DA)} = 
  \begin{cases}
  \acute{\Psi} -
  \Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'& \mathtt{DA} \text{ absorbs
    intercepts}\\
\Psi -
  \Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  & \text{ otherwise}.
\end{cases}
\end{equation*}

In design-based formulation, randomness of $A$ can propagate to
sampling variability of $\hat{\mathbf{p}}$,  so we must attend to
variance contributions from $[\absorbInterceptsEF(a_{i}, \mathbf{p}): i]$. 
The $\mathbf{p}$ being another nuisance parameter estimated in a
preliminary regression, we can get variance propagation for it by
another elaboration of estimating functions along the lines of
\eqref{eq:15}. We facilitate doing
this by \st{setting up our
\texttt{sandwich::estfun.DirectAdjusted()} to return} creating a
helper \texttt{estfun\_DB\_blockabsorb()}, applying to
\texttt{DirectAdjusted} objects \texttt{DB} and returning
\begin{equation*}
\begin{cases}
  \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' &
  \mathtt{DA} \text{ absorbs intercepts}\\
0 & \text{ otherwise}\\
\end{cases}
\end{equation*}
where 
$\AbsorbInterceptsEF= [\absorbInterceptsEF(\vec{z}_{i}; \mathbf{p}): i]'$, 
$A_{\mathbf{p}, \mathbf{p}} = n^{-1}\sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E} [\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{Z}_{i};
\mathbf{p})]$, and $A_{\tau, \mathbf{p}} = n^{-1} \sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E}[\nabla_{\mathbf{p}}\acute{\psi}(\vec{Z}_{i};
\mathbf{p}, \beta, \tau)]$.  For design-based variance-covariance
estimation one uses as estimating function
\begin{multline*}
  \mathtt{estfun.DirectAdjusted(DA)} -
  \mathtt{estfun\_DB\_blockabsorb(DA)} =\\
\begin{cases}
\acute{\Psi} -
  \Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' & \mathtt{DA} \text{ absorbs intercepts}\\
 \Psi -
  \Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}' 
 & \text{ otherwise}.\\
\end{cases}
\end{multline*}

In these displays, $A_{\mathbf{p}, \mathbf{p}}$ is
an average of diagonal matrices $\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{Z}_{i};
\mathbf{p})$ with $-1$s for diagonal entries $(b_{i}-1)s+1, \ldots,
b_{i}s$ and zeros in diagonal entries $r+1, \ldots, r+k$, each
$r \in \{1, \ldots, s\}\setminus \{b_{i}\}$, while $A_{\tau,
  \mathbf{p}}$ is a scaled sum of $\mathbb{E} \nabla_{\mathbf{p}}\acute{\psi}(\vec{Z}_{i};
\mathbf{p}, \beta, \tau)$ terms, $i \in \bigcup \mathcal{Q}$. The average $A_{\tau,
  \mathbf{p}}$ of expected contributions $\mathbb{E} \nabla_{\mathbf{p}}\acute{\psi}(\vec{Z}_{i};
\mathbf{p}, \beta, \tau)$ is estimable by corresponding means of
random variables $\nabla_{\mathbf{p}}\acute{\psi}(\vec{Z}_{i};
\mathbf{p}, \beta, \tau)$.  The sample observation $i$ contribution $\nabla_{\mathbf{p}}\acute{\psi}(\vec{z}_{i};
\mathbf{p}, \beta, \tau)$ is the product of the $k \times k$
diagonal matrix with entries $y_{i}[j] - g(\vec{x}_{i}\beta) -
\tau_{j}$, $j=1, \ldots, k$, with a $k \times (ks)$ block matrix
consisting of the opposite of the $k\times k$ identity at the
$b_{i}$th block and the 0 matrix for the $r$th $k \times k$ block whenever
$r\neq b_{i}$.

\section{Of possible future relevance}

As of this writing, we do not plan to implement moderator effect estimation by way of moderatore absorption.  Should these plans change, Section~\ref{sec:accomm-moder-vari} may be of use.
\subsection{Accommodating moderator variables via absorption}
\label{sec:accomm-moder-vari}


For a continuous moderator $m$, absorption of the moderator into the assignment variable
corresponds to solving the system $0 = \sum
\absorbModeratorEF(\vec{z}_{i}; \mathbf{p},{\gamma})$ for
$\gamma$, where $\mathbf{p}$ solves $0 =
\sum_{i}\absorbInterceptsEF(\vec{z}_{i}; \mathbf{p})$,
$\absorbInterceptsEF(\cdot)$ as above, 
\begin{equation}
  \label{eq:18}
    \absorbModeratorEF(\vec{z}_{i}; \mathbf{\gamma})=
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (
         % \indicator{a_{i}=0}-p_{0b_{i}}-\gamma_{0}
         % \tilde{m}_{i}
         % )m_{i} \\
         (
         \indicator{a_{i}=1}-p_{1b_{i}}-\gamma_{1}
         \tilde{m}_{i}
         )m_{i} \\
         \vdots\\
         (\indicator{a_{i}=k}-p_{kb_{i}}
         - \gamma_{k} \tilde{m}_{i})m_{i}\\
       \end{array}
  \right)%
\end{equation}
and $\tilde{m}$ is $m$ with block centering. Equivalently, $\gamma$
solves $0 = \sum
\absorbModeratorEF(\vec{z}_{i}; \mathbf{p},{\gamma})$ with 
$\absorbModeratorEF(\vec{z}_{i}; \mathbf{\gamma})$ given by 
\begin{equation*}
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (\indicator{a_{i}=0}-p_{0b_{i}}-\gamma_{0}
         % \tilde{m}_{i})\tilde{m}_{i} \\
         (\indicator{a_{i}=1}-p_{1b_{i}}-\gamma_{1}
         \tilde{m}_{i})\tilde{m}_{i} \\
         \vdots\\
         (\indicator{a_{i}=k}-p_{kb_{i}}
         - \gamma_{k} \tilde{m}_{i})\tilde{m}_{i}\\
       \end{array}
     \right).
\end{equation*}
If there were no blocks,
or no absorption of block intercepts, then $\tilde{m} = m -
\bar{m}_{w}$,  where $\bar{m}_{w}$ is $m$'s weighted mean, and $p_{jk}
\equiv p_{j}$, with $\hat{p}_{j}$ equal to a weighted mean of
$\indicator{a_{i}=j}$. 

For a subgrouping variable $m$ with $t>1$ levels, and corresponding
indicator variables $m_{\ell} =\indicator{m_{i}=\ell}$, $\ell=1,
\ldots, t$, and block-centered versions $\tilde{m}_{\ell}$  moderator absorption into the assignment variable
corresponds to solving $0 = \sum
\absorbModeratorEF(\vec{z}_{i}; \mathbf{p}, {\gamma})$ for $\gamma$, where
\begin{align}
    \absorbModeratorEF(\vec{z}_{i}; \mathbf{\gamma})
&=
                                                 \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}  \left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   % (\indicator{a_{i}=0}-p_{0b_{i}}-\tilde{m}_{1i}\gamma_{1})m_{1i}&\ldots&(\indicator{a_{i}=0}-p_{0s}-\tilde{m}_{ti}\gamma_{t})m_{ti}
                                                   % \\
                                                   (\indicator{a_{i}=1}-p_{1b_{i}}-\tilde{m}_{1i}\gamma_{1})m_{1i}&\ldots&(\indicator{a_{i}=1}-p_{1s}-\tilde{m}_{ti}\gamma_{t})m_{ti}
                                                   \\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{a_{i}=k}-p_{kb_{i}}
                                                   - \tilde{m}_{1i}\gamma_{1})m_{1i}&\ldots&(\indicator{a_{i}=k}-p_{ks}-\tilde{m}_{ti}\gamma_{t})m_{ti}\\                                                 \end{array}
  \right)%
  \right) .  \label{eq:17}
\end{align}

In the model-based formulation neither of these demands any special
accounting in covariance estimation, since both $A$ and the moderating
variable are held fixed by conditioning. We get a legit covariance
without doing anything special, using an $\acute{\psi}$ given by
\begin{multline*}
  \acute{\psi}(\vec{z}; \tau, \mathbf{p}, \beta) =
    \indicator{i \in \bigcup \mathcal{Q}}\owt[] \times \\
\left(
  \begin{array}{r}
    % \big[y_{i}[a_{i}]
    % -
    % g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{a_{i}}\big]
    % (
    % \indicator{a_{i}=0} -
    % \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}
    % - \tilde{m}_{i}\gamma)\\
    \big[y_{i}[a_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{a_{i}}\big]
    (
    \indicator{a_{i}=1}-
    \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r}
    - \tilde{m}_{i}\gamma)\\
    \vdots \\
    {}\big[y_{i}[a_{i}]
    - g(\vec{x}_{i}\beta) -\alpha_{0} -
    \tau_{a_{i}}\big]
    (
    \indicator{a_{i}=k}
    -
    \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r}
    -
    \tilde{m}_{i}\gamma)\\
  \end{array}
\right) .
\end{multline*}
Because the covariance estimates will
involve residuals that have not been decorrelated from the moderating
variable, they may err on the side of conservatism relative to
covariance estimates issuing from a fit that addressed the moderator
not by absorption but by adding it to the outcome regression
equation. We might consider alternate elaborations of the estimating
function that introduce moderator main effects to the outcome equation.

For design-based calculations, \texttt{estfun\_DB\_blockabsorb(DA)}
 should return
 \begin{equation*}
   A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' +
  \AbsorbModeratorEF{}A_{\gamma \gamma}^{-1}\hat{A}_{\tau \gamma}'.
 \end{equation*}
 In this way, the overall estimating function
 \texttt{estfun.DirectAdjusted(DA)} $-$ \texttt{estfun\_DB\_blockabsorb(DA)} 
 becomes
 \begin{equation}
   \label{eq:19}
  \acute{\Psi} -
  \Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}' - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' -
  \AbsorbModeratorEF{}A_{\gamma \gamma}^{-1}\hat{A}_{\tau \gamma}'
 \end{equation}
with $A_{\gamma \gamma}$ and $A_{\tau \gamma}$ defined in the natural
way given the $\absorbModeratorEF()$ definitions stated just above. 

\subsection{Notes regarding design-based variance calculations}

\st{Potentially helpful} probably unhelpful observation:
% I updated these expressions to use random weights without
% stopping to consider whether the original idea properly allowed
% for the weights to depend on $A_{i}$.
design-based covariances satisfy
\begin{align*}
  \operatorname{Cov}[&n^{-1}\sum_{i}\upsilon(\vec{Z}_{i};
  \mathbf{p}, \beta, \tau)] \\
                     &=   \operatorname{Cov}\left[n^{-1}\sum_{i \in \bigcup \mathcal{Q}}
                       \owt[{[A_{i}]}]
\left(
                                             \begin{array}{c}                                               
           % {}[y_{i}[0]
           %                                     -
           %                                     g(\vec{x}_{i}\beta)-\alpha_{0}](
           %                                     \indicator{A_{i}=0} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
           {}[y_{i}[1]
                                               -
                                               g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{1}](
                                               \indicator{A_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
                  \vdots \\
{}           [y_{i}[k]
                                               - g(\vec{x}_{i}\beta) - \alpha_{0} -
                                               \tau_{k}](\indicator{A_{i}=k}
                                               - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\                                             \end{array}
\right) \right]\\
                     &\stackrel{\text{sometimes}}{=}  \operatorname{Cov}\left[n^{-1}\sum_{i \in \bigcup \mathcal{Q}}
\owt[{[A_{i}]}]
\left(
                                             \begin{array}{c}                                               
           % [y_{i}[0]
           %                                     -
           %                                     g(\vec{x}_{i}\beta)-\alpha_{0}]
           %                                     \indicator{A_{i}=0}\\
           {}[y_{i}[1]
                                               -
                                               g(\vec{x}_{i}\beta)-\alpha_{0} -\tau_{1}]
                                               \indicator{A_{i}=1}\\
                  \vdots \\
{}           [y_{i}[k]
                                               - g(\vec{x}_{i}\beta) - \alpha_{0}-
                                               \tau_{k}]\indicator{A_{i}=k}\\                                             \end{array}
\right)
\right].
\end{align*}
This is because the centering values
$\sum_{r=1}^{s}p_{jr}\indicator{b_{i}=r}$ are nonrandom from the
design-based perspective, as are the residual-like%
\footnote{Recall that the actual residuals would be $y_{i}[j] -
  g(\vec{x}_{i}\beta) - \tau_{j} - \sum_{r=1}^{s}\alpha_{r}\indicator{b_{i}=r}$, not $y_{i}[j] - g(\vec{x}_{i}\beta) - \tau_{j}$.}
quantities
$y_{i}[j] - g(\vec{x}_{i}\beta) - \tau_{j}$ that they get
multiplied by.   If the weights are flat, or inverse probability
weights, or inverse assignment odds weights, then weighted sums of
these quantities will be design constants.  Unfortunately, however, if
the weights also involve weighting factors arising from the data,
e.g. cluster sizes, then the corresponding weighted sums need no
longer be design constants.

Similar considerations yield
\begin{align}
\operatorname{Cov}_{d}[
  &n^{-1}\sum_{i}\absorbInterceptsEF(\vec{Z}; \mathbf{p})] \nonumber\\
  =&\operatorname{Cov}_{d}[n^{-1}
                                                 \sum_{i \in \bigcup
     \mathcal{Q}} \owt[{[A_{i}]}] \times \nonumber \\
&  \operatorname{vec}\left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   % (\indicator{A_{i}=0}-p_{01})\indicator{b_{i}=1}&\ldots&(\indicator{A_{i}=0}-p_{0s})\indicator{b_{i}=s}\\                           
                                                   (\indicator{A_{i}=1}-p_{11})\indicator{b_{i}=1}&\ldots&(\indicator{A_{i}=1}-p_{1s})\indicator{b_{i}=s}\\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{A_{i}=k}-p_{k1})\indicator{b_{i}=1}&\ldots&(\indicator{A_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                                   
                                                 \end{array}
  \right)%
  \right)
  \Biggr] \nonumber \\ 
  \stackrel{\text{sometimes}}{=}&\operatorname{Cov}_{d}[n^{-1}
                                                 \sum_{i \in \bigcup
     \mathcal{Q}} \owt[{[A_{i}]}] \times \nonumber \\
  &\operatorname{vec}\left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   % \indicator{A_{i}=0}\indicator{b_{i}=1}&\ldots&\indicator{A_{i}=0}\indicator{b_{i}=s}\\                                             
                                                   \indicator{A_{i}=1}\indicator{b_{i}=1}&\ldots&\indicator{A_{i}=1}\indicator{b_{i}=s}\\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   \indicator{A_{i}=k}\indicator{b_{i}=1}&\ldots&\indicator{A_{i}=k}\indicator{b_{i}=s}\\                                                   
                                                 \end{array}  
  \right)%
  \right)
    \Biggr] \label{eq:11}
\end{align}
Can we also represent estimating functions
$\phi(\vec{Z}_{i}; \beta)$ in form
$b(\mathbf{x}_{i}) + \sum_{j=0}^{k}c_{j}(y_{i}[j], \vec{x}_{i}, \beta)\indicator{A_{i}=j}$, after conditioning
on potential outcomes? If so, then as a consequence we can think of
\eqref{eq:10} as having rows that are also of this form.
\eqref{eq:11} says $\absorbInterceptsEF(\vec{Z}_{i}; \mathbf{p})$
has this form, so if we can get represent
$\phi(\vec{Z}_{i}; \beta)$ in this way then \eqref{eq:19}
will have this form as well. That strikes me as likely to facilitate
derivation, 
presentation in print and coding in R of these design-based covariances. 

To do:
sketch out $\phi()$'s under different covariance adjustment scenarios:
covariance model fit to quasiexperimental controls but no other
observations in the quasiexperimental sample; covariance model fit to
entire quasiexperimental sample, without a contribution from $A$;
covariance model fit to entire quasiexperimental sample, with linear
contribution from $A$; covariance model fit to entire
quasiexperimental sample with a $A$ term and also $A$-interactions
with covariates.

\nobibliography{estfun_DA}
\end{document}

\begin{multline}
  \label{eq:9}
  \psi(\vec{z}; \tau, \alpha, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}} \owt[]\times \\  
  \left(
    \begin{array}{c}
           y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \beta m_{i}-
      \sum_{j=1}^{k}(\tau_{j}+\delta_{j}m_{j})\indicator{a_{i}=j} \\
      \big[y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \beta m_{i}-
      \sum_{j=1}^{k}(\tau_{j}+\delta_{j} m_{i})\indicator{a_{i}=j}
      \big] m_{i}\\
      \big[y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \beta m_{i}-
      (\tau_{1}+\delta_{1} m_{i}) \big](m_{i}\indicator{a_{i}=1})\\      
\vdots \\
      \big[y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \beta  m_{i}-
      (\tau_{k}+\delta_{k} m_{i}) \big](m_{i}\indicator{a_{i}=k})\\      
         {}  [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                  \sum_{r=1}^s\alpha_r\indicator{b_i=r}-\tau_{1}]\indicator{a_{i}=1}\\
                  \vdots \\
          {} [y_{i}[a_{i}] - g(\vec{x}_{i}\beta)-
                 \sum_{r=1}^s\alpha_r\indicator{b_i=r} - \tau_{k}]\indicator{a_{i}=k}\\
    \end{array}
\right).
\end{multline}

\subsubsection{Alt def of M's}
\[
  \begin{array}{cc}
    M_{11}  = n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\vec{Z}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\vec{Z}_{i};
                                                                  \beta
                                                                  ), \sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi'(\vec{Z}_{i}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = n^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi(\vec{Z}_{i};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
With these definitions,
\[
  \operatorname{Cov}(\hat\tau) \approx n^{-1}A_{22}^{-1}\{M_{22} - [M_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}M_{21}] + (n/n_C)A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]


We can express HC0 type estimators with matrix rather than summation notation, using $F$ for the \{0,1\}-valued $n
\times m$ matrix with an indicator column for each cluster.  
\[
  \begin{array}{cccc}
  \hat{M}_{11} &=n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} &= n_{C}^{-1}(F'\Phi)(F'\Psi)\\
    \hat{M}_{21} &=\hat{M}_{12}' & \hat{M}_{22}&= n^{-1}(F'\Psi)(F'\Psi)\\
    \end{array}
\]




\section{Communicated in ``Josh/Xinhe Meeting Notes," Nov 30 2022
  post-mtg note}
There are random estimating functions $n_{\mathcal{C}}^{-1}\sum_{i\in\mathcal{C}}\phi(\vec{Z}_{i}; \beta)$  and 
       $n_{\mathcal{Q}}^{-1}\sum_{i\in
         \mathcal{Q}}\psi(\vec{Z}_{i}; \tau, \alpha, \beta)$,
       $\psi(\vec{Z}_{i}; \tau, \alpha, \beta) =
       \owt[][y_{i} - \tau(a_{i} - \bar{a}) -
       g(\vec{x}_{i}\beta)](a_{i} - \bar{a})$ where $\bar{a}$ solves
       \begin{equation}
         \label{eq:4}
         0=\sum_{i \in \mathcal{Q}}w_{i \mathcal{Q}}(a_{i} - \bar{a}),
       \end{equation}
       inducing corresponding estimating equations
\[ \begin{array}{c} 0 \\ 0 
   \end{array} = \left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{z}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{z}_{i}; \tau, \alpha, \beta )
     \end{array}
\right).
\]

We seek to represent $\sum_{i\in
         \mathcal{Q}}\psi(\vec{Z}_{i}; \tau, \alpha, \beta)$ as a
       linear combination of
       \begin{align}
         \label{eq:1}
         \sum_{i \in \mathcal{Q}}\psi_{0i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                             \rho_{0}](1-a_{i})\\
         \label{eq:2}
         \sum_{i \in \mathcal{Q}}\psi_{1i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                              \rho_{1}]a_{i}\\
         \label{eq:3}
         &\rho_{1} - \rho_{0} - \tau.
       \end{align}

       If my algebra is OK, we have
       \begin{equation*}
         \sum_{i\in
         \mathcal{Q}}\psi(\vec{Z}_{i}; \tau, \alpha, \beta) =
       \sum_{i \in \mathcal{Q}}\psi_{0i} -  \bar{a}\sum_{i \in \mathcal{Q}}(\psi_{0i} + \psi_{1i})
     \end{equation*}
     by some algebra including \eqref{eq:4}.  I had to call time
     before seeing about whether and how to accommodate scenarios with
     strata being absorbed into the treatment variable; there are at
     least a few additional wrinkles lurking there. 
 

M-estimation's characteristic covariance approximation is  
\[ \operatorname{Cov}([\hat\beta',\hat\tau']) \approx
  A^{-1} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{Z}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{Z}_{i}; \tau, \alpha, \beta )
     \end{array}
\right)A^{-1}
\]

The right-hand side covariance isn't quite a constant multiple of $B$ as in previous comments here (in particular with scaling factor $n_{\mathcal{Q}\mathcal{C}}^{-1}$ on off-diagonals).  Rather, with $B$ as defined there,
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{Z}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{Z}_{i}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}B_{11}& \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{12}\\
    \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{21} &
                                                                              n_{\mathcal{Q}}^{-1}B_{22}
  \end{array}
\right).
\]

From this I get
\[
  \operatorname{Cov}(\hat\tau) \approx n_Q^{-1}A_{22}^{-1}\{B_{22} - (n_{CQ}/n_C)[B_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}B_{21}] + (n_Q/n_C)A_{21}A_{11}^{-1}B_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]
(with $A$s and $B$s as defined in earlier comments in the thread).
