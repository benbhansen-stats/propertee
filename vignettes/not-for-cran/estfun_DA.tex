\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}%for \llbracket
\usepackage{mathtools}% for DeclarePairedDelimiter
\usepackage{soul}
\usepackage{colonequals}
\newcommand{\defeq}{\ensuremath{\colonequals}}
\newcommand{\eqdef}{\ensuremath{\equalscolon}}

\newcommand{\EE}{\operatorname{E}}
\DeclarePairedDelimiter{\indicator}{\llbracket}{\rrbracket}
\newcommand{\owt}[1][z_i]{\ensuremath{w_{i[#1]}}}
\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{plain}

\usepackage{hyperref}


\author{BBH}

\begin{document}
\nobibliography*

\section{\texttt{sandwich::estfun()} method for DA objects (cf
  \#79)}
(BBH notes; \today)
\subsection{Concept}

Let $\mathcal{C}$ and $\mathcal{Q}$ denote the covariance and
quasiexperimental samples, respectively, expressed at the level of
unit of assignment: if assignment is at the level of the classroom,
then these are collections of classes not collections of students, nor
of student-year observations.  In turn, identify each unit of assignment
with the set of elemental observations they contain: a classroom is
identified with the 
set of students, or student-years, associated with that classroom;
if there is no clustering, then we think of $\mathcal{C}$ and
$\mathcal{Q}$ as consisting of singleton sets, each containing a
single element.  (Additional structure: any clusters $c, c' \in \mathcal{C}
\cup \mathcal{Q}$ that are distinct, $c  \neq c'$, are nonoverlapping, $c \cap c' = \emptyset$.) This permits us to write $\mathcal{Q}$ for the
quasiexperimental sample as expressed in terms of clusters, and $\bigcup
\mathcal{Q}$ for the collection of all elements belonging to the
quasiexperimental sample.

There are random estimating functions $\Phi(\mathbf{Y}; \beta ) =
[\phi(\tilde{\mathbf{Y}}_{i};
\beta) : i]'$, an $n$ (number of elements) by $p$ matrix,  and 
       $\Psi(\mathbf{Y}; \rho, \beta) = [\psi(\tilde{\mathbf{Y}}_{i};
       \rho, \beta): i]'$, \textit{also} an $n$ by $k$ matrix, with 
       $\phi(\tilde{\mathbf{Y}}_{i}; \beta)=$ covariance model estfun
       contrib times $\indicator{i \in \bigcup \mathcal{C}}$ and, for
       now (to be superceded by \eqref{eq:14}):
\begin{equation}
         \label{eq:5}
         \psi(\tilde{\mathbf{Y}}_{i}; \rho, \beta) =\\
         \left( \begin{array}{c}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[0] [Y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \rho_{0}]\indicator{z_{i}=0}\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[1] [Y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \rho_{1}]\indicator{z_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[k] [Y_{i}[k] - g(\vec{x}_{i}\beta)-
                  \rho_{k}]\indicator{z_{i}=k}\\
                \end{array}\right)
\end{equation}
where $Y_{i}[j]$ represents $i$'s potential outcome following
assignment to condition $i$; $\owt[j]$ represents the product of 1, any user-provided weight attaching to observation unit $i$, and, as requested by the user, odds- or inverse probability-based weights attaching to $i$'s assignment unit $[i]$ under assignment $j$; and $\tau_{j} = \rho_{j}-\rho_{0}$, for $j=1,
\ldots, k$. I say ``for now'' because \eqref{eq:14} will present a
$\psi(\cdot)$ with equivalent solutions that is better aligned with
\texttt{lm()}'s default treatment effect parameterizations and their
adaptations involving ``absorption'' of intercept terms (also denoted $\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta)$, see \eqref{eq:9} and following, and $\upsilon(\tilde{\mathbf{Y}}_{i}; \tau, \mathbf{p}, \beta)$, see \eqref{eq:upsilondef} and following; both in \S~\ref{sec:accomm-interc-via} below).  I've written ${\Psi}(\cdot)$ as
it would appear in the model-based interpretation; in design-based
inference we would have
\begin{equation*}
           \psi(\tilde{\mathbf{Y}}_{i}; \rho, \beta) =
         \left( \begin{array}{c}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[0][y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \rho_{0}]\indicator{Z_{i}=0}\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[1][y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \rho_{1}]\indicator{Z_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[k][y_{i}[k] - g(\vec{x}_{i}\beta)-
                  \rho_{k}]\indicator{Z_{i}=k}\\                  
                \end{array}\right).
\end{equation*}
(Under the former expansion, appropriate to model-based inference,
${\Psi}_{i}$s are independent across if not within
clusters; in the design-based interpretation independence obtains only across blocks or
strata.)  \textbf{To do}: present appropriate $\psi(\cdot)$ for inference
about moderator:treatment coefficient(s) following ``absorption'' of the moderator
variable into the treatment variable (cf. \href{https://github.com/benbhansen-stats/flexida/issues/81#issuecomment-1282944562}{\#81}).
      
       Suppose that we define \texttt{estfun.DirectAdjusted()} associating
with a direct adjustment model a matrix of evaluated estimating function
contributions of form
\begin{equation*}
  \tilde{\Psi} = c_{2}\Psi - c_{1}\Phi A_{11}^{-1}A_{21}'.
\end{equation*}
Note that this is $n$ by
$k+1$, $n$ being the size of the combined $\mathcal{C}$ and
$\mathcal{Q}$ sample, with columns corresponding to $\rho$.%
(Assuming we're estimating a single marginal treatment effect, as
opposed to moderation effect[s], either without blocking or with
neither explicit nor implicit block fixed effects; otherwise the
number of columns could be $k$\footnote{For a main effect
DA model fitted with absorption of the block intercept or
intercepts; see \S~\ref{sec:accomm-interc-via} below.} or something
larger than $k+1$\footnote{In the case of subgroup effects and/or unabsorbed block fixed effects.},
depending. To obviate the ambiguity we'll say
\begin{equation*}
  \tilde{k} \defeq \mathtt{ncol}(\tilde{\Psi}).)
\end{equation*}

Values of the constants $c_{1}$ and $c_{2}$ are to be determined.
I conjecture these decisions can be made in such away that 
\begin{enumerate}
\item Using this as a basis for a meat matrix while using
  $A_{22}^{-1}$  ``bread matrix'' lands us in the same place as the
  inversion formula as expressed with A's and B's on p.373 of Carroll
  et al. 2006.
\item Suppose we've got multiple DA models that have been fit over the
  same CA model, and we wish to compute joint covariances for their
  distinct parameters
  (\href{https://github.com/benbhansen-stats/flexida/issues/79}{\#79
    on GitHub}). We get appropriate joint bread matrices by
  diagonally combining bread matrices of the DA models, and we get
  appropriate meat matrices by cbind-ing the estfun's.
\item For ``natural'' choices of $c_{1}$ and $c_{2}$, in the
  presence of clustering the corresponding bread matrices are
  reasonable and interpretable, at least in cases of HC0 and HC1
  degree of freedom adjustments. 
\end{enumerate}

\subsection{$\psi(\tilde{\mathbf{Y}}_{i};
  \alpha, \tau, \beta)$ with intercept and $\tau$'s}
\label{sec:psit-rho_0-tau}

The estimating equation $\psi(\tilde{\mathbf{y}}; \rho, \beta)=0$,
$\psi(\cdot)$ as in \eqref{eq:5} above, has the same solutions as $\sum_{i}\psi(\tilde{\mathbf{y}}_{i};
  \alpha, \tau, \beta)=0$, with $\alpha=\rho_{0}$, $\tau_{j}=\rho_{j}-\alpha$ for
$1\leq j \leq k$ and $\psi(\cdot)$ now given by 
\begin{equation} \label{eq:14}
         \psi(\tilde{\mathbf{y}}_{i};
  \alpha, \tau, \beta) =
         \left( \begin{array}{l}
           \indicator{i \in \bigcup \mathcal{Q}}\owt{}\big[y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[1][y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha-\tau_{1}]\indicator{z_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[k][y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha - \tau_{k}]\indicator{z_{i}=k}\\
                \end{array}\right), 
\end{equation}
the omission of ``$\indicator{z_{i}=0}$'' from the first of these
being intentional and deliberate. It doesn't affect the solution
$(\alpha, \tau_{1}, \ldots, \tau_{k})$
because this solution also satisfies
\begin{equation*}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt{}[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha-\tau_{j}]\indicator{z_{i}=j}
\end{equation*}
for each $j=1, \ldots, k$, and therefore
\begin{equation*}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt{}\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha-\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\indicator{z_{i}
                  \neq 0}.
\end{equation*}
The complement of condition $\indicator{z\neq 0}$ being
$\indicator{z=0}$, under which $\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}=0$, it follows that
\begin{align*}
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[0]\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha\big]\indicator{z_{i}
             =0}\\
           & \Updownarrow \\
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt{}\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big].
\end{align*}
One advantage of this representation is that it better represents what
\texttt{lm()} ordinarily does, when given a factor treatment
variable.  Another is that it isolates the intercept parameter, which
we have no use for and will be done away with via absorption (\S~\ref{sec:accomm-interc-via} below). 

\subsection{The covariance we're trying to
  produce}\label{sec:covar-were-trying}

Let $\hat\beta$, $\hat\alpha$ and $\hat\tau$ solve
\[\sum_{i\in \bigcup \mathcal{C}}\phi(\tilde{\mathbf{y}}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\psi(\tilde{\mathbf{y}}_{j}; \tau,
\alpha, \beta )  =0
  \]
  in $\beta$ and $\tau$, whereas $\beta_{0}$, $\alpha_{0}$ and $\tau_{0}$ solve 
\[\sum_{i\in \bigcup \mathcal{C}}\EE\phi(\tilde{\mathbf{Y}}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\EE\psi(\tilde{\mathbf{Y}}_{j}; \tau,
\alpha, \beta )  =0.
  \]
  Writing
  \begin{align*}
%    \label{eq:13}
    A(\alpha, \beta, \tau) &= \left(
      \begin{array}{cc}
        n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\EE [\nabla_{\beta}\phi(\tilde{\mathbf{Y}}_{i};
        \beta )]&0\\
        n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\beta}\psi(\tilde{\mathbf{Y}}_{j};
        \tau, \alpha, \beta )]  & n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\alpha, \tau}\psi(\tilde{\mathbf{Y}}_{j};
        \tau, \alpha, \beta )]
      \end{array}
    \right)\\
    &\eqdef \left(
      \begin{array}{cc}
        A_{11}(\beta)& 0\\
        A_{21}(\beta) & A_{22}(\alpha, \tau)
      \end{array}
\right),
  \end{align*}
where the structure of $\psi(\cdot)$ as given in \eqref{eq:14} ensures
that $A_{21}(\alpha, \beta, \tau)$ and $A_{22}(\alpha, \beta, \tau)$ are functions only of $\beta$ and of $(\alpha, \tau)$ respectively,
  define 
  \begin{align*}
   \tilde\beta
    \defeq& \beta_{0} + A_{11}^{-1}(\beta_{0})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta_{0})
  \quad \text{and}\\
    \left(\begin{array}{c}\tilde\alpha\\ \tilde\tau \end{array}\right)
    \defeq& \left(\begin{array}{c}\alpha_{0}\\
                    \tau_{0} \end{array}\right)
    + A^{-1}(\alpha_{0}, \beta_{0}, \tau_{0})\left(
    \begin{array}{c}
      n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta_{0})\\
      n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{j};
                 \tau_{0},\beta_{0} )
    \end{array}
\right)\\
    \\
               &=\left(\begin{array}{c}\alpha_{0}\\
                         \tau_{0} \end{array}\right) +
    A_{22}^{-1}(\alpha_{0}, \tau_{0})\times \\
    & \left[
                 n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{j};
                 \tau_{0},\beta_{0} ) - A_{21}(\beta_{0}) A_{11}^{-1}(\beta_{0})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta_{0})\right],
\end{align*}
known as \textit{linearizations} (of $\hat\beta$ and $(\hat\alpha, \hat\tau)$).


Observe that these expressions define the same $\tilde\beta$ and $\tilde\tau$
  irrespective of the choice of nonzero constants $n_{\mathcal{C}}$
  and $n$.  To promote alignment with existing \texttt{sandwich}
  functions%
  \footnote{%
    This $n_{\mathcal{C}}$ is the constant that \texttt{sandwich} functions
will select for the covariance model, given its sample size.  It's
incumbent on us to supply \texttt{sandwich::estfun()} and/or
\texttt{sandwich::bread()} methods for \texttt{DirectAdjusted} objects
with compatible implicit
scaling constants. \texttt{sandwich::bread()} and
\texttt{sandwich::breadCL()} take the scaling constant to be the
reciprocal of the number of rows in whatever
\texttt{sandwich::estfun()} returns; it will fall to us to ensure our
$\hat{A}_{21}$ and $\hat{A}_{22}$ observe compatible scalings.}%
, we set $n_{\mathcal{C}}$ and $n$ to be the number of elements
(not clusters) in the covariance and in the combined samples,
respectively.

Assuming that $|\hat\beta -\tilde\beta|_{2}$, 
$|\hat\tau -\tilde\tau|_{2} \stackrel{P}{\rightarrow} 0$,
$A_{11}(\beta_{0})$ and $A_{22}(\beta_{0}, \tau_{0})$ are nonsingular
with eigenvalues bounded away from 0 and $A_{11}(\cdot)$, $A_{21}(\cdot)$ and $A_{22}(\cdot, \cdot)$ are suitably
regular\footnote{E.g, they admit of Lipschitz constants: see \S~9.8,
  \bibentry{keener2010TheorStats}, while bearing in mind issues raised
  by \bibentry{feng2014exact}.}, 
we have the covariance approximation characteristic of M-estimation,
\begin{align*} \operatorname{Cov}([\hat\beta',\hat\alpha', \hat\tau']) \approx&
  \operatorname{Cov}([\tilde\beta',\hat\alpha',\tilde\tau'])\\
  &=
  A^{-1}(\alpha_{0}, \beta_{0}, \tau_{0}) \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta_{0} )\\
       n^{-1}\sum_{j=1}^{n}\psi(\tilde{\mathbf{Y}}_{j}; \tau_{0}, \alpha_{0}, \beta_{0} )
     \end{array}
\right)A^{-1}(\alpha_{0}, \beta_{0}, \tau_{0})
\end{align*}
(recalling that we have so defined $\psi(\cdot)$ that $\psi(\tilde{\mathbf{Y}}_{j}; \tau, \alpha, \beta) = 0$ when
$j\not\in \bigcup \mathcal{Q}$).


To help mark our nonstandard scaling constants, as well as the difference from the package's first implementations of these estimating equation stacks (which took pains to avoid scaling constants), 
bread matrices will be denoted with the letter ``M,'' not ``B.''
We have
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta )\\
       n^{-1}\sum_{j=1}^{n}\psi(\tilde{\mathbf{Y}}_{j}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}M_{11}& n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{12}\\
    n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{21} & n^{-1}M_{22}
  \end{array}
\right),
\]
where
\[
  \begin{array}{cc}
    M_{11}  = n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\tilde{\mathbf{Y}}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  (n_{\mathcal{C}}n)^{-1/2} \operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i};
                                                                  \beta
                                                                  ), \sum_{j}\psi'(\tilde{\mathbf{Y}}_{j}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = n^{-1}\operatorname{Cov}[\sum_{j}\psi(\tilde{\mathbf{Y}}_{j};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
Note that $M_{11}$ is a natural meat matrix, while $M_{22}$'s and
$M_{12}$/$M_{21}$'s scaling constants --- $n^{-1}$ rather than $(\# \bigcup
\mathcal{Q})^{-1}$ (or just $(\# \mathcal{Q})^{-1}$) in $M_{22}$'s
case and $(n_{\mathcal{C}}n)^{-1/2}$ not $[n (\# \bigcup
\mathcal{Q})]^{-1/2}$ in $M_{12}$/$M_{21}$'s --- are progressively more weird.

With these definitions,
\begin{align}
  \operatorname{Cov}(\hat\tau) \approx& A_{22}^{-1}\{n^{-1} M_{22} -
                                 n_{\mathcal{C}}^{-1/2}n^{-1/2}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-1}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}\nonumber
  \\
                               &= n^{-1}A_{22}^{-1}\{
                                 M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
                                . \label{eq:6}
\end{align}
An advantage of the expression with leading factor $n^{-1}$ is that
that way it parallels the calculation of
\texttt{sandwich::sandwich()} and, I think, other \texttt{sandwich}
functions. (Assuming that we write
\texttt{estfun.DirectAdjusted()}  to return a matrix of row extent
$n$.) 

For model-based estimation with units of observation and of assignment being
the same, $M$'s are estimated as follows. 
\begin{equation}\label{eq:7}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} \Phi'\Phi & \hat{M}_{12} = (n_{\mathcal{C}}^{-1/2}\Phi)'(n^{-1/2} \Psi)\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}\Psi'\Psi.\\
    \end{array} 
  \end{equation}
  In the more general case with potentially clustering of observation
units, by units of assigment and maybe more, we can express HC0
type estimators with matrix rather than summation notation by using
$F$ for the \{0,1\}-valued $n \times m$ matrix with an indicator
column for each cluster:
\begin{equation} \label{eq:8}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} = [n_{\mathcal{C}}^{-1/2} (F'\Phi)]'[n^{-1/2} (F'\Psi)]\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}(F'\Psi)'(F'\Psi)\\
    \end{array}
  \end{equation}
  
Leaving the clusterwise aggregation to \texttt{sandwich}, as
appropriate, it seems that if we define
\texttt{sandwich::estfun.DirectAdjusted()} to return the $n \times \tilde{k}$ matrix
\begin{equation} \label{eq:15}
  \Psi -
  \left(\frac{n}{n_{\mathcal{C}}}\right)^{1/2}\Phi
  \hat{A}_{11}^{-1}\hat{A}_{21}', 
\end{equation}
while continuing to allow \texttt{sandwich::bread()} as applied to
\texttt{DirectAdjusted} objects to return only
the lower $A_{22}^{-1}$ matrix, then the effect of this will be for \texttt{sandwich::sandwich()}
and \texttt{sandwich::vcovCL()} to return \eqref{eq:6} with
$\hat{M}$'s,  as
given by \eqref{eq:7} or \eqref{eq:8} respectively, substituted for
$M$'s, and with $\hat{A}$'s for $A$'s.

(In \eqref{eq:15}
we use
\begin{align*}
  \hat{A}_{11}(\beta) &\defeq  n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\nabla_{\beta}\phi(\tilde{\mathbf{y}}_{i};
        \beta ),\\
  \hat{A}_{21}(\beta) &\defeq n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\nabla_{\beta}\psi(\tilde{\mathbf{y}}_{j};
        \tau, \alpha, \beta ),\\
  \hat{A}_{11}&\defeq \hat{A}_{11}(\hat\beta)\, \text{and}\, \hat{A}_{21}\defeq \hat{A}_{21}(\hat\beta).
\end{align*}
Under the model based
formulation, $\hat{A}_{21}(\cdot) \equiv {A}_{21}(\cdot)$; when the
first-stage model is a glm, also $\hat{A}_{11}(\cdot) \equiv
{A}_{11}(\cdot)$.  However, in design based formulations neither
equivalence holds in general.)

Alternately put,
defining $\tilde{k}$ by 1 random matrices 
\begin{equation}\label{eq:10}
    \tilde{\psi}(\tilde{\mathbf{Y}}; \tau, \alpha, 
    \beta) = \psi (\tilde{\mathbf{Y}}; \tau,\alpha, 
    \beta) -
    \left(\frac{n}{n_{\mathcal{C}}}\right)^{1/2}
    \hat{A}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\tilde{\mathbf{Y}}_{i};
    \beta), 
\end{equation}
we have in the model-based formulation 
\begin{multline*}
  \operatorname{Cov}\left(n^{-1}\sum_{i=1}^{n} \tilde{\psi}(\tilde{\mathbf{Y}}_{i}; \tau_{0},\alpha_{0},
    \beta_{0})\right) = \\
  M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t},
                               \end{multline*}
regardless of what adjustments for
clustering or heteroskedasticity that we might seek to apply when
calculating the bread matrix.   In the design-based formulation there
would likely be approximate equality here, noting that in that setting
$\hat{A}_{21}(\beta_{0})$ and $\hat{A}_{11}(\beta_{0})$ needn't
coincide with ${A}_{21}(\beta_{0})$ and ${A}_{11}(\beta_{0})$, respectively.


\subsection{Accommodating intercept(s) via absorption}\label{sec:accomm-interc-via}
As $\alpha$ is a nuisance parameter, let's use the
Frisch-Waugh-Lovell (FWL) Theorem%
\footnote{Or rather, we invoke the variant of the FWL theorem calling
  for regression of the unchanged independent variable on residualized
or partialed out dependent variables.  See e.g. Section 8.4.1,
``Regressing the partialled-out X on the full Y,'' of
\href{https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html}{Robinson's
  blog writeup of Wooldridge's 10 biggest principles of econometrics},
which in turn references Angrist \& Pitschke (2009).} to get rid of it.  In the process we'll
introduce at least $k$ new nuisance parameters, but we can think of these as
being estimated not simultaneously but in a prior regression fit.
We can fold them into the estimator stack in the same way that a prior
covariance model is folded in.

First, let's switch over to using $[\alpha_{1}, \ldots, \alpha_{s}]$
as the intercept parameter(s), allowing for absorption of $s$ block or
stratum fixed effects; if \texttt{absorb=FALSE}, then let $s=1$.    Our estimating function becomes
\begin{multline}
  \label{eq:9}
  \psi(\tilde{\mathbf{y}}; \tau, \alpha, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}} \times \\
  \owt{}
  \left(
    \begin{array}{l}
           [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} -
      \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}]\indicator{b_{i}=1}\\
      \vdots\\ 
          {} [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{s} -
      \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}]\indicator{b_{i}=s}\\      
         {}  [y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \sum_{r=1}^s\alpha_r\indicator{b_i=r}-\tau_{1}]\indicator{z_{i}=1}\\
                  \vdots \\
          {} [y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \sum_{r=1}^s\alpha_r\indicator{b_i=r} - \tau_{k}]\indicator{z_{i}=k}\\
    \end{array}
\right).
\end{multline}

The FWL theorem is used to switch this out for the pair of systems of
equations $0 = \sum_{i}\tilde{\phi}(\tilde{\mathbf{y}}_{i}; \mathbf{p})$ and
$0= \sum_{i}\upsilon(\tilde{\mathbf{y}}_{i}; \tau, \mathbf{p},
\beta)$,  where
\begin{align}
  \tilde{\phi}(\tilde{\mathbf{y}}; \mathbf{p}) &=
                                                 \indicator{i \in \bigcup \mathcal{Q}} \owt{} \times \nonumber \\
  &\operatorname{vec}\left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}&\ldots&(\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
                                                   \\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}&\ldots&(\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                                   
                                                 \end{array}
  \right)%
  \right) ;\nonumber \\
  \upsilon(\tilde{\mathbf{y}}; \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt{} \nonumber \\
&\left(
                                             \begin{array}{l}                                               
           [y_{i}[1]
                                               -
                                               g(\vec{x}_{i}\beta)-\tau_{1}](
                                               \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
                  \vdots \\
{}           [y_{i}[k]
                                               - g(\vec{x}_{i}\beta) -
                                               \tau_{k}](\indicator{z_{i}=k}
                                               - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\                                             \end{array}
\right) . \label{eq:upsilondef}
\end{align}
What the FWL theorem says is that this maneuver gets us to the same
place, with regard to $\tau$, as does including the block-specific
intercepts $\alpha_{r}$.

The $\mathbf{p}$ being another nuisance parameter estimated in a
preliminary regression, we'll get variance propagation for it and the
covariance model by setting up our
\texttt{sandwich::estfun.DirectAdjusted()} to return
\begin{equation}\label{eq:12}
  \Upsilon -
  \left(\frac{n}{n_{\mathcal{C}}}\right)^{1/2}\Phi
  \hat{A}_{11}^{-1}\hat{A}_{21}' - \tilde{\Phi}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}', 
\end{equation}
where $\Upsilon =
[\upsilon(\tilde{\mathbf{y}}_{i}; \mathbf{p}, \beta, \tau) : i]'$,
$\tilde{\Phi}= [\tilde{\phi}(\tilde{\mathbf{y}}_{i}; \mathbf{p}): i]'$, 
$A_{\mathbf{p}, \mathbf{p}} = n^{-1}\sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E} [\nabla_{\mathbf{p}}\tilde{\phi}(\tilde{\mathbf{Y}}_{i};
\mathbf{p})]$, and $A_{\tau, \mathbf{p}} = n^{-1} \sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E}[\nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{Y}}_{i};
\mathbf{p}, \beta, \tau)]$.

Barring algebra mistakes on my part in unpacking these formulas, $A_{\mathbf{p}, \mathbf{p}}$ is
an average of diagonal matrices $\nabla_{\mathbf{p}}\tilde{\phi}(\tilde{\mathbf{Y}}_{i};
\mathbf{p})$ with $-1$s for diagonal entries $(b_{i}-1)s+1, \ldots,
b_{i}s$ and zeros in diagonal entries $r+1, \ldots, r+k$, each
$r \in \{1, \ldots, s\}\setminus \{b_{i}\}$, while $A_{\tau,
  \mathbf{p}}$ is a scaled sum of $\mathbb{E} \nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{Y}}_{i};
\mathbf{p}, \beta, \tau)$ terms, $i \in \bigcup \mathcal{Q}$, each equal to $\nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{y}}_{i};
\mathbf{p}, \beta, \tau)$ in the design-based scenario if not the
model-based scenario.  In either scenario, the average $A_{\tau,
  \mathbf{p}}$ of expected contributions $\mathbb{E} \nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{Y}}_{i};
\mathbf{p}, \beta, \tau)$ is estimable by corresponding means of
random variables $\nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{Y}}_{i};
\mathbf{p}, \beta, \tau)$.  The sample observation $i$ contribution $\nabla_{\mathbf{p}}\upsilon(\tilde{\mathbf{y}}_{i};
\mathbf{p}, \beta, \tau)$ is the product of the $k \times k$
diagonal matrix with entries $y_{i}[j] - g(\vec{x}_{i}\beta) -
\tau_{j}$, $j=1, \ldots, k$, with a $k \times (ks)$ block matrix
consisting of the opposite of the $k\times k$ identity at the
$b_{i}$th block and the 0 matrix for the $r$th $k \times k$ block whenever
$r\neq b_{i}$.  In particular, the product
$A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}'$ at right
of \eqref{eq:12} turns out the same under model- and design-based
interpretations of the expected value operation.

For a subgrouping variable with $m$ levels,
we can also absorb in main effects of these levels by adding another
$m$ parameters and complicating the $\tilde{\phi}$ definition accordingly. 


\subsection{Notes regarding design-based variance calculations}

\st{Potentially helpful} probably unhelpful observation:
% I updated these expressions to use random weights without
% stopping to consider whether the original idea properly allowed
% for the weights to depend on $Z_{i}$.
design-based covariances satisfy
\begin{align*}
  \operatorname{Cov}[&n^{-1}\sum_{i}\upsilon(\tilde{\mathbf{Y}}_{i};
  \mathbf{p}, \beta, \tau)] \\
                     &=   \operatorname{Cov}\left[n^{-1}\sum_{i \in \bigcup \mathcal{Q}}
                       \owt[Z_{i}]
\left(
                                             \begin{array}{l}                                               
           [y_{i}[1]
                                               -
                                               g(\vec{x}_{i}\beta)-\tau_{1}](
                                               \indicator{Z_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
                  \vdots \\
{}           [y_{i}[k]
                                               - g(\vec{x}_{i}\beta) -
                                               \tau_{k}](\indicator{Z_{i}=k}
                                               - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\                                             \end{array}
\right) \right]\\
                     &\stackrel{\text{sometimes}}{=}  \operatorname{Cov}\left[n^{-1}\sum_{i \in \bigcup \mathcal{Q}}
\owt[Z_{i}]
\left(
                                             \begin{array}{l}                                               
           [y_{i}[1]
                                               -
                                               g(\vec{x}_{i}\beta)-\tau_{1}]
                                               \indicator{Z_{i}=1}\\
                  \vdots \\
{}           [y_{i}[k]
                                               - g(\vec{x}_{i}\beta) -
                                               \tau_{k}]\indicator{Z_{i}=k}\\                                             \end{array}
\right)
\right].
\end{align*}
This is because the centering values
$\sum_{r=1}^{s}p_{jr}\indicator{b_{i}=r}$ are nonrandom from the
design-based perspective, as are the residual-like%
\footnote{Recall that the actual residuals would be $y_{i}[j] -
  g(\vec{x}_{i}\beta) - \tau_{j} - \sum_{r=1}^{s}\alpha_{r}\indicator{b_{i}=r}$, not $y_{i}[j] - g(\vec{x}_{i}\beta) - \tau_{j}$.}
quantities
$y_{i}[j] - g(\vec{x}_{i}\beta) - \tau_{j}$ that they get
multiplied by.   If the weights are flat, or inverse probability
weights, or inverse assignment odds weights, then weighted sums of
these quantities will be design constants.  Unfortunately, however, if
the weights also involve weighting factors arising from the data,
e.g. cluster sizes, then the corresponding weighted sums need no
longer be design constants.

Similar considerations yield
\begin{align}
\operatorname{Cov}_{d}[
  &n^{-1}\sum_{i}\tilde{\phi}(\tilde{\mathbf{Y}}; \mathbf{p})] \nonumber\\
  =&\operatorname{Cov}_{d}[n^{-1}
                                                 \sum_{i \in \bigcup
     \mathcal{Q}} \owt[Z_{i}] \times \nonumber \\
&  \operatorname{vec}\left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   (\indicator{Z_{i}=1}-p_{11})\indicator{b_{i}=1}&\ldots&(\indicator{Z_{i}=1}-p_{1s})\indicator{b_{i}=s}\\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{Z_{i}=k}-p_{k1})\indicator{b_{i}=1}&\ldots&(\indicator{Z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                                   
                                                 \end{array}
  \right)%
  \right)
  \Biggr] \nonumber \\ 
  \stackrel{\text{sometimes}}{=}&\operatorname{Cov}_{d}[n^{-1}
                                                 \sum_{i \in \bigcup
     \mathcal{Q}} \owt[Z_{i}] \times \nonumber \\
  &\operatorname{vec}\left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   \indicator{Z_{i}=1}\indicator{b_{i}=1}&\ldots&\indicator{Z_{i}=1}\indicator{b_{i}=s}\\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   \indicator{Z_{i}=k}\indicator{b_{i}=1}&\ldots&\indicator{Z_{i}=k}\indicator{b_{i}=s}\\                                                   
                                                 \end{array}  
  \right)%
  \right)
    \Biggr] \label{eq:11}
\end{align}
Can we also represent estimating functions
$\phi(\tilde{\mathbf{Y}}_{i}; \beta)$ in form
$b(\mathbf{x}_{i}) + \sum_{j=0}^{k}c_{j}(y_{i}[j], \vec{x}_{i}, \beta)\indicator{Z_{i}=j}$, after conditioning
on potential outcomes? If so, then as a consequence we can think of
\eqref{eq:10} as having rows that are also of this form.
\eqref{eq:11} says $\tilde{\phi}(\tilde{\mathbf{Y}}_{i}; \mathbf{p})$
has this form, so if we can get represent
$\phi(\tilde{\mathbf{Y}}_{i}; \beta)$ in this way then \eqref{eq:12}
will have this form as well. That strikes me as likely to facilitate
derivation, 
presentation in print and coding in R of these design-based covariances. 

To do:
sketch out $\phi()$'s under different covariance adjustment scenarios:
covariance model fit to quasiexperimental controls but no other
observations in the quasiexperimental sample; covariance model fit to
entire quasiexperimental sample, without a contribution from $Z$;
covariance model fit to entire quasiexperimental sample, with linear
contribution from $Z$; covariance model fit to entire
quasiexperimental sample with a $Z$ term and also $Z$-interactions
with covariates.

\nobibliography{estfun_DA}
\end{document}
\subsubsection{Alt def of M's}
\[
  \begin{array}{cc}
    M_{11}  = n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\tilde{\mathbf{Y}}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i};
                                                                  \beta
                                                                  ), \sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi'(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = n^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
With these definitions,
\[
  \operatorname{Cov}(\hat\tau) \approx n^{-1}A_{22}^{-1}\{M_{22} - [M_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}M_{21}] + (n/n_C)A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]


We can express HC0 type estimators with matrix rather than summation notation, using $F$ for the \{0,1\}-valued $n
\times m$ matrix with an indicator column for each cluster.  
\[
  \begin{array}{cccc}
  \hat{M}_{11} &=n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} &= n_{C}^{-1}(F'\Phi)(F'\Psi)\\
    \hat{M}_{21} &=\hat{M}_{12}' & \hat{M}_{22}&= n^{-1}(F'\Psi)(F'\Psi)\\
    \end{array}
\]




\section{Communicated in ``Josh/Xinhe Meeting Notes," Nov 30 2022
  post-mtg note}
There are random estimating functions $n_{\mathcal{C}}^{-1}\sum_{i\in\mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta)$  and 
       $n_{\mathcal{Q}}^{-1}\sum_{i\in
         \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta)$,
       $\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta) =
       \owt{}[y_{i} - \tau(z_{i} - \bar{z}) -
       g(\vec{x}_{i}\beta)](z_{i} - \bar{z})$ where $\bar{z}$ solves
       \begin{equation}
         \label{eq:4}
         0=\sum_{i \in \mathcal{Q}}w_{i \mathcal{Q}}(z_{i} - \bar{z}),
       \end{equation}
       inducing corresponding estimating equations
\[ \begin{array}{c} 0 \\ 0 
   \end{array} = \left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\tilde{\mathbf{y}}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\tilde{\mathbf{y}}_{i}; \tau, \alpha, \beta )
     \end{array}
\right).
\]

We seek to represent $\sum_{i\in
         \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta)$ as a
       linear combination of
       \begin{align}
         \label{eq:1}
         \sum_{i \in \mathcal{Q}}\psi_{0i} =& \sum_{i \in
                                             \mathcal{Q}}\owt{}[y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                             \rho_{0}](1-z_{i})\\
         \label{eq:2}
         \sum_{i \in \mathcal{Q}}\psi_{1i} =& \sum_{i \in
                                             \mathcal{Q}}\owt{}[y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                              \rho_{1}]z_{i}\\
         \label{eq:3}
         &\rho_{1} - \rho_{0} - \tau.
       \end{align}

       If my algebra is OK, we have
       \begin{equation*}
         \sum_{i\in
         \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta) =
       \sum_{i \in \mathcal{Q}}\psi_{0i} -  \bar{z}\sum_{i \in \mathcal{Q}}(\psi_{0i} + \psi_{1i})
     \end{equation*}
     by some algebra including \eqref{eq:4}.  I had to call time
     before seeing about whether and how to accommodate scenarios with
     strata being absorbed into the treatment variable; there are at
     least a few additional wrinkles lurking there. 
 

M-estimation's characteristic covariance approximation is  
\[ \operatorname{Cov}([\hat\beta',\hat\tau']) \approx
  A^{-1} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta )
     \end{array}
\right)A^{-1}
\]

The right-hand side covariance isn't quite a constant multiple of $B$ as in previous comments here (in particular with scaling factor $n_{\mathcal{Q}\mathcal{C}}^{-1}$ on off-diagonals).  Rather, with $B$ as defined there,
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\tilde{\mathbf{Y}}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\tilde{\mathbf{Y}}_{i}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}B_{11}& \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{12}\\
    \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{21} &
                                                                              n_{\mathcal{Q}}^{-1}B_{22}
  \end{array}
\right).
\]

From this I get
\[
  \operatorname{Cov}(\hat\tau) \approx n_Q^{-1}A_{22}^{-1}\{B_{22} - (n_{CQ}/n_C)[B_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}B_{21}] + (n_Q/n_C)A_{21}A_{11}^{-1}B_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]
(with $A$s and $B$s as defined in earlier comments in the thread).
