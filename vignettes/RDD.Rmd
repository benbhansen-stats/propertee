---
title: "Regression Discontinuity Designs"
author: "Adam Sales"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Regression Discontinuity Designs}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r}
library(flexida)
```


## Data and Design

The data were compiled by Ludwig & Miller (2007; henceforth LM), largely from the National Archives and Records Administration and Vital Statistics, and were cleaned and provided by the `RDHonest` package for R.

At the onset of the Head Start preschool program in 1965, the Office of Economic Opportunity provided technical support in applying for funding to the 300 US counties with the highest 1960 poverty rates. LM show that this assistance paid off, in that couties with poverty rates just above the cutoff received substantially more Head Start funding than those with poverty rates just below the cutoff. LM then sought to estimate the impact of this additional funding on two county-level outcomes that may plausibly affected by Head Start: child mortality due to health risks addressed by the Head Start program, and high school completion rates. 


The variable `povrate60` encodes county-level poverty rates, as of 1960, relative to the 300th poorest county (Campbell County, TN).
`mortHS` records 1973-1983 county-level child mortality rates for causes targeted by Head Start, and `highSchool` records 1990 high school graduation rates for adults ages 18-24, LM's two main outcomes.

This figure plots each outcome as a function of county-level 1960 poverty rates. The cutoff for assistance, between the 300th and 301st poorest counties, is denoted with a dotted line. Darker points are binned means and lighter points are the raw data.
```{r rddPlots,fig.width=7}
## headst%>%
##     filter(abs(povrate60)<36)%>% ## LM's biggest bandwidth
##     filter(mortHS<15)%>% ## remove outliers
##     mutate(Z=ifelse(povrate60>=0,"Assistance","No Assistance"))%>% ## the "treatment"--assistance in applying for funding
##     pivot_longer(cols=c(mortHS,highSchool),names_to="Outcome")%>%
##     ggplot(aes(povrate60,value,color=Z))+
##     stat_summary_bin(geom="point",fun="mean",bins=50)+
##     geom_point(alpha=0.05)+
##     facet_wrap(~Outcome,scales="free_y")+
##     geom_smooth()+
##     geom_vline(xintercept=-0.005,linetype="dotted")+
##     theme(legend.title=element_blank(),legend.pos='top')
```

## Background

### Regression Discontinuity Designs

What characterizes discontinuity designs (RDDs), including the LM study, is that treatment is assigned as a function of a numeric "running variable" $R$, along with a prespecified cutoff value $c$, so that treatment is assigned to subjects $i$ for whom $R_i>c$. 
If $Z_i$ denote's $i$'s treatment assignment, we write $Z_i=\mathbf{1}\{R_i>c\}$, where $\mathbf{1}\{x\}$ is the indicator function--equal to 1 when $x$ is true and 0 otherwise. 
In the LM study, $i$ indexes counties, each county's 1960 poverty rank is the running variable, and the treatment under study is assistance with application for Head Start funding. Then $Z_i=\mathbf{1}\{rank(povrate60)>n-300\}$, where $n$ is the total number of counties.
(In "fuzzy" RDDs, $R$'s value relative to $c$ doesn't completely determine treatment, but merely affects the probability of treatment, so subjects with $R_i>c$ are more likely to be treated than those with $R_i<c$; in fuzzy RDDs, $Z_i=\mathbf{1}\{R_i>c\}$ is typically modeled as an instrument for treatment receipt.)

RDDs hold a priviledged place in causal inference, because unlike most other observational study designs, the mechanism for treatment assignment is known. 
That is, $R$ is the only confounder. 
On the other hand, since $Z$ is completely determined by $R$, there are no subjects with the same values for $R$ but different values for $Z$, so common observational study techniques, such as matching on $R$,  are impossible. 
Instead, it is necessary to adjust for $R$ with modeling--typically regression. 


### Analyzing RDDs with ANCOVA

Traditionally, the typical way to analyze data from RDDs is with ANCOVA, by fitting a regression model such as
$$Y_i=\beta_0+\beta_1R_i+\beta_2Z_i+\epsilon_i$$
where $Y_i$ is the outcome of interest measured for subject $i$, and $\epsilon_i$ is a random regression error. 
Then, the regression coefficient for $Z$, $\beta_2$, is taken as an estimate of the treatment effect. 
Common methodological updates to the ANCOVA approach include an interaction term between $Z_i$ and $R_i$, and the substitution semi-parametric regression, such as local linear or polynomial models, for linear ordinary least squares.

Under suitible conditions, the ANCOVA model is said to estimate a "Local Average Treatment Effect" (LATE). 
If $Y_1$ and $Y_0$ are the potential outcomes for $Y$, then the LATE is defined as
$$\displaystyle\lim_{r\rightarrow c^+} E(Y_1|R=r)-\displaystyle\lim_{r\rightarrow c^-} E(Y_0|R=r)$$
or equivalently
$$\displaystyle\lim_{\Delta\rightarrow 0^+} E(Y_1-Y_0 |R\in (c-\Delta,c+\Delta))$$
where $E$ denotes expectation--that is, the LATE is the limit of average treatment effects for subjects with $R$ in ever-shrinking regions around $c$. 

Among other considerations, the LATE target suggests that data analyists restrict their attention to subjects with $R$ falling within a bandwidth $b>0$ of $c$, e.g. only fitting the model to subjects $i$ with $R_i\in (c-b,c+b)$.
A number of methods have been proposed to select $b$, including cross validation, non-parametric modeling of the second derivative of $f(r)=E(Y|R=r)$, and specification tests. 

### The `flexida` Approach to RDDs

The `flexida` approach to RDDs breaks the data analysis into three steps:

1. Conduct specification tests and choose a bandwidth $b>0$, along with, possibly, other data exclusions, resulting in an analysis sample $W$.. 
2. Fit a covariance model $Y_{i0}=g(R_i,x_i;\beta)$, modeling $Y_{i0}$ as a function of running variable $R_i$ and (optionally) other covariates $\mathbf{x}_i$. Fitting a covariance model to only the control subjects, as in other designs, would entail extrapolation of a model fit to subjects with $R_i\in (c-b,c)$ to subjects with $R_i\in (c,c+b)$, which is undesirable. Instead, we fit the covariance model to the full analysis sample $W$, including both treated and untreated subjects. However, since we are interested in modeling $Y_0$, and not $Y_1$ or $Y$, so rather than fitting $g(\cdot)$, we fit an extended model 
$$\tilde{g}(R_i,x_i,Z_i;\beta,\gamma)=g(R_i,x_i;\beta)+\gamma Z_i$$ 
including a term for treatment assignment. The estimate for $\gamma$ is, in essence, a provisional estimate of the treatment effect.
3. Let $\hat{Y}_{i0}=g(R_i,x_i;\hat{\beta})$, using only the model for $Y_0$---not including the term for $Z$---along with $\hat{\beta}$ estimated in step 2. Then estimate the average treatment effect for subjects in $W$ using the difference in means estimator:
\[
d_{\hat{\beta}}=\frac{\sum_{i\in W} Z_i(Y_i-\hat{Y}_{i0})}{\sum_{i \in W} Z_i}-\frac{\sum_{i\in W} (1-Z_i)(Y_i-\hat{Y}_{i0})}{\sum_{i \in W} (1-Z_i)}
\]

The estimator $d_{\hat{\beta}}$ will be consistent if the model $g(R_i,x_i;\beta_0)$, with $\beta_0$ as the probability limit for $\hat{\beta}$, successfully removes all confounding due to $R$, i.e. $Y_0-g(R_i,x_i;\beta_0) \perp \!\!\! \perp Z$ for $i\in W$. 





