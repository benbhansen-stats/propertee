---
title: 'Enumeration of data objects required for standard errors of prior covariance model-assisted effect estimation'
author: 'Ben Hansen'
output: rmarkdown::html_vignette
bibliography: bbhrefs.bib
vignette: >
  %\VignetteIndexEntry{Enumeration of data objects required for standard errors of prior covariance model-assisted effect estimation}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
date: 'April 2022'
---

## Context

Suppose an estimator chain beginning with fitting a covariance model,
`covmod` say, to a covariance sample, and ending with a model along the
lines of `lm(promotion ~ treat, data=qe_spl, weights=ate(des),
offset=cov_adj(covmod))` or `lm(promotion ~ treat*demographics,
data=qe_spl, weights=ate(des), offset=cov_adj(covmod))` --- a *direct
adjustment* model.  (Much or all of what's said here may apply also
with models of more general forms, for example `lm(promotion ~
treat + cov_adj(covmod), data=qe_spl, weights=ate(des))` or
`lm(promotion ~ treat * cov_adj(covmod), <...>)`. But for now our focus
is covariance modeling followed by direct adjustment.)  The covariance
sample and quasiexperimental sample may be disjoint, identical or
overlapping.  This note describes computational infrastructure for
handling the second two cases, where errors from the fitting of the
covariance model need to be propagated into standard errors reported
for `treat` coefficients.

The direct adjustment model may be fit in a different context than the
covariance model, using only the artifacts of that modeling that are
stored in `cov_adj(covmod)`.  We determine this list, but we want it
to be a minimal list, for storage, privacy and regulatory reasons. If
the units of assignment are clusters of students or patients, then
where feasible the result of `cov_adj(covmod)`/`cov_adj(covmod,
<...>)` should contain only elements that are aggregated to that
level.


## Proposal

(Class & function names are tentative.)

<!-- couldn't get the below to render properly--> 
```{r classdiagram, eval=FALSE, echo=FALSE, fig.height=6, fig.width=6}
DiagrammeR::mermaid("
classDiagram
  VeganLayerKit <|-- SandwichLayerKit
  VeganLayerKit <|-- VeganLayer
  SandwichLayerKit <|-- SandwichLayer
  numeric <|-- SandwichLayer
  numeric <|-- VeganLayer

  VeganLayerKit: +character covariance_sample_keys
  VeganLayerKit: +double estfun_matrix
  VeganLayerKit: +double bread_matrix
  VeganLayerKit: +function predict_with_gradient
  VeganLayerKit: +add_meat()

  SandwichLayerKit : +double meat_matrix

  VeganLayer : +double .Data
  VeganLayer : +double prediction_gradient

  SandwichLayer : +double .Data
  SandwichLayer : +double prediction_gradient
")
```
<!-- Rendering from https://mermaid.live/ --> 
![](images/sandwich_layers.png)

- S4 classes `SandwichLayer` and `VeganLayer`, carrying certain extracts of the fit of a linear or asymptotically linear model.

  When the predictions of this model, model A, are used as independent variables in the fitting of a subsequent linear or asymptotically linear model, model B, objects and data carried in the `SandwichLayer` derived from model A will suffice to estimate the variance of errors of estimation in model B's parameters that is propagated down from estimation of model A. For variance propagation under the design-based perspective, it suffices to use a `VeganLayer`, i.e. a `SandwichLayer` minus the meat.^[A `SandwichLayer` object deriving from model A carries estimates of model A's "bread" and "meat" matrices, along with additional material relating to predictions and estimating functions of model A that will serve to connect it to the next "deck" of a multi-decker sandwich.  `VeganLayer` has all of this minus the meat. See detail below.]

  These classes extend the base numeric vector type, with numeric vectors (of predictions) in their `@.Data` slots.  Their `@.Data` and their `@prediction_gradient` slots are not shared with the related classes `SandwichLayerKit` and `VeganLayerKit`, although the rest of their slots are so shared.

- S4 classes `SandwichLayerKit` and `VeganLayerKit`, containing materials sufficient to generate a `SandwichLayer` or `VeganLayer` when supplied with a data set.

  These classes lack `@.Data` and `@prediction_gradient` slots, although otherwise they carry the same material as is carried in a {`Sandwich`/`Vegan`}`Layer`. (Instead they have an `@predictor` slot for a function to generate an `@.Data` vector and an `@prediction_gradient` matrix, from a supplied data frame.)

- S3 generic makers of `VeganLayer`s and `VeganLayerKit`s, with methods for `glm`, ...
- A separate `add_meat()` function accepting a `glm` or other fitted model and a `VeganLayer` or `VeganLayerKit`, transforming the `VeganLayer` or `VeganLayerKit` into a `SandwichLayer` or `SandwichLayerKit`.  
- A maker of `SandwichLayer` and `VeganLayer` objects, a function of the {`Sandwich`/`Vegan`}`LayerKit` and a data frame carrying variables needed for prediction from model A.

  This presumes that the {`Sandwich`/`Vegan`}`LayerKit` carries what's needed to generate model A's predictions for a new data set.^[Predictions from model A aren't themselves needed to connect model A's sandwich layer to the next deck of the sandwich.  But their derivatives (with respect to each of model A's parameters) are required, and once we're taking responsibility for those the predictions themselves aren't much more to add.  Plus, by bundling these in the {`Sandwich`/`Vegan`}`LayerKit` without also encasing microdata in there, storing a {`Sandwich`/`Vegan`}`LayerKit` becomes a means of storing what's needed for predictions from model A while respecting privacy and data sharing agreement constraints.]

- `cov_adj()` invokes the maker function described at the last step, returning a {`Sandwich`/`Vegan`}`Layer`, the predictions of which are on the response scale.  


As a result, in the model frame stored with model B, the column of predictions deriving from model A has type {`Sandwich`/`Vegan`}`Layer`, and carries the needed info from fitting of model A to calculated sandwich SEs for the multi-decker sandwich.  Two more thing will facilitate folding our calculations into existing R workflows:

- `as.multi_decker_lm()`, a conversion function accepting an object `x` inheriting from `lm`.

   This identifies {`Sandwich`/`Vegan`}`Layer` objects among its predictor variables, assembling the associated {`Sandwich`/`Vegan`}`Layer` objects into a list that's tacked on to `x`.  The class attribute ot `x` is then prepended with `multi_decker_lm`, and the result is returned.
- Method `vcov.multi_decker_lm()`


## Basis in known extensions of Huber-White setup to chained estimators

With reference to the formulas for stacked estimating equations of
Carroll, Ruppert, Stefanski and Crainiceanu (2006, p.373), the
covariance model has psi functions (estimating equations) $\phi(\tilde{\mathbf{Y}},
\alpha)$ with parameters $\alpha$, and Fisher information and estimating-equation covariance matrices $A_{11}$ and $B_{11}$ respectively; while the direct adjustment model's
are $\psi(\tilde{\mathbf{Y}}, \tau, \alpha)$, the `treat` coefficients
being $\tau$, with sandwich components $A_{22}$ and $B_{22}$. (The symbols "$\phi()$", "$\psi()$" and "$\alpha$" are
used as Carroll et al use them, while our "$\tau$" corresponds to
their "$\mathcal{B}$".)

The leading $n^{-1}$ factor should be struck from the expression for
$\mathrm{var}(\hat{\mathcal{B}})$.  We take $i$ to range over the
units of assignment (clusters) not elements^[We'll need to aggregate
(by summing) the elementwise estimating equation contributions to the
cluster level, to form cluster-wise estimating equation
contributions.]. Carroll et al's formulas for $A_{11}$, $A_{21}$ and
$A_{22}$ then apply, although design-based standard errors call for
different calculations of $B_{11}$, $B_{12}$ and $B_{22}$.  Denote the
clusters/units of assignment that are represented in covariance and
quasiexperimental samples by $\mathcal{C}$ and $\mathcal{Q}$
respectively.

## Required materials for SE calculations

To estimate variances and covariances of $\tau$, we'll need to assemble the following materials. 

1. Sufficient information about $\mathcal{C}$ and $\mathcal{Q}$ to identify their intersection $\mathcal{C}\cap\mathcal{Q}$, as is needed to estimate $B_{21}$; 
2. Vectors of estimating
functions $\{\phi(\tilde{\mathbf{Y}}_i; \hat{\alpha}): i \in \mathcal{C}\cap\mathcal{Q}\}$
and $\{\psi(\tilde{\mathbf{Y}}_i, \hat{\tau}, \hat{\alpha}): i \in \mathcal{C}\cap\mathcal{Q}\}$, as are needed for $B_{21}$; 
3. For the quasiexperimental sample $\mathcal{Q}$, matrices
$$\nabla_{\alpha} \{\psi(\tilde{\mathbf{Y}}_i, \hat{\tau}, {\alpha}):
i \in \mathcal{Q}\} \vert_{\alpha=\hat\alpha},$$ corresponding to
$A_{21}$;
4. Estimates of the direct adjustment model's A matrix $n A_{22} =
\nabla_\tau \sum_{i \in \mathcal{Q}}
\mathbb{E}[\psi(\tilde{\mathbf{Y}}_i, \tau, \hat{\alpha})]
\vert_{\tau=\hat\tau}$, i.e. its unscaled Fisher information
w.r.t. $\tau$ only, and unscaled B matrix $n B_{22} =
\mathrm{Cov}[\sum_{i \in \mathcal{Q}} \psi(\tilde{\mathbf{Y}}_i,
{\tau}, {\alpha})]_{({\tau}, {\alpha})=(\hat{\tau}, \hat{\alpha})}$;
5. The covariance model's observed information matrix/empirical A
matrix estimate $n \hat{A}_{11} = \sum_{i \in \mathcal{C}}
\nabla_\alpha [\phi(\tilde{\mathbf{Y}}_i;
{\alpha})]_{\alpha=\hat\alpha}$; and
6. for covariance estimation in the conventional "model-based" setup
only, estimates of the covariance model's B matrix $nB_{11} =
\mathrm{Cov}[\sum_{i \in \mathcal{C}}\phi(\tilde{\mathbf{Y}}_i;
{\alpha})]_{\alpha=\hat\alpha}$ (a "clustered" covariance estimate).

In (5), observed information is preferred to "observed expected"
information, $\sum_{i \in \mathcal{C}} \nabla_\alpha \mathbb{E}
[\phi(\tilde{\mathbf{Y}}_i; {\alpha})]_{\alpha=\hat\alpha}$, because
observed information is agnostic as to whether expectation is
calculated with conditioning on potential outcomes, ie the finite
population perspective, or with conditioning on treatment assignment,
the model based perspective. In the special case of quantile regression^[Strictly speaking, the estimating equations of a quantile regression aren't differentiable in $\alpha$. All directional derivatives will exist, and I'm expecting this to be enough for our purposes, but I haven't thought it through.], observed information isn't ordinarily used in standard error calculations, and it may take some doing to get.

Regarding (6), $B_{11}$ is not needed for design-based standard
errors, as in this setting observations outside of the
quasiexperimental sample do not contribute to the covariance model's B
matrix.  Only quasiexperimental sample observations do, and we'll have
access to these when the direct adjustment model is fit. As we also
have $\{\phi(\tilde{\mathbf{Y}}_i; \hat{\alpha}): i \in \mathcal{Q}\}$
and $\{\psi(\tilde{\mathbf{Y}}_i, \hat{\tau}, \hat{\alpha}): i \in
\mathcal{Q}\}$, we can use these materials to estimate $B_{12}$ and
$B_{22}$.

## Software implementation comments on 1--6 above, including contents of {`Sandwich`/`Vegan`}{`Layer`/`LayerKit`} objects

1\. The {`Sandwich`/`Vegan`}{`Layer`/`LayerKit`} classes carry an `@estfun_matrix` slot for a matrix of estimating function contributions, $\{\phi(\tilde{\mathbf{Y}}_i; \hat{\alpha}): i \in \mathcal{C}\}$ (rows being sums of elementwise contributions).  These contributions correspond either to members of $\mathcal{C}$ or, in cases where more than one of those have been associated with single unit of assignment described in the Design, with a unit of assigment described in the Design. 

2\. A {`Sandwich`/`Vegan`}{`Layer`/`LayerKit`} object carries a `covariance_sample_keys` data frame with which to identify rows of the `estfun_matrix` with units of assignment (that may potentially be described in a Design object.  The association can be many-one (but not one-many); it is not required that named units appear in the design. As `covmod` itself won't be aware of these cluster associations, assembling this info at runtime calls for trickery, as well as a means for users to override the trickery and directly provide key variables that the design will need. An NA in `covariance_sample_keys` indicates a unit not appearing in the Design. 

3\. The {`Sandwich`/`Vegan`}`Layer` class differs from {`Sandwich`/`Vegan`}`LayerKit` not ony with the inclusion of an `@.Data` slot carrying an numeric vector of predictions, but also an `@prediction_gradient` slot for a numeric matrix.  This matrix has as many rows as there are entries in the `.Data` vector, and as many columns as there are estimating equations.

The `@prediction_gradient` slot carries $\{\nabla_\alpha f_{\alpha}(\tilde{\mathbf{Y}}_i)|_\alpha=\hat\alpha, i \in \mathcal{Q}\}$, where $f_\alpha(\mathbf{Y})$ represents the prediction for data $\mathbf{Y}$ from a fitted model of `class(cov_mod)` with parameters $\alpha$. For $\psi()$'s that use only "predictions" of the covariance model, as ours does, the first derivative of the covariance model predictions as applied to data in $\mathcal{Q}$ will provide sufficient information from the covariance model to complete the calculation of $\{\nabla_{\alpha}  \psi(\tilde{\mathbf{Y}}_i, \hat{\tau}, {\alpha}) \vert_{\alpha=\hat\alpha}: i \in \mathcal{Q}\}$. 

(To calculate these gradients from what's carried in a {`Sandwich`/`Vegan
`}`LayerKit`, there are two options.

a. Assume that the function in the `@predictor` slot of the `LayerKit` furnishes not only model predictions but also first partial derivatives w.r.t. variables figuring in the prediction.  
b.  Don't expect partial derivatives from the `@predictor` function, but do expect it to be fast, and use it to figure numerical derivatives of the prediction.

I understand that `geex` does (b).  I'm attracted to (a).  The two approaches aren't necessarily incompatible, in that one could have dedicated `scores()` methods for specific model classes plus a generic method that figures the derivatives numerically.)

4\. The {`Sandwich`/`Vegan`}{`Layer`/`LayerKit`} classes aren't implicated in (4).  We can take extant calculations of a direct adjustment model's information matrix, with the proviso that we keep track of whatever scaling those calculations may have applied. For design-based SEs we'll need our own B matrix calculation.  For model-based SEs we can plug in to extant routines for $B_{22}$ also, but being careful ensure clustering on the units of assignment (as named in a Design object). Scaling of these matrices should default to the conventions of the sandwich package. (I haven't considered whether use of HC0--3 etc for $B_{22}$ calls for corresponding adjustment to estimation of $B_{21}$ and/or $B_{11}$, nor whether heuristics animating these adjustments make sense in this context.)

5\. {`Sandwich`/`Vegan`}{`Layer`/`LayerKit`} classes carry a `bread_matrix` slot for the inverse of the covariance models's observed information matrix^[As extracted by `sandwich::bread()`.].  The rows and columns of this matrix align with the columns of the `@estfun_matrix`.

We can take extant calculations of a covariance model's information matrix, defaulting to scaling conventions implemented in the sandwich package. 

6\. The `SandwichLayer` class carries a slot for an estimated B ("meat") matrix.  `VeganLayer` does not carry this slot, and that's the whole of the distinction between the `VeganLayer` and `SandwichLayer` classes.

We can use extant routines for calculation of a covariance model's B matrix, with the provisos that we keep track of whatever scaling those calculations may have applied and we look to the study design for clustering.  (I haven't considered whether use of HC0--3 etc for $B_{11}$ calls for corresponding adjustment to estimation of $B_{21}$ and/or $B_{22}$, nor whether heuristics animating these adjustments make sense in this context.)

## References
