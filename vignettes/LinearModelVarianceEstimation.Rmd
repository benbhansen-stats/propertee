---
title: "Variance Estimation Testing Using A Simple Linear Specification"
author: "Mark Fredrickson"
date: '2022-05-09'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The goal of this document is to consider a simple model that can be arranged as two separate regressions and relate the variance of the second stage coefficients to those that are fit in the entire model. With this information, we should be able to create tests of our variance estimation routines.

## Combined model 

Consider two sets of background variables and treatment assignments, $x$ and $z$ with dimension $p_x$ and $p_z$, respectively. While we allow correlation between the variables, we will assume that the matrix $\begin{pmatrix} X & Z \end{pmatrix}' \begin{pmatrix} X & Z \end{pmatrix}$ has an inverse (I think most of these results would work with generalized inverses, but if does not exist it makes the dimesionality of the coefficients a little tricky).

$$Y = \alpha'x + \beta'z + \epsilon, \quad E(\epsilon) = 0, \text{Var}(\epsilon) = \sigma^2$$

Standard results give us

\[
\begin{pmatrix} \hat \alpha_1 \\ \hat \beta_1 \end{pmatrix} = \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X'y \\ Z'y \end{pmatrix}
\]
and that the variance of the estimators is
$$\sigma^2 \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1}$$
Results for blocked matrices (e.g., The Matrix Cookbook) give the variance for just $\hat \beta$ as
$$\text{Var}(\hat \beta_1) = \sigma^2 \left[Z'Z - Z' X (X'X)^{-1} X'Z\right]^{-1}$$
Write $H = I - X (X'X)^{-1} X'$, matrix that creates the residuals of the regression on $x$ alone. Then,
$$\text{Var}(\hat \beta_1) = \sigma^2 \left[Z'H Z\right]^{-1}$$

## Two regressions

Let $\hat \alpha_2$ and $\hat \beta_2$ be the estimators from first regression on $Y$ on $x$ alone and then regressing $Y - \hat \alpha_2'x$ on $Z$. Again by standard results we have

$$\hat \alpha_2 = (X'X)^{-1} X' y$$
And 
$$\hat \beta_2 = (Z'Z)^{-1} Z' (y - X \hat \alpha_2)  = (Z'Z)^{-1} Z'(y - X (X'X)^{-1} X' y) = (Z'Z)^{-1} Z' H y$$
The variance of $\hat \beta_2$ would be
$$\text{Var}(\hat \beta_2) = \sigma^2 (Z'Z)^{-1} Z' H Z (Z'Z)^{-1}$$
Consider the SVD of $Z = UDV'$. Then we have
$$\text{Var}(\hat \beta_2) = \sigma^2 V D^{-2} V' V D U' H U D V' V D^{-2} V' = \sigma^2 V D^{-1} U' H U D^{-1} V' = \sigma^2 \left[Z^{-}\right]' H^{-} Z^{-} $$

Where $Z^{-}$ is the generalized inverse of $Z$ and $H$ is its own generalized inverse as it is idempotent. 

## Estimating $\sigma^2$

Define $G = I - Z(Z'Z)^{-1}Z'$

In the the first model, the usual estimator of $\sigma^2$ would be 
\[
\begin{aligned}
\hat \sigma_1^2 &= \frac{1}{n - (p_x + p_z)} (y - X \hat \alpha_1 - Z \hat \beta_1)' (y - X \hat \alpha_1 - Z \hat \beta_1) \\
&= \frac{1}{n - (p_x + p_z)} y' \left[I - \begin{pmatrix} X & Z \end{pmatrix} \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X & Z \end{pmatrix}' \right] y\\
&= \frac{1}{n - (p_x + p_z)} y' \left[I - X (X'GX)^{-1} X'G - Z(Z'HZ)^{-1} Z'H \right] y
\end{aligned}
\]

<!-- Proof of last line
\[
\begin{aligned}
a &= \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1
} \\
&= \begin{pmatrix} (X'GX)^{-1} & 0 \\ 0 & (Z'HZ)^{-1} \end{pmatrix} \begin{pmatrix} I & - X'Z (Z'Z)^{-1} \\ - Z'X (X'X)^{-1} & I \end{pmatrix}
\end{aligned}
\]

\[
\begin{aligned}
\begin{pmatrix}X & Z \\ 0 & 0 \end{pmatrix}\begin{pmatrix} (X'GX)^{-1} & 0 \\ 0 & (Z'HZ)^{-1} \end{pmatrix}
 &= \begin{pmatrix}
 X (X'GX)^{-1} & Z (Z'HZ) ^{-1} \\
 0 & 0 \end{pmatrix}
 \end{aligned}
\]

\[
\begin{pmatrix} I & - X'Z (Z'Z)^{-1} \\ - Z'X (X'X)^{-1} & I \end{pmatrix} \begin{pmatrix} X' & 0 \\ Z' & 0 \end{pmatrix} = 
\begin{pmatrix}
X'G & 0 \\
Z'H & 0
\end{pmatrix}
\]

\[
\begin{pmatrix} X & Z \end{pmatrix} \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X & Z \end{pmatrix}' =
\begin{pmatrix} X (X'GX)^{-1} X' G + Z(Z'HZ)^{-1} Z'H & 0 \\
0 & 0\end{pmatrix} = X (X'GX)^{-1} X'G + Z(Z'HZ)^{-1} Z'H
\]
-->


In the second case, the usual naive estimate of the variance that does not take into account the fact that $\hat \alpha_2$ is random would be

\[ 
\begin{aligned}
\hat \sigma_2^2  
  &= \frac{1}{n - p_z} (y - X \hat \alpha_2 - Z \hat \beta_2)' (y - X \hat \alpha_2 - Z \hat \beta_2) \\
  &= \frac{1}{n - p_z} y' (H - Z (Z'Z)^{-1} Z' H )' (H - Z (Z'Z)^{-1} Z' H) y \\
  &= \frac{1}{n - p_z} y' [(I - Z (Z'Z)^{-1} Z') H ]' (I - Z (Z'Z)^{-1} Z') H y \\
  &= \frac{1}{n - p_z} y' H G H y 
\end{aligned}
\]

## TODO

- Relate the above to estimating equations for the joint and sequential models. Chapter 7 of Boos and Stefanski's textbook contains relevant material for the combined model. The errors-in-variables textbook has the extensions for the sequential models.
- Implement as tests. See `test.cov_adj.R` for verifications of these results. 
