---
title: "Variance Estimation Testing Using A Simple Linear Specification"
author: "Mark Fredrickson, Ben Hansen"
date: '2025-11-07'
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Variance Estimation Testing Using A Simple Linear Specification}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}output: html_document
---

```{r setup, include=FALSE}
library("utils")
library("devtools")
library("testthat")
library("propertee")
library("sandwich")
```

The goal of this document is to consider a simple model that can be arranged as two separate regressions and relate the variance of the second stage coefficients to those that are fit in the entire model. With this information, we should be able to create tests of our variance estimation routines.

Consider two sets of background variables and treatment assignments, $x$ and $z$ with dimension $p_x$ and $p_z$, respectively, to be treated as nonrandom or fixed by conditioning. Our first and most important application will be to binary $z$, but $z$ representing contrasts of factor levels and interactions with binary or multinomial treatments are also of interest.


# Treatment-free covariance modeling followed by residual diff-in-diff

While we allow correlation between the variables, we will assume that the matrix $\begin{pmatrix} X & Z \end{pmatrix}' \begin{pmatrix} X & Z \end{pmatrix}$ has an inverse (I think most of these results would work with generalized inverses, but if does not exist it makes the dimesionality of the coefficients a little tricky).

$$Y = \alpha'x + \beta'z + \epsilon, \quad E(\epsilon) = 0, \text{Var}(\epsilon) = \sigma^2$$

Standard results give us

\[
\begin{pmatrix} \hat \alpha_1 \\ \hat \beta_1 \end{pmatrix} = \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X'y \\ Z'y \end{pmatrix}
\]
and that the variance of the estimators is
$$\sigma^2 \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1}$$
Results for blocked matrices (e.g., The Matrix Cookbook) give the variance for just $\hat \beta$ as
$$\text{Var}(\hat \beta_1) = \sigma^2 \left[Z'Z - Z' X (X'X)^{-1} X'Z\right]^{-1}$$
Write $H = I - X (X'X)^{-1} X'$, matrix that creates the residuals of the regression on $x$ alone. Then,
$$\text{Var}(\hat \beta_1) = \sigma^2 \left[Z'H Z\right]^{-1}$$

The same $\hat \beta_1$ arises as the coefficient of a regression of $HY$ on $HZ$ (by the so-called Frisch-Waugh-Lovell Theorem).  In consequence,
$$\hat \beta_1 = (Z'HZ)^{-1}(Z'Hy) .$$

## Diff-in-diff after covariance modeling, take 1.

Let $\hat \alpha_2$ and $\hat \beta_2$ be the estimators from first regression on $Y$ on $x$ alone and then regressing $Y - \hat \alpha_2'x$, that is $YH$ with $H$ as defined above, on $Z$. Standard results give

$$\hat \alpha_2 = (X'X)^{-1} X' y$$
and
$$\hat \beta_2 = (Z'Z)^{-1} Z' (y - X \hat \alpha_2)  = (Z'Z)^{-1} Z'(y - X (X'X)^{-1} X' y) = (Z'Z)^{-1} Z' H y$$

As both $Z$ and $X$ are taken to be nonrandom, we may pass between $\hat \beta_1$ and $\hat \beta_2$ via nonrandom linear transformations, as follows:
$$\hat \beta_1 = (Z'HZ)^{-1}(Z'Z) \hat \beta_2;\quad \hat \beta_2 = (Z'Z)^{-1}(Z'HZ) \hat \beta_1 .$$
Accordingly
$$ \operatorname{Cov}(\hat \beta_1) = (Z'HZ)^{-1}(Z'Z)
\operatorname{Cov}(\hat \beta_{2})(Z'Z) (Z'HZ)^{-1}$$
which may serve as a basis for tests.

<!-- The standard expression of $\hat \beta_2$'s variance is
$$\text{Var}(\hat \beta_2) = \sigma^2 (Z'Z)^{-1} Z' H Z (Z'Z)^{-1}.$$
Considering the SVD of $Z = UDV'$, an alternate variance expression above entails
$$\text{Var}(\hat \beta_2) = \sigma^2 V D^{-2} V' V D U' H U D V' V D^{-2} V' = \sigma^2 V D^{-1} U' H U D^{-1} V' = \sigma^2 \left[Z^{-}\right]' H^{-} Z^{-} $$

where $Z^{-}$ is the generalized inverse of $Z$ and $H$ is its own generalized inverse as it is idempotent.-->

<!-- ## Estimating $\sigma^2$

Define $G = I - Z(Z'Z)^{-1}Z'$

In the the first model, the usual estimator of $\sigma^2$ would be 
\[
\begin{aligned}
\hat \sigma_1^2 &= \frac{1}{n - (p_x + p_z)} (y - X \hat \alpha_1 - Z \hat \beta_1)' (y - X \hat \alpha_1 - Z \hat \beta_1) \\
&= \frac{1}{n - (p_x + p_z)} y' \left[I - \begin{pmatrix} X & Z \end{pmatrix} \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X & Z \end{pmatrix}' \right] y\\
&= \frac{1}{n - (p_x + p_z)} y' \left[I - X (X'GX)^{-1} X'G - Z(Z'HZ)^{-1} Z'H \right] y
\end{aligned}
\]
--> 
<!-- Proof of last line
\[
\begin{aligned}
a &= \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1
} \\
&= \begin{pmatrix} (X'GX)^{-1} & 0 \\ 0 & (Z'HZ)^{-1} \end{pmatrix} \begin{pmatrix} I & - X'Z (Z'Z)^{-1} \\ - Z'X (X'X)^{-1} & I \end{pmatrix}
\end{aligned}
\]

\[
\begin{aligned}
\begin{pmatrix}X & Z \\ 0 & 0 \end{pmatrix}\begin{pmatrix} (X'GX)^{-1} & 0 \\ 0 & (Z'HZ)^{-1} \end{pmatrix}
 &= \begin{pmatrix}
 X (X'GX)^{-1} & Z (Z'HZ) ^{-1} \\
 0 & 0 \end{pmatrix}
 \end{aligned}
\]

\[
\begin{pmatrix} I & - X'Z (Z'Z)^{-1} \\ - Z'X (X'X)^{-1} & I \end{pmatrix} \begin{pmatrix} X' & 0 \\ Z' & 0 \end{pmatrix} = 
\begin{pmatrix}
X'G & 0 \\
Z'H & 0
\end{pmatrix}
\]

\[
\begin{pmatrix} X & Z \end{pmatrix} \begin{pmatrix} X'X & X'Z \\ Z'X & Z'Z \end{pmatrix}^{-1} \begin{pmatrix} X & Z \end{pmatrix}' =
\begin{pmatrix} X (X'GX)^{-1} X' G + Z(Z'HZ)^{-1} Z'H & 0 \\
0 & 0\end{pmatrix} = X (X'GX)^{-1} X'G + Z(Z'HZ)^{-1} Z'H
\]
-->


<!-- In the second case, the usual naive estimate of the variance that does not take into account the fact that $\hat \alpha_2$ is random would be

\[ 
\begin{aligned}
\hat \sigma_2^2  
  &= \frac{1}{n - p_z} (y - X \hat \alpha_2 - Z \hat \beta_2)' (y - X \hat \alpha_2 - Z \hat \beta_2) \\
  &= \frac{1}{n - p_z} y' (H - Z (Z'Z)^{-1} Z' H )' (H - Z (Z'Z)^{-1} Z' H) y \\
  &= \frac{1}{n - p_z} y' [(I - Z (Z'Z)^{-1} Z') H ]' (I - Z (Z'Z)^{-1} Z') H y \\
  &= \frac{1}{n - p_z} y' H G H y 
\end{aligned}
\]
--> 

## Unit tests of the above

Taken from (earlier versions of) `test.cov_adj.notforCRAN.R`.
The following test (suppressed from vignette output) is being retained there as well as here:

```{include=FALSE}
test_that("cov_adj variance estimates for orthogonal predictors", {
  library(sandwich)
  set.seed(230274373)
  k <- 25
  n <- 4 * k
  x1 <- c(rep(1, 2 * k), rep(-1, 2 * k))
  x2 <- c(rep(1, k), rep(-1, 2 * k), rep(1, k))
  z <- c(rep(1, k), rep(-1, k), rep(1, k), rep(-1, k))

  # all orthogonal by specification
  expect_equal((t(x1) %*% x2)[1], 0)
  expect_equal((t(x1) %*% z)[1], 0)
  expect_equal((t(x2) %*% z)[1], 0)

  xy <- function(b0 =  1, b1 = 2, b2 = 3, b3 = -1, sigma2 = 4) {
    y <- b0 + b1 * x1 + b2 * x2 + b3 * z + rnorm(n, sd = sqrt(sigma2))
    data.frame(id = 1:n, x1 = x1, x2 = x2, z = z, y = y)
  }

  df <- xy()
  mboth <- glm(y ~ x1 + x2 + z, data = df)
  m1 <- glm(y ~ x1, data = df)

  # we'll start by manually doing the offest Y - \hat Y
  m2 <- glm(y ~ x2 + z, data = df, offset = predict(m1))

  # Because of orthogonality, we should get $\hat \beta_0= \hat \alpha_0$,
  # $\hat \beta_1 = \hat \alpha_1$, and $\hat beta_2 = \hat gamma_1$.

  expect_equal(coef(mboth), c(coef(m1), coef(m2)[2:3]))

  # now repeat with cov_adj
  specification <- rct_spec(z == 1 ~ unitid(id), data = df)
  m2ca <- glm(y ~ x2 + z, data = df, offset = cov_adj(m1, specification = specification))

  expect_equal(coef(mboth), c(coef(m1), coef(m2ca)[2:3]))

  ## naive case
  m2naive <- glm(y ~ x2 + z, data = df)

  expect_false(all(vcov(m2naive) == vcov(m2ca)))

  hc_both  <- vcovHC(mboth, type = "HC0")
  hc_naive <- vcovHC(m2naive, type = "HC0")
  hc_m2ca  <- vcovHC(m2ca, type = "HC0")

  # trim down the bigger model to just the variables in the second stage
  in_two <- c("(Intercept)", "x2", "z")
  hc_both_trimmed <- hc_both[in_two, in_two]

  expect_equal(hc_m2ca, hc_both_trimmed)
  expect_false(all(hc_naive == hc_m2ca))
})
```


The following test (suppressed from vignette output) now appears only in this file.

```{include=FALSE}
test_that("cov_adj variance estimates for correlated predictors", {
  library(sandwich)
  set.seed(230274373)
  k <- 25
  n <- 4 * k
  x1 <- c(rep(1, 2 * k), rep(-1, 2 * k))
  z <- sample(c(rep(1, 2 * k), rep(0, 2 * k)), prob = x1 + 2)

  xy <- function(b0 =  1, b1 = 2, b2 = 3, sigma2 = 4) {
    y <- b0 + b1 * x1 + b2 * z + rnorm(n, sd = sqrt(sigma2))
    data.frame(id = 1:n, x1 = x1,  z = z, y = y)
  }

  df <- xy()
  mboth <- lm(y ~ x1 + z, data = df)
  m1 <- lm(y ~ x1, data = df) # includes an intercept

  # we'll start by manually doing the offset Y - \hat Y
  # the intercept is in the "x" variables, not the z variables
  m2 <- lm(y ~ z - 1, data = df, offset = predict(m1))

  ## verify some of the results of the VarianceEstimation vignette
  X <- model.matrix(m1)
  Z <- model.matrix(m2)
  Id <- diag(n)

  # some quantities we will need
  XtXinv <- solve(t(X) %*% X)
  ZtZinv <- solve(t(Z) %*% Z)
  H <- Id - X %*% XtXinv %*% t(X)
  G <- Id - Z %*% ZtZinv %*% t(Z)

  a <- solve(t(X) %*% G %*% X)
  b <- solve(t(Z) %*% H %*% Z)
  s2_1 <- (1/(n - 3)) * t(df$y) %*% (Id - X %*% a %*% t(X) %*% G - Z %*% b %*% t(Z) %*% H) %*% df$y
  s2_1 <- s2_1[1,1]
  expect_equal(summary(mboth)$sigma, sqrt(s2_1))

  # variance we will get from the combined model (just subsetting for the 'z' variable)
  var_beta_1 <- s2_1 * b
  expect_equal(vcov(mboth)[3,3], var_beta_1[1,1])

  alpha_2 <- (XtXinv %*% t(X) %*% df$y)[,1]
  beta_2 <- (ZtZinv %*% t(Z) %*% H %*% df$y)[1,1]
  expect_equal(coef(m1), alpha_2)
  expect_equal(coef(m2), beta_2)

  s2_2 <- (1 / (n - 1)) * t(df$y) %*% H %*% G %*% H %*% df$y
  s2_2 <- s2_2[1,1]
  expect_equal(summary(m2)$sigma, sqrt(s2_2))

  r_2 <- resid(m2)
  expect_equal(summary(m2)$sigma, sqrt((1/(n - 1)) * sum(r_2^2)))
  expect_equal(r_2, df$y - X %*% alpha_2 - Z %*% beta_2, ignore_attr = TRUE)
  expect_equal(r_2, G %*% H %*% df$y, ignore_attr = TRUE)

  # now repeat with cov_adj
  specification <- rct_spec(z ~ unitid(id), data = df)
  m2ca <- lm(y ~ assigned() - 1, data = df, offset = cov_adj(m1, specification = specification), weights = ate(specification))

  m2ca_da <- as.lmitt(m2ca)

  ## TODO: What are we guaranteeing about vcov and sandwich about m2ca_da?
})
```

# Covariance modeling *with* treatment followed by diff-in-diff on *partial* residuals

Let $\hat{\alpha}_{1}$ and $\hat{\beta}_{1}$ be $X$ and $Z$ coefficients in a linear covariance model as above, i.e. $\operatorname{Proj}(Y|X, Z) = X{\hat\alpha}_{1} + Z{\hat\beta}_{1}$.  These coefficients solve
\[
\begin{pmatrix}
(Y - X\hat{\alpha}_{1} - Z\hat{\beta}_{1})'X\\
(Y - X\hat{\alpha}_{1} - Z\hat{\beta}_{1})'Z\\
\end{pmatrix} =0;
\]
under our assumption that $\begin{pmatrix} X & Z \end{pmatrix}' \begin{pmatrix} X & Z \end{pmatrix}$ is invertible, the solution is unique. 

The partial residual given $X$ is $Y-X\hat{\alpha}_{1}$.  The regression of these partial residuals on Z delivers coefficients $\hat{\beta}_{0}$ solving
\[
(Y - X\hat{\alpha}_{1} - Z\hat{\beta}_{0})'Z =0;
\]

From the uniqueness of the solution $(\hat{\alpha}_{1}', \hat{\beta}_{1}')'$ of the earlier estimating equation, we have $\hat{\beta}_{0} = \hat{\beta}_{1}$ (and ${\beta}_{0} = {\beta}_{1}$). 

Under the constant-variance linear model we have $\operatorname{Var}(\hat{\beta}_1) = \sigma^2 S_{Z'Z}^{-1}$, for $S_{Z'Z} = Z'Z - Z'X(X'X)^{-1}X'Z$, the Schur complement of $X'X$ in
$\left(\begin{smallmatrix}
X'X & Z'X \\
X'Z & Z'Z \\
\end{smallmatrix}\right)$.  Without the assumption of constant residual variance we continue to have the sandwich formula
\begin{align*}
\operatorname{Var}(\hat{\beta}_1) = &
\begin{pmatrix}
-S_{Z'Z}^{-1}(Z'X)(X'X)^{-1} & S_{Z'Z}^{-1}
\end{pmatrix} \times\\
& 
\operatorname{Cov}\begin{pmatrix}
(Y - X{\alpha}_{1} - Z{\beta}_{1})'X\\
(Y - X{\alpha}_{1} - Z{\beta}_{1})'Z\\
\end{pmatrix}
\begin{pmatrix}
-(X'X)^{-1}(X'Z)S_{Z'Z}^{-1} \\ S_{Z'Z}^{-1}
\end{pmatrix}, \text{ or equivalently}\\
\operatorname{Var}(\hat{\beta}_1)=&
\begin{pmatrix}
-(Z'Z)^{-1}(Z'X)S_{X'X}^{-1} & (Z'Z)^{-1}+\cdots
\end{pmatrix} \times\\
& 
\operatorname{Cov}\begin{pmatrix}
(Y - X{\alpha}_{1} - Z{\beta}_{1})'X\\
(Y - X{\alpha}_{1} - Z{\beta}_{1})'Z\\
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z)(Z'Z)^{-1} \\ (Z'Z)^{-1}+\cdots
\end{pmatrix}\\
&= (Z'Z)^{-1}\begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & I_{p_{z}} + (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}
\end{pmatrix} \times\\
&
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ I_{p_{z}} + (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)
\end{pmatrix}(Z'Z)^{-1},
\end{align*}
where "$(Z'Z)^{-1} + \cdots$" stands for $(Z'Z)^{-1} + (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}$, $S_{X'X}= X'X - X'Z(Z'Z)^{-1}Z'X$, the Schur complement of $Z'Z$ in
$\left(\begin{smallmatrix}
X'X & Z'X \\
X'Z & Z'Z \\
\end{smallmatrix}\right)$, and $\left(\begin{smallmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{smallmatrix}\right)=\operatorname{Cov}\left(\begin{smallmatrix}
(Y - X{\alpha}_{1} - Z{\beta}_{1})'X\\
(Y - X{\alpha}_{1} - Z{\beta}_{1})'Z\\
\end{smallmatrix}\right)$.

Now writing "$I_{p_{z}} + \cdots$" for
$I_{p_{z}} + (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}$, we have

\begin{align*}
(Z'Z)\operatorname{Var}(\hat{\beta}_1) (Z'Z)=&
\begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & I_{p_{z}} + \cdots
\end{pmatrix} 
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (I_{p_{z}} + \cdots)'
\end{pmatrix}\\
=&\left\{
\begin{pmatrix}
-(Z'X)S_{X'X}^{-1} &  \cdots
\end{pmatrix} +
\begin{pmatrix}
0 & I_{p_{z}}
\end{pmatrix}
\right\}
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\left\{
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (\cdots)'
\end{pmatrix} +
\begin{pmatrix}
0 \\ I_{p_{z}}
\end{pmatrix}
\right\}\\
=& 
\begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}
\end{pmatrix} \times\\
& 
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)
\end{pmatrix} +\\
&\begin{pmatrix} 0 & I_{p_{z}}\end{pmatrix}
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)
\end{pmatrix}+\\
& \begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}
\end{pmatrix} 
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix} 0 \\ I_{p_{z}} \end{pmatrix} + \\
& \begin{pmatrix} 0 & I_{p_{z}}\end{pmatrix}
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix} 0 \\ I_{p_{z}} \end{pmatrix}
\\
=& 
\begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}
\end{pmatrix} \times\\
& 
\begin{pmatrix}
B_{xx} & B_{xz}\\
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)
\end{pmatrix} +\\
&\begin{pmatrix}
B_{zx} & B_{zz}
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z) \\ (Z'Z)^{-1}(Z'X)S_{X'X}^{-1}(X'Z)
\end{pmatrix}+\\
& \begin{pmatrix}
-(Z'X)S_{X'X}^{-1} & (Z'X)S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}
\end{pmatrix} 
\begin{pmatrix}
 B_{xz}\\
 B_{zz}
\end{pmatrix}\\
&+ B_{zz}.
\end{align*}
Recalling that $\beta_0=\beta_1$,
\[
{\operatorname{Cov}}\begin{pmatrix}
X'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{0})\\
\end{pmatrix}
=
\begin{pmatrix}
B_{xx} & B_{xz}& B_{xz}\\
B_{zx} & B_{zz}& B_{zz}\\
B_{zx} & B_{zz}& B_{zz}
\end{pmatrix}.
\]

The corresponding $A$ matrix is
\[
A = \mathbf{E}\left[\nabla_{\alpha_{1},\beta_{1},\beta_{0}}
\begin{pmatrix}
X'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{0})\\
\end{pmatrix}
\right]  =
- \begin{pmatrix}
X'X & X'Z & 0\\
Z'X & Z'Z & 0\\
Z'X & 0 & Z'Z\\
\end{pmatrix}.
\]
For representation of $\operatorname{Var}(\hat{\beta}_0)$, we only need $A^{-1}$'s bottom row:
\begin{align*}
[{A}^{-1}]_{3 -} &=
- \begin{pmatrix}
-(Z'Z)^{-1}
\begin{pmatrix}
Z'X & 0
\end{pmatrix}
\begin{pmatrix}
X'X & X'Z \\
Z'X & Z'Z \\
\end{pmatrix}^{-1} &
(Z'Z)^{-1}
\end{pmatrix}\\
&=
- \begin{pmatrix}
-(Z'Z)^{-1}
\begin{pmatrix}
Z'X & 0
\end{pmatrix}
\begin{pmatrix}
S_{X'X}^{-1} & -S_{X'X}^{-1}X'Z(Z'Z)^{-1} \\
-(Z'Z)^{-1}Z'X S_{X'X}^{-1} & (Z'Z)^{-1} + \cdots \\
\end{pmatrix} &
(Z'Z)^{-1}
\end{pmatrix}\\
&=-(Z'Z)^{-1}\begin{pmatrix}
-(Z'X) S_{X'X}^{-1} & (Z'X) S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}& I_{p_z}
\end{pmatrix}.
\end{align*}
Since
\[
\operatorname{Var}(\hat{\beta}_{0}) = [A^{-1}]_{3-}
\operatorname{Cov}\left(\begin{matrix}
X'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{1})\\
Z'(Y - X{\alpha}_{1} - Z{\beta}_{0})\\
\end{matrix}\right)
[A^{-1}]_{3-}'
\]
we have 
\begin{align*}
{\operatorname{Var}}(\hat{\beta}_{0}) =&
[{A}^{-1}]_{3-}
\begin{pmatrix}
B_{xx} & B_{xz}& B_{xz}\\
B_{zx} & B_{zz}& B_{zz}\\
B_{zx} & B_{zz}& B_{zz}
\end{pmatrix}
[{A}^{-1}]_{3-}';\\
(Z'Z) {\operatorname{Var}}(\hat{\beta}_{0}) (Z'Z)
&=
\left\{
\begin{pmatrix}
-(Z'X) S_{X'X}^{-1} & (Z'X) S_{X'X}^{-1}(X'Z)(Z'Z)^{-1}& 0
\end{pmatrix}+
\begin{pmatrix} 0 & 0 & I_{p_{z}} \end{pmatrix}
\right\} \times\\
& \begin{pmatrix}
B_{xx} & B_{xz}& B_{xz}\\
B_{zx} & B_{zz}& B_{zz}\\
B_{zx} & B_{zz}& B_{zz}
\end{pmatrix}
\left\{
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z)  \\ (Z'Z)^{-1}(Z'X) S_{X'X}^{-1}(X'Z)\\ 0
\end{pmatrix}+
\begin{pmatrix} 0 \\ 0 \\ I_{p_{z}} \end{pmatrix}
\right\}\\
=&
\begin{pmatrix}
-(Z'X) S_{X'X}^{-1} & (Z'X) S_{X'X}^{-1}(X'Z)(Z'Z)^{-1} & 0 
\end{pmatrix}\times \\
&
\begin{pmatrix}
B_{xx} & B_{xz} & B_{xz}\\
B_{zx} & B_{zz} & B_{zz}\\
B_{zx} & B_{zz} & B_{zz}\\
\end{pmatrix}
\begin{pmatrix}
-S_{X'X}^{-1}(X'Z)  \\ (Z'Z)^{-1}(Z'X) S_{X'X}^{-1}(X'Z)  \\ 0
\end{pmatrix} + \\
&\begin{pmatrix} B_{zx} & B_{zz}& B_{zz}\end{pmatrix} \begin{pmatrix}
-S_{X'X}^{-1}(X'Z)  \\ (Z'Z)^{-1}(Z'X) S_{X'X}^{-1}(X'Z)\\ 0
\end{pmatrix} +\\
& \begin{pmatrix}
-(Z'X) S_{X'X}^{-1} & (Z'X) S_{X'X}^{-1}(X'Z)(Z'Z)^{-1} & 0
\end{pmatrix} \begin{pmatrix}
B_{xz}\\
B_{zz}\\
B_{zz}
\end{pmatrix}+\\
& B_{zz}.
\end{align*}
Allowing for zeroes, this expression is identical to that given for $(Z'Z)\operatorname{Var}(\hat{\beta}_1)(Z'Z)$ above. 

<!--
=& (Z'Z)^{-1}\begin{pmatrix}
-Z'X S_{X'X}^{-1}B_{xx} +  Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zx} + B_{zx}
&
-Z'X S_{X'X}^{-1}B_{xz} + Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zz} + B_{zz} 
&
-Z'X S_{X'X}^{-1}B_{xz} + Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zz} + B_{zz}
\end{pmatrix}\\
&\times
\begin{pmatrix}
-S_{X'X}^{-1}X'Z \\
(Z'Z)^{-1}Z'X S_{X'X}^{-1}X'Z
\\ I
\end{pmatrix}
(Z'Z)^{-1}\\
=& (Z'Z)^{-1}\big[
Z'X S_{X'X}^{-1}B_{xx}S_{X'X}^{-1}X'Z - Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zx}S_{X'X}^{-1}X'Z - B_{zx}S_{X'X}^{-1}X'Z +\\
&-Z'X S_{X'X}^{-1}B_{xx}(Z'Z)^{-1}Z'X S_{X'X}^{-1}X'Z +  Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zx}(Z'Z)^{-1}Z'X S_{X'X}^{-1}X'Z + \\
& B_{zx}(Z'Z)^{-1}Z'X S_{X'X}^{-1}X'Z+ -Z'X S_{X'X}^{-1}B_{xx} +  Z'X S_{X'X}^{-1}X'Z(Z'Z)^{-1}B_{zx} + B_{zx}\big] (Z'Z)^{-1}
 -->