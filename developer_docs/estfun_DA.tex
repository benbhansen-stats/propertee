\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}%for \llbracket
\usepackage{mathtools}% for DeclarePairedDelimiter
\usepackage{blkarray}
\usepackage{soul}
\usepackage{colonequals}
\newcommand{\defeq}{\ensuremath{\colonequals}}
\newcommand{\eqdef}{\ensuremath{\equalscolon}}

\newcommand{\EE}{\operatorname{E}}
\DeclarePairedDelimiter{\indicator}{\llbracket}{\rrbracket}
\newcommand{\owt}[1][{[z_{i}]}]{\ensuremath{\check{w}_{i#1}}}
\newcommand{\absorbInterceptsEF}{\upsilon}
\newcommand{\AbsorbInterceptsEF}{\Upsilon}
\newcommand{\absorbModeratorEF}{\grave{\upsilon}}
\newcommand{\AbsorbModeratorEF}{\grave{\Upsilon}}

\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{plain}

\usepackage{hyperref}


\author{BBH}
\title{\texttt{sandwich::estfun()} method for DA objects}
\begin{document}
\nobibliography*
\maketitle
\section{Separate and combined estimating functions for covariance and comparison fits}

\subsection{Setting}

Let $\mathcal{C}$ and $\mathcal{Q}$ denote the covariance and
quasiexperimental samples, respectively, expressed at the level of
unit of assignment: if assignment is at the level of the classroom,
then these are collections of classes not collections of students, nor
of student-year observations.  In turn, identify each unit of assignment
with the set of elemental observations they contain: a classroom is
identified with the
set of students, or student-years, associated with that classroom;
if there is no clustering, then we think of $\mathcal{C}$ and
$\mathcal{Q}$ as consisting of singleton sets, each containing a
single element.  (Additional structure: any clusters $c, c' \in \mathcal{C}
\cup \mathcal{Q}$ that are distinct, $c  \neq c'$, are nonoverlapping, $c \cap c' = \emptyset$.) This permits us to write $\mathcal{Q}$ for the
quasiexperimental sample as expressed in terms of clusters, and $\bigcup
\mathcal{Q}$ for the collection of all elements belonging to the
quasiexperimental sample.

Denote by $n_{\mathcal{C}}= \# \left( \bigcup \mathcal{C}\right)$,
$n_{\mathcal{Q}}= \# \left( \bigcup \mathcal{Q}\right)$ and
$n = \# \left[\left( \bigcup \mathcal{C}\right) \cup \left(\bigcup
    \mathcal{Q}\right)\right]$ the numbers of elements in the 3
overlapping samples. There are random estimating functions
$\Phi(\mathbf{V}; \beta ) = [\indicator{ i \in
  \left(\bigcup\mathcal{C}\right)}\phi(\vec{V}_{i}; \beta) : i]'$ that define the covariance
regression, padding with rows of zeroes in
the complement of the covariance sample so
as to have dimension $n \times p$ rather than $n_{\mathcal{C}}\times
p$; and estimating functions
$\grave{\Psi}(\mathbf{V}; \rho, \beta) = [\psi(\vec{V}_{i}; \rho,
\beta): i]'$, padded with zeroes on the complement of $\bigcup
\mathcal{Q}$ in order also to have row extent $n$, to define the
treatment condition comparison.  For now, let
\begin{equation}
         \label{eq:5}
         \grave{\psi}(\vec{V}_{i}; \rho, \beta) =\\
         \left( \begin{array}{c}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[0]}] [Y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \rho_{0}]\indicator{Z_{i}=0}\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[1]}] [Y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \rho_{1}]\indicator{Z_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[k]}] [Y_{i}[k] - g(\vec{x}_{i}\beta)-
                  \rho_{k}]\indicator{Z_{i}=k}\\
                \end{array}\right),
\end{equation}
where
\begin{itemize}
\item $Y_{i}[j]$ represents $i$'s potential outcome following
  assignment to condition $i$;
\item $Z_{i}$ is the treatment assignment of $i$'s cluster $[i]$;
\item $\vec{V}_{i} = (Y_{i}[0],\ldots, Y_{i}[k], \vec{X}_{i}, M_{i}, W_{i}, Z_{i})$, where $\vec{X}_{i}$, $M_{i}$ and $W_{i}$ are covariates, an optional moderator variable and a
  frequency or importance weight associating with $i$ (1 if no such weight was
  given);
\item $\owt[{[j]}]$ is either $w_i$ or,
   if requested by the user, $w_{i}$'s product with an
  odds- or inverse probability-based weight attaching to $i$'s
  assignment unit $[i]$ under assignment $j$.
\end{itemize}
(I say ``for now'' because \eqref{eq:14} will present a
$\psi(\cdot)$ with equivalent solutions that is better aligned with
\texttt{lm()}'s default treatment effect parameterizations, and
\eqref{eq:9} in \S~\ref{sec:accomm-interc-via} below will generalize $\psi()$'s definition
to add block-specific intercepts.  The symbols $\vec{x}_{i}$, $m_{i}$
and $w_{i}$ are given in lowercase as we in general condition upon
$\{(\vec{X}_{i}, M_{i}, W_{i}): i\}$.)
The interest parameter is $(\rho_{0}, \tau)$, where $\tau \defeq (\rho_{1}, \ldots, \rho_{k}) - \rho_{0}$. In the model-based interpretation, occurrences of ``$Z_{i}$'' in \eqref{eq:5} may be replaced with ``$z_{i}$,'' while under design-based we can write ``$y_{i}[j]$'' instead of ``$Y_{i}[j]$''; see Section~\ref{sec:des-vs-mod-based} below.

\subsection{Goal} \label{sec:goal}
We aim to define \texttt{estfun.teeMod()} associating
with a direct adjustment model a matrix of evaluated estimating function
contributions of form
\begin{equation} \label{eq:22}
  \tilde{\Psi} = c_{2}\Psi - c_{1}\Phi A_{11}^{-1}A_{21}^{t},
\end{equation}
and a corresponding \texttt{bread.teeMod()} which together will enable \texttt{sandwich::vcov()} to provide valid standard errors.
Note that this is $n$ by
$k+1$, $n$ being the size of the combined $\mathcal{C}$ and
$\mathcal{Q}$ sample, with columns corresponding to $\rho$.%
(Assuming we're estimating a single marginal treatment effect, as
opposed to moderation effect[s], either without blocking or with
neither explicit nor implicit block fixed effects; otherwise the
number of columns could be $k$\footnote{For a main effect
DA model fitted with absorption of the block intercept or
intercepts; see \S~\ref{sec:accomm-interc-via} below.} or something
larger than $k+1$\footnote{In the case of subgroup effects and/or unabsorbed block fixed effects.},
depending. To obviate the ambiguity we'll say
\begin{equation*}
  \tilde{k} \defeq \mathtt{ncol}(\tilde{\Psi}).)
\end{equation*}

Values of the constants $c_{1}$ and $c_{2}$ are to be determined.
I conjecture these decisions can be made in such away that
\begin{enumerate}
\item Using this as a basis for a meat matrix while using
  $A_{22}^{-1}$  ``bread matrix'' lands us in the same place as the
  inversion formula as expressed with A's and B's on p.373 of Carroll
  et al. 2006.
\item Suppose we've got multiple DA models that have been fit over the
  same CA model, and we wish to compute joint covariances for their
  distinct parameters
  (\href{https://github.com/benbhansen-stats/propertee/issues/79}{\#79
    on GitHub}). We get appropriate joint bread matrices by
  diagonally combining bread matrices of the DA models, and we get
  appropriate meat matrices by cbind-ing the estfun's.
\item For ``natural'' choices of $c_{1}$ and $c_{2}$, in the
  presence of clustering the corresponding bread matrices are
  reasonable and interpretable, at least in cases of HC0 and HC1
  degree of freedom adjustments.
\end{enumerate}

\subsection{$\psi(\vec{V}_{i};
  \alpha_{0}, \tau, \beta)$, an intercept-form equivalent of $\grave{\psi}(\vec{V}_{i};
  \rho, \beta)$}
\label{sec:psit-rho_0-tau}

Write $\tau_{j}=\rho_{j}-\rho_{0}$ and introduce the intercept parameter ``$\alpha$,''  for the time being one-dimensional with $\alpha_{0}=\rho_{0}$.   The estimating equation $\sum_{i}\grave{\psi}(\vec{v}_{i}; \rho, \beta)=0$,
$\grave{\psi}(\cdot; \cdot)$ as in \eqref{eq:5} above, has the same solutions as $\sum_{i}\psi(\vec{v}_{i};
  \alpha_{0}, \tau, \beta)=0$, with $\psi(\cdot; \cdot)$ as in \eqref{eq:14} immediately below:
\begin{equation} \label{eq:14}
         \psi(\vec{v}_{i};
         \alpha_{0}, \tau, \beta) =
         \indicator{i \in \bigcup \mathcal{Q}}
         \left( \begin{aligned}
           \owt[{[z_{i}]}] &\big[y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\\
           \owt[{[1]}]\indicator{z_{i}=1} & \big[y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{1}\big]\\
                  \vdots \\
           \owt[{[k]}]\indicator{z_{i}=k} & \big[y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_{0} - \tau_{k}\big]\\
                \end{aligned}\right),
\end{equation}
the omission of a ``$\indicator{z_{i}=0}$'' factor from the topmost
expression being intentional and deliberate.
Abbreviating ``$\owt{}$'' as ``$\owt[]$'' and ``$y_{i}[z_{i}]$'' as
``$y_{i}$,'' we have
\begin{equation}\label{eq:27}
         \psi(\vec{v}_{i};
         \alpha_{0}, \tau, \beta) =
         \indicator{i \in \bigcup \mathcal{Q}}
  \owt[] \Big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0} -
                  \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\Big]
                  \begin{pmatrix} 1 \\ \indicator{z_{i}=1}\\ \vdots
                    \\ \indicator{z_{i}=k}\end{pmatrix} .
\end{equation}
Since the solution in
$(\alpha_{0}, \tau_{1}, \ldots, \tau_{k})$ of
$\sum_{i}\psi(\vec{v}_{i}; \alpha_{0}, \tau, \beta) =0$ satisfies
\begin{equation} \label{eq:16}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[][y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{j}]\indicator{z_{i}=j}, \text{ each } j=1, \ldots, k,
\end{equation}
this solution also satisfies
\begin{equation*}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\indicator{z_{i}
                  \neq 0}.
\end{equation*}
The complement of condition $\indicator{z\neq 0}$ being
$\indicator{z=0}$, under which
$\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}=0$, it follows from \eqref{eq:16} that
\begin{align*}
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[{[0]}]\big[y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}\big]\indicator{z_{i}
             =0}\\
           & \Updownarrow \\
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big].
\end{align*}
Representations \eqref{eq:14} and \eqref{eq:27} better represent what
\texttt{lm()} does, while \eqref{eq:5} centers the interpretation of $\rho_{0}$/$\alpha_{0}$ as a
marginal mean of the outcome, or partial residual outcome, under control.

\subsection{The covariance we're trying to
  produce}\label{sec:covar-were-trying}

Let $\hat\beta$, $\hat{\alpha}$ and $\hat\tau$ solve
\[\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{v}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\psi(\vec{v}_{j}; \tau,
\alpha, \beta )  =0
  \]
  in $\beta$ and $\tau$, whereas $\beta_{(n)}$, $\alpha_{(n)}$ and $\tau_{(n)}$ solve or nearly solve\footnote{%
In actuality we only need the estimator sequence to be consistent for the deterministic sequences $(\beta_{(n)}:n)$, $(\alpha_{(n)}:n)$ and $(\tau_{(n)}:n)$, i.e. $|\hat\beta - \beta_{(n)}|_{2}$, $|\hat{\alpha} - \alpha_{(n)}|_{2}$ and $|\hat\tau - \tau_{(n)}|_{2}$ are all $o_{P}(1)$.%
}
\[\sum_{i\in \bigcup \mathcal{C}}\EE\phi(\vec{V}_{i}; \beta )
  =0\quad\text{and}\quad
\sum_{j\in \bigcup \mathcal{Q}}\EE\psi(\vec{V}_{j}; \tau,
\alpha, \beta )  =0.
  \]
In these expectations and those that follow, $\vec{x}_{i}$ and $w_{i}$
are taken as fixed but $Z_{i}$ and/or potential outcomes $(Y_{i}[z]:
z=0, \ldots, k)$ may be random according as we follow a design- or
model-based framework; see \S~\ref{sec:des-vs-mod-based}.
  Write
\begin{align}
\nonumber
    A(\alpha, \beta, \tau) &= \left(
      \begin{array}{cc}
        n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\EE [\nabla_{\beta}\phi(\vec{V}_{i};
        \beta )]&0\\
        n_{\mathcal{Q}}^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\beta}\psi(\vec{V}_{j};
        \tau, \alpha, \beta )]  & n_{\mathcal{Q}}^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\alpha, \tau}\psi(\vec{V}_{j};
        \tau, \alpha, \beta )]
      \end{array}
                                  \right)\\
    \label{eq:13}
    &\eqdef \left(
      \begin{array}{cc}
        A_{11}(\beta)& 0\\
        A_{21}(\beta) & A_{22}(\alpha, \tau)
      \end{array}
\right),
  \end{align}
where the structure of $\psi(\cdot)$ as given in \eqref{eq:14} ensures
that $A_{21}(\alpha, \beta, \tau)$ and $A_{22}(\alpha, \beta, \tau)$
are functions only of $\beta$ and of $(\alpha, \tau)$
respectively. Assuming $A_{11}(\beta_{(n)})$ and $A_{22}(\alpha_{(n)},
\tau_{(n)})$ to be invertible, define \textit{linearizations} (of
$\hat\beta$ and $(\hat{\alpha}_{0}, \hat\tau)$) as follows:
  \begin{align}
    \left(\begin{array}{c}\tilde{\beta}_{(n)}\\\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)
    \defeq& \left(\begin{array}{c}\beta_{(n)} \\\alpha_{(n)}\\
                    \tau_{(n)} \end{array}\right)
    + A^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})\left(
    \begin{array}{c}
      n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\\
      n_{\mathcal{Q}}^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} )
    \end{array}
    \right),\, \text{i.e.} \nonumber \\
   \nonumber \\
       \tilde{\beta}_{(n)}
    \defeq& \beta_{(n)} + A_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})
  \, \text{and} \nonumber\\
\left(\begin{array}{c}\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)               &=\left(\begin{array}{c}\alpha_{(n)}\\
                         \tau_{(n)} \end{array}\right) +
    A_{22}^{-1}(\alpha_{(n)}, \tau_{(n)})\times \label{eq:20}\\
    & \left[
                 n_{\mathcal{Q}}^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} ) - A_{21}(\beta_{(n)}) A_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\right],\nonumber
\end{align}
where we've used the block inversion relationship
\begin{equation*}
  \label{eq:23}
  \left(\begin{array}{cc}A & 0 \\ B & C \end{array} \right)^{-1} =
  \left(\begin{array}{cc}A^{-1} & 0 \\ 0 & C^{-1} \end{array} \right)
  \left(\begin{array}{cc} I & 0 \\ -BA^{-1} & I \end{array} \right)
\end{equation*}
(a special case of factorization and inversion based on Schur complements).

The constants $n_{\mathcal{C}}$ and
$n_{\mathcal{Q}}$ in the above are somewhat
arbitrary.  Provided they're positive, replacing them throughout with any other
positive constants defines the same linearizations (although there may
be numerical differences).
Using the numbers of elements (not
clusters) in the covariance and in the comparison samples,
respectively, aligns the expressions with the computing procedures
generating $\hat\beta$, $\hat{\alpha}$ and $\hat\tau$ from specific
datasets, but our strategy for invoking the \texttt{sandwich} package
in standard error computations calls for  $\psi()$ \texttt{estfun}'s
of dimension $n$ rather than $n_{\mathcal{Q}}$.%
\footnote{%
  The constant that \texttt{sandwich}
  functions will select for the covariance model is $n_{\mathcal{C}}$, given its sample
  size.  It's incumbent on us to supply \texttt{sandwich::estfun()}
  and/or \texttt{sandwich::bread()} methods for
  \texttt{teeMod} objects with compatible implicit scaling
  constants. \texttt{sandwich::bread()} and
  \texttt{sandwich::breadCL()} take the scaling constant to be the
  reciprocal of the number of rows in whatever
  \texttt{sandwich::estfun()} returns; it will fall to us to ensure
  our $\hat{A}_{21}$ and $\hat{A}_{22}$ observe compatible scalings.}%
 This leads us to
define
\begin{align}\label{eq:29}
  \tilde{A}_{21}(\beta) &= n^{-1} \sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\beta}\psi(\vec{V}_{j};
        \tau, \alpha, \beta )] = \frac{n_{\mathcal{Q}}}{n} {A}_{21}(\beta),\\
  \tilde{A}_{22}(\alpha, \tau) &= n^{-1} \sum_{j\in \bigcup
        \mathcal{Q}}\EE[ \nabla_{\alpha, \tau}\psi(\vec{V}_{j};
        \tau, \alpha, \beta )] = \frac{n_{\mathcal{Q}}}{n}
                                 {A}_{22}(\alpha, \tau) \text{ and}\nonumber\\
  \tilde{A} &= \left(
              \begin{array}{cc}
                A_{11} & 0 \\
                \tilde{A}_{21} & \tilde{A}_{22}\\
              \end{array}
\right).\nonumber
\end{align}
Substitution into  expression \eqref{eq:20} for
$\tilde{\alpha}_{(n)}$ and $\tilde{\beta}_{(n)}$ gives
\begin{align}
      \left(\begin{array}{c}\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)         &=\left(\begin{array}{c}\alpha_{(n)}\\
                         \tau_{(n)} \end{array}\right) +
    \tilde{A}_{22}^{-1}(\alpha_{(n)}, \tau_{(n)})\times \label{eq:21}\\
    & \left[
                 n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} ) -
      \tilde{A}_{21}(\beta_{(n)})
      {A}_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\right] \nonumber
\end{align}
or equivalently
\begin{equation*}
\left(\begin{array}{c}\tilde{\beta}_{(n)}\\ \tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)
    = \left(\begin{array}{c}\beta_{(n)} \\\alpha_{(n)}\\
                    \tau_{(n)} \end{array}\right)
    + \tilde{A}^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})\left(
    \begin{array}{c}
      n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\\
      n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} )
    \end{array}
\right).
\end{equation*}
Assume that: the linearization errors $|\hat\beta -\tilde{\beta}_{(n)}|_{2}$,
$|\hat\tau -\tilde{\tau}_{(n)}|_{2}$ and $|\hat{\alpha} - \tilde{\alpha}_{(n)}|_{2}$ tend in probability to 0;
$A_{11}(\beta_{(n)})$ and $A_{22}(\beta_{(n)}, \tau_{(n)})$ are nonsingular
with eigenvalues bounded away from 0; and $A_{11}(\cdot)$, $A_{21}(\cdot)$ and $A_{22}(\cdot, \cdot)$ are suitably
regular\footnote{E.g, they admit of Lipschitz constants: see \S~9.8,
  \bibentry{keener2010TheorStats}, while bearing in mind issues raised
  by \bibentry{feng2014exact}.}.
Then we have the covariance approximation characteristic of M-estimation,
\begin{align*} \operatorname{Cov}([\hat\beta',\hat{\alpha}', \hat\tau']) \approx&
  \operatorname{Cov}([\tilde{\beta}_{(n)}',\hat{\alpha}_{(n)}',\tilde{\tau}_{(n)}'])\\
  &=
    \tilde{A}^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)}) \times \\
  &\hspace{1.5em} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)} )\\
       n^{-1}\sum_{j=1}^{n}\psi(\vec{V}_{j}; \tau_{(n)}, \alpha_{(n)}, \beta_{(n)} )
     \end{array}
    \right)
  \tilde{A}^{-t}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})
\end{align*}
(recalling that we have so defined $\psi(\cdot)$ that $\psi(\vec{V}_{j}; \tau, \alpha, \beta) = 0$ when
$j\not\in \bigcup \mathcal{Q}$).


To help mark the difference from the package's first implementations of these estimating equation stacks (which took pains to avoid scaling constants),
meat matrices will be denoted with the letter ``M,'' not ``B.''
We have
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n^{-1}\sum_{j=1}^{n}\psi(\vec{V}_{j}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}M_{11}& n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{12}\\
    n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{21} & n^{-1}M_{22}
  \end{array}
\right),
\]
where
\[
  \begin{array}{cc}
    M_{11}  = \operatorname{Cov}[n_{\mathcal{C}}^{-1/2}\sum_{i\in \bigcup
             \mathcal{C}} \phi(\vec{V}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  \operatorname{Cov}[n_{\mathcal{C}}^{-1/2}\sum_{i\in \bigcup
             \mathcal{C}}\phi(\vec{V}_{i};
                                                                  \beta
                                                                  ), n^{-1/2}\sum_{j}\psi'(\vec{V}_{j}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = \operatorname{Cov}[n^{-1/2}\sum_{j}\psi(\vec{V}_{j};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
Note that $M_{11}$ is scaled just as in estimates of $\operatorname{Cov}(\hat\beta)$, while $M_{22}$'s and
$M_{12}$/$M_{21}$'s scaling constants --- $n^{-1}$ rather than
$n_{\mathcal{Q}}^{-1}$ in $M_{22}$'s
case and $(n_{\mathcal{C}}n)^{-1/2}$ not $(n_{\mathcal{C}} n_{\mathcal{Q}})^{-1/2}$ in $M_{12}$/$M_{21}$'s --- are progressively more weird.

With these definitions,
\begin{align}
  \operatorname{Cov}(\hat\tau) \approx& \tilde{A}_{22}^{-1}\{n^{-1} M_{22} -
                                 n_{\mathcal{C}}^{-1/2}n^{-1/2}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-1}\tilde{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}\tilde{A}_{21}^{t}\}\tilde{A}_{22}^{-t}\nonumber
  \\
                               &= n^{-1}\tilde{A}_{22}^{-1}\{
                                 M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}\tilde{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}\tilde{A}_{21}^{t}\}\tilde{A}_{22}^{-t}
                                . \label{eq:6}
\end{align}
An advantage of the expression with leading factor $n^{-1}$ is that
that way it parallels the calculation of
\texttt{sandwich::sandwich()} and, I think, other \texttt{sandwich}
functions. (Assuming that we write
\texttt{estfun.teeMod()}  to return a matrix of row extent
$n$.)
% A potential disadvantage is that it makes $A_{21}$ and
% $A_{22}$ not means of estimating equation contributions but rather
% means down-scaled by $n_{\mathcal{Q}}/n$, a potentially small factor;
% more on this presently.

  \subsection{Plain-vanilla estimation of $A$s and $M$s}
\subsubsection {Aside: ``Model-based'' vs. ``Design-based''}\label{sec:des-vs-mod-based}

In the model-based perspective, expected value and covariance are
conditional on
\[ \mathcal{M} = \sigma\left(\left[(X_{i}, W_{i}, M_{i}, Z_{i}: i \in c): c \in
      \mathcal{C}\cup \mathcal{Q} \right]\right)\]


In the design-based perspective, $\mathcal{Q}$ is partitioned into
blocks $b$, and expected value and covariance are
conditional on
\[\mathcal{X} = \sigma(\{(X_{i}, W_{i}, M_{i}, Y_{i}[1], \ldots Y_{i}[k]):
  i \in c; c\})\]
and
\[\mathcal{Z} = \sigma(\{\sum_{c \in b}\, \![Z_{c}=j\!] : j \in \{0, \ldots, k\}; b\} ),\]
where $z_{c}$ denotes the common treatment assignment of all $i\in
c$. (So estimating equations ${\Psi}_{i}$s are independent across but not within
clusters $c \ni i$.)

To incorporate $c\in \mathcal{C}\setminus\mathcal{Q}$ into this
definition, let each such cluster $c$ constitute a block unto itself,
$\{c\}$. This has the consequence that observations $i
\in \bigcup\left(\mathcal{C}\setminus\mathcal{Q}\right)$ do not
contribute to $M_{11}$, $M_{12}$ or $M_{21}$, at least in the
design-based perspective.

  \subsubsection{Conceptual vs practical bread estimates} \label{sec:conc-vs-pract}
  \paragraph{Model-based $\hat{A}$'s as estimates of design-based $A$ matrices.}%
In general $\EE
[\nabla_{\beta}\psi(\vec{V}_{i};  \beta, \alpha_{0},\tau ) \mid \mathcal{M}] \equiv
\nabla_{\beta}\psi(\vec{V}_{i};  \beta, \alpha_{0}, \tau )$; for many types of
covariance model one also has $\EE
[\nabla_{\beta}\phi(\vec{V}_{i};  \beta ) \mid \mathcal{M}] \equiv
\nabla_{\beta}\phi(\vec{V}_{i};  \beta )$. As a result, under the
model-based perspective
$A_{11}(\beta)$ and perhaps $A_{21}(\beta)$ are essentially observed,
i.e.  $\hat{A}_{11}(\beta) \equiv A_{11}(\beta)$ and (perhaps)
$\hat{A}_{21}(\beta) \equiv A_{21}(\beta)$.   These equivalences don't
hold under the design-based perspective, but under that perspective
the same matrices do serve as estimates of $A_{11}(\beta)$ and
$A_{21}(\beta)$ as given in \eqref{eq:13}.

\paragraph{Numerical stability by combining ${A}_{21}$ and
  $\tilde{A}_{22}$, rather than either
  ${A}_{21}$ and ${A}_{22}$ or $\tilde{A}_{21}$ and $\tilde{A}_{22}$.}
Recall from \eqref{eq:29} that $A_{2*}$ and $\tilde{A}_{2*}$ are
scaled sums of expected $\psi$-gradients differing only in their
scaling factors of $n_{\mathcal{Q}}^{-1}$ and $n^{-1}$,
respectively. The $n^{-1}$-scaling embedded
in $\tilde{A}_{21}$ and $\tilde{A}_{22}$ may invite numerical
instability.

  In setups with
  $n_{\mathcal{C}} \gg n_{\mathcal{Q}}$, $n^{-1}$-scaling may lead to underflows,
  collapsing of distinct values into numeric zeroes, if performed
  before taking sums; even if we don't scale before summing,
  $n_{\mathcal{Q}}^{-1}$ scaling of these matrices could faciliated improved
  use of numerically optimized mean and weighted mean functions in
  these calculations.  (By application of such functions only to the
  $n_{\mathcal{Q}}$ contributions to $\hat{A}_{2*}$  from $i \in \bigcup \mathcal{Q}$, not the broader set of $n = \#\left[\bigcup (\mathcal{C}\cup
    \mathcal{Q}) \right]$, which includes $n - n_{\mathcal{Q}}$ structural
  zeroes.)

  In parallel with \eqref{eq:6} we
  have $\operatorname{Cov}(\hat\tau) \approx$
  \begin{align*}
  &A_{22}^{-1}\{\frac{n}{n_{\mathcal{Q}}^{2}} M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}n_{\mathcal{Q}}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-1}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-t}\nonumber
  \\
                               =& n^{-1}A_{22}^{-1} \times\\
                                 &\left\{\left(\frac{n}{n_{\mathcal{Q}}}\right)^{2}M_{22} -
                                 \frac{n^{3/2}}{n_{\mathcal{C}}^{1/2}n_{\mathcal{Q}}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\right\}\\
    &
                                 \times A_{22}^{-t}\\
                               =& n^{-1}A_{22}^{-1} \times\\
                                 &\left\{\left(\frac{n}{n_{\mathcal{Q}}}\right)^{2}M_{22} -
                                 \left(\frac{n}{n_{\mathcal{Q}}}\right)\left(\frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}\right)[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\right\}\\
    &
                                 \times A_{22}^{-t}
                                .
\end{align*}
In computations that assemble the three inner summands separately, we may combine $\psi$-\texttt{estfun}'s of dimension
$n$ with $\hat{A}_{21}$ and $\hat{A}_{22}$ that have been scaled by
$n_{\mathcal{Q}}$, as opposed to $n$.  However, for Section~\ref{sec:goal}'s purpose of
associating \texttt{teeMod} objects witth estimating functions of \eqref{eq:22}'s form $c_{2}\Psi - c_{1}\Phi
A_{11}^{-1}A_{21}^{t}$, this formula invites unhelpful
complication. It seems to call for $c_{2}=({n}/{n_{\mathcal{Q}}})$
and as well as a $c_{1}=({n}/{n_C})^{1/2}$.  An $A$/$\tilde{A}$
compromise will allow us to keep $c_{2}=1$.

To this end, substituting $(n_{\mathcal{Q}}/n)A_{21}$ for $\tilde{A}_{21}$ in \eqref{eq:6} while leaving
$\tilde{A}_{22}$ in place, $\operatorname{Cov}(\hat\tau) \approx$
  \begin{align*}
  &\tilde{A}_{22}^{-1}\{\frac{1}{n} M_{22} -
                                 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}^{1/2}n^{3/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{n_{\mathcal{C}}n^{2}}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}\tilde{A}_{22}^{-t}\nonumber
    \\
  &= \frac{1}{n}\tilde{A}_{22}^{-1}\{M_{22} -
                                 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}^{1/2}n^{1/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{n_{\mathcal{C}}n}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}\tilde{A}_{22}^{-t} .\nonumber
  \end{align*}
 We engender corresponding estimates of $\operatorname{Cov}(\hat\tau)$ by setting \eqref{eq:22}'s
 $(c_{2}, c_{1})$ to $(1, n_{\mathcal{Q}}n_{\mathcal{C}}^{-1})$, that is having
 \texttt{estfun.DA()} return $\Psi -
 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
 \hat{A}_{11}^{-1}\hat{A}_{21}^{t}$, or simply $\Psi$ if $n_{\mathcal{C}}=0$, while having \texttt{bread.DA()}
 return $\hat{\tilde{A}}_{22}^{-1}$.


  \subsubsection{Clustered estimation of meat matrices}
For model-based estimation with units of observation and of assignment being
the same, $M$'s are estimated as follows.
\begin{equation}\label{eq:7}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} \Phi'\Phi & \hat{M}_{12} = (n_{\mathcal{C}}^{-1/2}\Phi)'(n^{-1/2} \Psi)\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}\Psi'\Psi.\\
    \end{array}
  \end{equation}
  In the more general case with potential clustering of observation
units, by units of assignment and maybe more, we can express
basic estimators with matrix (rather than summation) notation as
follows. Let
$F$ be the one-hot encoding of cluster membership, that is the \{0,1\}-valued $n \times m$ matrix with an indicator
column for each cluster. The HC0 estimators of meat matrix components
are then
\begin{equation} \label{eq:8}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} = [n_{\mathcal{C}}^{-1/2} (F'\Phi)]'[n^{-1/2} (F'\Psi)]\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}(F'\Psi)'(F'\Psi)\\
    \end{array}
  \end{equation}

\subsection{Realization of goal} \label{sec:realization-goal}
Leaving the clusterwise aggregation to \texttt{sandwich}, as
appropriate, it seems that if we define
\texttt{sandwich::estfun.teeMod()} to return an $n \times \tilde{k}$ matrix
\begin{equation} \label{eq:15}
  \Psi -
  \frac{n}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{\tilde{A}}_{21}' = \Psi -
 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
 \hat{A}_{11}^{-1}\hat{A}_{21}^{\prime},
\end{equation}
while \texttt{sandwich::bread()} as applied to
\texttt{teeMod} objects returns $\hat{\tilde{A}}_{22}^{-1} = nn_{\mathcal{Q}}^{-1}\hat{A}_{22}^{-1}$, then the effect of this will be for \texttt{sandwich::sandwich()}
and \texttt{sandwich::vcovCL()} to return \eqref{eq:6} with
$\hat{M}$'s,  as
given by \eqref{eq:7} or \eqref{eq:8} respectively, substituted for
$M$'s, and with $\hat{A}$'s for $A$'s.

(In \eqref{eq:15}
we use
\begin{align*}
  \hat{A}_{11}(\beta) &\defeq  n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\nabla_{\beta}\phi(\vec{v}_{i};
        \beta ),\\
  \hat{\tilde{A}}_{21}(\beta) &\defeq n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\nabla_{\beta}\psi(\vec{v}_{j};
        \tau, \alpha_{0}, \beta ),\\
  \hat{A}_{11}&\defeq \hat{A}_{11}(\hat\beta)\, \text{and}\, \hat{\tilde{A}}_{21}\defeq \hat{\tilde{A}}_{21}(\hat\beta).
\end{align*}
Under the model based
formulation, $\hat{\tilde{A}}_{21}(\cdot) \equiv \tilde{A}_{21}(\cdot)$; when the
first-stage model is a glm, also $\hat{A}_{11}(\cdot) \equiv
\tilde{A}_{11}(\cdot)$.  However, in design based formulations neither
equivalence holds in general.)

Alternately put,
defining $\tilde{k}$ by 1 random matrices
\begin{equation}\label{eq:10}
    \psi_\text{DA}(\vec{V}; \tau, \alpha_{0},
    \beta) = \psi (\vec{V}; \tau,\alpha_{0},
    \beta) -
    \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}
    \hat{{A}}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{V}_{i};
    \beta),
\end{equation}
assuming the first-stage fit to be a glm, in the model-based formulation we have
\begin{multline}
  \operatorname{Cov}\left(n^{-1/2}\sum_{i=1}^{n} \psi_\text{DA}(\vec{V}_{i}; \tau_{(n)},\alpha_{(n)},
    \beta_{(n)})\right) = \\
  M_{22} -
                                 \frac{n_{\mathcal{Q}}}{(n n_{\mathcal{C}})^{1/2}}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{nn_C}{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}{A}_{21}^{t}, \label{eq:12}
                               \end{multline}
regardless of what adjustments for
clustering or heteroskedasticity we might seek to apply.


In the design-based formulation such equality would not generally
hold, as in that setting
$\hat{\tilde{A}}_{21}(\beta_{(n)})$ and $\hat{A}_{11}(\beta_{(n)})$ needn't
coincide with $\tilde{A}_{21}(\beta_{(n)})$ and ${A}_{11}(\beta_{(n)})$, respectively.  In both design- and model-based settings, absorption (e.g. of intercepts) introduces additional wrinkles.

This generalizes readily beyond comparison regressions of the type encoded in \eqref{eq:14}. Should $\tilde{\psi}(\cdot; \cdot)$ encode such a regression with a subgroup or continuous moderator variable, or even with the offset $g(\vec{x}\beta)$ replaced with independent variables $\lambda_{0}g(\vec{x}\beta) + \sum_{j=1}^{k}\lambda_{j} g(\vec{x}\beta)\indicator{z_{i}=j}$, then we only have to generate a corresponding ${A}_{21}(\cdot)$ to use
\begin{equation*}\label{eq:24}
      \tilde{\psi}_\text{DA}(\vec{V}; \tau, \alpha_{0},
    \beta) = \tilde{\psi} (\vec{V}; \tau,\alpha_{0},
    \beta) -
    \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}
    \hat{\mathrm{A}}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{V}_{i};
    \beta)
\end{equation*}
in combination with $\hat{\tilde{A}}_{22}^{-1}$ as bread matrix. As long as the comparison regression value inherits from \texttt{lm} (or \texttt{glm}), \texttt{sandwich::bread()} as applied to it will with little or no modification return
$\hat{\mathrm{A}}_{22}^{-1}$, which we can readily rescale to obtain $\hat{\tilde{\mathrm{A}}}_{22}^{-1}$; \texttt{sandwich::vcov()} and \texttt{sandwich::vcovCL()} should do the right thing.


\section{Accommodating intercept(s) via absorption}\label{sec:accomm-interc-via}
For treatment effect estimators that partial out block fixed effects,
but do so without saddling the comparison or Q-regression with a new
parameter for each block, we use the
Frisch-Waugh-Lovell (FWL) Theorem to get rid of
it: use preliminary $\owt[]$-weighted least-squares regressions of
treatment and of moderator variables, if present, on block indicator
variables, to create block-mean centered versions of these intended
regressors; optionally recenter the outcome variable by subtracting
off some measure of central tendency; and finally regress intended outcome variables (with or without
prior centering) on the transformed
regressors.\footnote{This is the variant of the FWL theorem calling
  for regression of the unchanged independent variable on residualized
or partialed out dependent variables.  See e.g. Section 8.4.1,
``Regressing the partialled-out X on the full Y,'' of
\href{https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html}{Robinson's
  blog writeup of Wooldridge's 10 biggest principles of econometrics},
which in turn references Angrist \& Pitschke (2009).}
This ``absorbed'' regression involving transformed regressors is also weighted by $\owt[]$,
and can be fitted with or without an intercept.\footnote{Setting aside
numerical differences, the presence or absence of an intercept won't
matter.}  
In the process, we implicitly introduce at least $k$ new propensity score parameters
for each block in order to absorb block effects into the
independent variable(s) representing interest parameters.
These are ``slack'' parameters that
in actuality are known, in RCTs and in observational studies analyzed
according to the mimetic analysis template;  the fact that information about them
need not accumulate as the sample size grows does not in itself
threaten the asymptotics. Estimates of them are determined in a prior and separate regression
fit, and have an instrumental role.
For standard error estimation we fold them into the estimator stack in the same way that a prior
covariance model is folded in.

The canonical reference implementation would have introduced $[\alpha_{1}, \ldots, \alpha_{s}]$
as block-specific intercept parameters, turning
estimating functions \eqref{eq:14} into
\begin{multline}
  \label{eq:9}
  \psi(\vec{v}; \tau, \alpha, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}} \owt[]\times \\
  \left(
    \begin{array}{c}
           [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} -
      \tau_{z_{i}}]\indicator{b_{i}=1}\\
      \vdots\\
          {} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{s} -
      \tau_{z_{i}}]\indicator{b_{i}=s}\\
         {}  [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \sum_{r=1}^s\alpha_r\indicator{b_i=r}-\tau_{1}]\indicator{z_{i}=1}\\
                  \vdots \\
          {} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                 \sum_{r=1}^s\alpha_r\indicator{b_i=r} - \tau_{k}]\indicator{z_{i}=k}\\
    \end{array}
\right);
\end{multline}
we won't be introducing these parameters.
Recall that $\tau_{0} = \rho_{0}-\rho_{0}=0$.
In contrast to the situation with a grand-mean
intercept but no block effects (as reflected in \eqref{eq:14}), \eqref{eq:9}'s block-specific intercepts $\alpha_s$ do not ordinarily correspond to block means under control.\footnote{%
The argument of \S~\ref{sec:psit-rho_0-tau} no longer applies
to show the system of equations
$0=\sum_{i}\psi(\vec{v}_{i}; \tau, \alpha, \beta)$
equivalent to $0=\sum_{i}\grave{\psi}(\vec{v}_{i}; \tau,
\alpha, \beta)$, where $\grave{\psi}(\vec{v}; \tau,
\alpha, \beta)$ has uppermost $s$ entries
\begin{equation*}
    \left(
    \begin{array}{c}
           [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} ]\indicator{b_{i}=1} \indicator{z_{i}=0}\\
      \vdots\\
          {} [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{s}]\indicator{b_{i}=s} \indicator{z_{i}=0}\\
    \end{array}
    \right),
  \end{equation*}
in parallel with \eqref{eq:5}, but bottom $k$ entries the same as ${\psi}(\vec{v}_{i}; \tau,
\alpha, \beta)$'s. That is, it may not be the case that the $\alpha$-solutions of
$0=\sum_{i}\psi(\vec{v}_{i}; \tau, \alpha, \beta)$ are
block-specific control group means.  Accordingly its $\tau$-solutions
do not necessarily bear interpretation as contrasts of Hajek estimators.
}

\subsection{Right-side-only absorption}\label{sec:right-side-only}
As applied to a \texttt{teeMod} object with
\texttt{absorbed\_intercepts} set to \texttt{T}, we need \texttt{estfun()} to switch out the system of equations
$0=\sum_{i}\psi(\vec{v}_{i}; \tau, \alpha, \beta)$
for the pair of systems $0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$ and
$0= \sum_{i}\acute{\psi}(\vec{v}_{i}; \tau, \tilde{\alpha}_{0}, \alpha_{0}, \mathbf{p},
\beta)$,  where
\begin{align}
  \absorbInterceptsEF(\vec{v}_{i}; \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{ccc}
      (\indicator{z_{i}=0}-p_{01})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s})\indicator{b_{i}=s}
      \\
      (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  \right)%
  \right) ;\nonumber \\
  \acute{\psi}(\vec{v}_{i}; \alpha_{0}, \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber \\
&\left(
  \begin{aligned}
    \indicator{z_{i}=0} \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\tilde{\alpha}_{0}\big]
    &(
   1 - \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}) \\
\big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}\big]& \\
   \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}\big]
    &(
    \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - {\alpha}_{0}-
    \tau_{z_{i}}\big]
    &(\indicator{z_{i}=k}
    - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\
  \end{aligned}
\right) . \label{eq:upsilondef}
\end{align}
Here $\alpha_{0}$ is a single intercept
that does not arise
in the canonical reference implementation \eqref{eq:9} without absorption, which has $\alpha_{1},
\ldots, \alpha_{s}$ but not $\alpha_{0}$,
but is presumed to be included in the
regression of the outcome on block-absorbed treatment indicators.  And
$\tilde{\alpha}_{0}$ is a new intercept parameter determined following
that regression, using partial residuals $y - g(\vec{x}\beta)$ and
estimated blockwise treatment proportions $(\hat{p}_{ij}:i \leq k, j
\leq r)$. In general $\alpha_{0}$ and $\tilde{\alpha}_{0}$ do not
coincide: $\tilde{\alpha}_{0}$ and its corresponding
$\acute{\Psi}$-column, $\hat{A}_{21}$ row and $\hat{\tilde{A}}_{22}$
row
% are not determined by the \texttt{lm} object furnishing the remaining
% coefficients and $\acute{\Psi}$, $\hat{A}_{21}$ or
%$\hat{\tilde{A}}_{22}$ entries, and
will have to be figured separately (and can replace
${\alpha}_{0}$ entries of the \texttt{coefficient} vector,
columns of the \texttt{estfun} and rows and columns of bread
matrices)\footnote{%
  We don't need the ${\alpha}_{0}$ row of $A_{21}$ or $A_{22}$ because we
  aren't interested in standard errors or covariances involving
  ${\alpha}_{0}$; so it's OK to replace to those rows with
  $\tilde{\alpha}_{0}$ rows. Matrix $A_{22}$'s $\tilde{\alpha}_{0}$- and
  ${\alpha}_{0}$-columns are zero except for diagonal entries
  $(\tilde{\alpha}_{0}, \tilde{\alpha}_{0})$ and $(\alpha_{0},
  \alpha_{0})$:
  that the ${\alpha}_{0}$ column of $A_{22}$ is zero outside of the diagonal
  follows from the fact that in light of $p_{jr}$'s implicit definition via
  $\EE \absorbInterceptsEF(\vec{V}; \mathbf{p})=0$,
  $\sum_{i: b_{i}=r}\EE\{\owt[][\indicator{Z_{i}=j} - p_{jr}]\}$=0; the
  $\tilde{\alpha}_{0}$ column is 0 because $\tilde{\alpha}_{0}$ doesn't figure in
  other terms. So, having replaced ${\alpha}_{0}$ rows
  of $A_{2*}$ with 
  with $\tilde{\alpha}_{0}$ rows, nothing else in the
  ${\alpha}_{0}$ column of $A_{22}$ needs to be replaced in order to relabel
it with ``$\tilde{\alpha}_{0}$.''}.
It is interpretable as a weighted average of control group
observations, as described in Section~\ref{sec:interc-relat-aspects} below.

By simplifying sums of \eqref{eq:upsilondef},  block $b$'s contribution
$\sum_{i \in (\bigcup b)} \acute{\psi}(\vec{v}_{i};
\tilde{\alpha}_{0}, \alpha_{0} \tau,
\mathbf{p}, \beta)$ can be seen to be 
\begin{multline}
\sum\acute{\psi}_{i}=  \big(\sum_{i} \owt[]\big)
        \left(
          \begin{array}{c}
            (1-{p}_{0b})
            \hat{p}_{0b}\left(\frac{\sum_{{z_i=0}}\owt[][y_{i}[0] - g(\vec{x}_{i}\beta)-\tilde{\alpha}_{0}] }{\sum_{{z_i=0}}\owt[]}
            \right)\\
         {} (1-{p}_{1b})
            \hat{p}_{1b}\frac{\sum_{{z_i=1}}\owt[]\tilde{y}_{i}}{\sum_{{z_i=1}}\owt[]}  -
      p_{1b}(1-\hat{p}_{1b})\frac{\sum_{{z_i\neq
            1}}\owt[]\tilde{y}_{i}}{\sum_{{z_i\neq 1}}\owt[]}\\
      \vdots\\
         {}
            (1-{p}_{kb})\hat{p}_{kb}\frac{\sum_{{z_i=k}}\owt[]\tilde{y}_{i}}{\sum_{{z_i=k}}\owt[]}  -
      p_{kb}(1-\hat{p}_{kb})\frac{\sum_{{z_i\neq
            k}}\owt[]\tilde{y}_{i}}{\sum_{{z_i\neq k}}\owt[]} \\
          \end{array}
        \right)\\
= 
        \left(
          \begin{array}{c}
            (1-{p}_{0b})
            {\sum_{{z_i=0}}\owt[] [y_{i}[0] - g(\vec{x}_{i}\beta)-\tilde{\alpha}_{0}]}
            \\
         {} (1-{p}_{1b})
            {\sum_{{z_i=1}}\owt[]\tilde{y}_{i}}  -
      p_{1b}{\sum_{{z_i\neq
            1}}\owt[]\tilde{y}_{i}}\\
      \vdots\\
         {}
            (1-{p}_{kb}){\sum_{{z_i=k}}\owt[]\tilde{y}_{i}}  -
      p_{kb}{\sum_{{z_i\neq
            k}}\owt[]\tilde{y}_{i}} \\
          \end{array}
        \right)\\
  \label{eq:31}
\end{multline}
where sums range over $\bigcup b$ and $\tilde{y}_{i}$ is the residual
$y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}$.

(If \texttt{absorb\_intercepts=FALSE}, then we'll revert to \eqref{eq:14}, i.e.
\begin{equation*}
         \psi(\vec{v}_{i};
  \alpha_{0}, \tau, \beta) = \indicator{i \in \bigcup \mathcal{Q}}\owt[]
         \left( \begin{array}{l}
           y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_0 - \tau_{z_{i}}\\
           {}[y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_0-\tau_{1}]\cdot \indicator{z_{i}=1}\\
                  \vdots \\
           {}[y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_0 - \tau_{k}]\cdot \indicator{z_{i}=k}\\%
                \end{array}\right),
\end{equation*}
skipping the introduction of slack parameters $\mathbf{p}$ and
replacement intercept $\tilde{\alpha}_{0}$.
In this case users will have to use inverse probability weighting (via
$\owt[]$ factors) to ensure causal interpretability in light of the
specification;  $\hat{\alpha}_0$ will correspond to an $\owt[]$-weighted
control group mean.)

Returning to \eqref{eq:upsilondef}, in the model based formulation $[Z_{i}: i]$ is held fixed, so
solutions $\hat{\mathbf{p}}$ of
$0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$ have
covariance 0.  Writing $\acute{\Psi} =
[\acute{\psi}(\vec{v}_{i}; \mathbf{p}, \beta, \tilde{\alpha}_{0}, \alpha_{0}, \tau) : i]'$, one
obtains valid\footnote{%
The residuals contributing to $\acute{\Psi}$ via \eqref{eq:upsilondef} do not remove stratum
effects, as conventional covariance estimators for this scenario
seem to do.  This aspect would inflate these standard errors
relative to those. Nonetheless, I maintain that
\texttt{estfun.teeMod()} as described here are sound as a
basis for variance-covariance estimation.}
  sandwich estimates by substitution of $\acute{\Psi}$ for
$\Psi$ in \eqref{eq:15}.  That is, for a
\texttt{teeMod} object \texttt{DA} we should have
\begin{equation*}\label{eq:26}
\mathtt{estfun.teeMod(DA)} =
  \begin{cases}
  \acute{\Psi} -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'& \mathtt{DA} \text{ absorbs
    intercepts}\\
\Psi -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  & \text{ otherwise}.
\end{cases}
\end{equation*}

In the design-based formulation, randomness of $Z$ can propagate to
sampling variability of $\hat{\mathbf{p}}$,  so we must attend to
variance contributions from $[\absorbInterceptsEF(z_{i}, \mathbf{p}): i]$.
The $\mathbf{p}$ being another nuisance parameter estimated in a
preliminary regression, we can get variance propagation for it by
another elaboration of estimating functions along the lines of
\eqref{eq:15}. We facilitate doing
this by creating a
helper \texttt{estfun\_DB\_blockabsorb()}, applying to
\texttt{teeMod} objects \texttt{DB} and returning
\begin{equation*}
\begin{cases}
  \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' &
  \mathtt{DA} \text{ absorbs intercepts}\\
0 & \text{ otherwise}\\
\end{cases}
\end{equation*}
where
$\AbsorbInterceptsEF= [\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p}): i]'$,
$A_{\mathbf{p}, \mathbf{p}} = n^{-1}\sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E} [\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p})]$, and $A_{\tau, \mathbf{p}} = n^{-1} \sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E}[\nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \tilde{\alpha}_{0}, \alpha_{0}, \tau)]$.  For design-based variance-covariance
estimation one uses as estimating function
\begin{multline*}\label{eq:25}
  \mathtt{estfun.teeMod(DA)} -
  \mathtt{estfun\_DB\_blockabsorb(DA)} =\\
\begin{cases}
\acute{\Psi} -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' & \mathtt{DA} \text{ absorbs intercepts}\\
 \Psi -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'
 & \text{ otherwise}.\\
\end{cases}
\end{multline*}

In these displays, $A_{\mathbf{p}, \mathbf{p}}$ is
an average of diagonal matrices $\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p})$ with $-1$s for diagonal entries $(b_{i}-1)(k+1)+1, \ldots,
b_{i}(k+1) +1$ and zeros elsewhere, while $A_{\tau,
  \mathbf{p}}$ is a scaled sum of $\mathbb{E} \nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \tilde{\alpha}_{0}, \alpha_{0}, \tau)$ terms, $i \in \bigcup \mathcal{Q}$. The average $A_{\tau,
  \mathbf{p}}$ of expected contributions $\mathbb{E} \nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha_{0}, \tau)$ is estimable by corresponding means of
random variables $\nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \tilde{\alpha}_{0}, \alpha_{0}, \tau)$.  The sample observation $i$ contribution $\nabla_{\mathbf{p}}\acute{\psi}(\vec{v}_{i};
\mathbf{p}, \beta, \tilde{\alpha}_{0}, \alpha_{0}, \tau)$ is the product of the $k \times k$
diagonal matrix with entries $y_{i}[j] - g(\vec{x}_{i}\beta) -
\tau_{j}$, $j=1, \ldots, k$, with a $k \times (ks)$ block matrix
consisting of the opposite of the $k\times k$ identity at the
$b_{i}$th block and the 0 matrix for the $r$th $k \times k$ block whenever
$r\neq b_{i}$.
\subsection{Both-sides absorption} \label{sec:both-sides-absorpt}

As ordinarily applied,  FWL Theorem ``partials out'' fixed effects
from both dependent and independent variables, but so far we've only
done so with independent variables.  Which of these we do makes no
difference for point estimates, but standard errors are another
matter.  Here we describe a design-based variance estimator making use
of both-sides absorption plus additional assumptions.

To ensure
applicability with small and moderately sized strata, we must do this without
introducing parameter for each block. Instead, define the
block residual mean function
\begin{equation*}
  \hat{\alpha}(\mathbf{v}_{b}; \theta) \defeq \left\{\sum_{i \in
    (\bigcup b)}\owt{} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta) -
  \alpha_{0} - \tau_{z_{i}}]\right\}\Big/ \left(\sum_{i \in (\bigcup b)}w_{i}\right),
\end{equation*}
where $\mathbf{v}_{b} = \Big\{\vec{v}_{i} : i \in \big(\bigcup b \big)\Big\}$ is the matrix of
data-and-potential-outcomes vectors for block $b \subseteq \mathcal{Q}$ and $\theta = (\tau, \alpha_{0}, \beta)$.  Now consider the pair
of systems $0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$
and
$0= \sum_{i}\breve{\psi}(\vec{v}_{i}; \tau, \alpha_{0}, \mathbf{p},
\beta)$, where $\absorbInterceptsEF(\cdot)$ is as above and for blocks
$b \subseteq \mathcal{Q}$,
\begin{multline}
  % \absorbInterceptsEF(\vec{v}; \mathbf{p}) &=
  %                                            \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  % &\operatorname{vec}\left(%
  %   \left(
  %   \begin{array}{ccc}
  %     (\indicator{z_{i}=0}-p_{01})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=0}-p_{0s})\indicator{b_{i}=s}
  %     \\
  %     (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
  %     \\
  %     \vdots & \vdots & \vdots \\
  %     (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  % \right)%
  % \right) ;\nonumber \\
  \breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p}) =
\sum_{i \in (\bigcup b)}\owt[] \times  \\
\left(
  \begin{aligned}                                               
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\tilde{\alpha}_{0}\big]
    &\indicator{z_{i}=0} (1 - p_{0b}) \\
    y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0} - \tau_{z_{i}}&\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}} - &\hat{\alpha}(\mathbf{v}_{b}; \theta)\big]
    (
    \indicator{z_{i}=1} - p_{1b})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{0}-
    \tau_{z_{i}} - &\hat{\alpha}(\mathbf{v}_{b}; \theta)\big]
    (\indicator{z_{i}=k} - p_{kb})\\
  \end{aligned}
\right) . \label{eq:upsilon-rsa-def}
\end{multline}
Denoting as $(\hat{p}_{0b}, \ldots, \hat{p}_{kb})$ the solutions in
$({p}_{0b}, \ldots, {p}_{kb})$ of
$0= \sum \{\absorbInterceptsEF(\vec{v}_{i}; {p}_{0b}, \ldots,
{p}_{kb}) : i \in (\bigcup b)\}$, and recalling that  $\tilde{y}_{i}$
is the residual
$y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}$, 
\eqref{eq:upsilon-rsa-def} gives, with sums understood to range over
$(\bigcup b)$:
\begin{multline*}
    \breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p}) = \Big(\sum_{i} \owt[]\Big) \times\\
        \left(
          \begin{array}{c}
            (1-{p}_{0b})
            \hat{p}_{0b}\left[\frac{\sum_{{z_i=0}}\owt[][{y}_{i}[0] - g(\vec{x}_{i}\beta)]}{\sum_{{z_i=0}}\owt[]}
            -
            \tilde{\alpha}\right]\\
            \frac{\sum \owt[] \tilde{y}_{i}}{\sum \owt[]} \\
         {} (1-{p}_{1b})
            \hat{p}_{1b}\frac{\sum_{{z_i=1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i=1}}\owt[]}  -
      p_{1b}(1-\hat{p}_{1b})\frac{\sum_{{z_i\neq
            1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i\neq 1}}\owt[]}\\
      \vdots\\
         {}
            (1-{p}_{kb})\hat{p}_{kb}\frac{\sum_{{z_i=k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i=k}}\owt[]}  -
      p_{kb}(1-\hat{p}_{kb})\frac{\sum_{{z_i\neq
            k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i\neq k}}\owt[]} \\
          \end{array}
        \right)\\
= 
        \left(
          \begin{array}{c}
            (1-{p}_{0b})
            {\sum_{{z_i=0}}\owt[]{[{y}_{i}[0] - g(\vec{x}_{i}\beta)
            -
            \tilde{\alpha}_{0}]}}\\
            \sum \owt[] \tilde{y}_{i}\\
         {} (1-{p}_{1b})
            {\sum_{{z_i=1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}  -
      p_{1b}{\sum_{{z_i\neq
            1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}\\
      \vdots\\
         {}
            (1-{p}_{kb}){\sum_{{z_i=k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}  -
      p_{kb}{\sum_{{z_i\neq
            k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]} \\
          \end{array}
        \right).\\
\end{multline*}
Under the assumption that $\tau$ captures person-level treatment
effects, $\operatorname{Var}[ \hat{\alpha}(\mathbf{v}_{b}; \theta) \mid \mathcal{X},
\mathcal{Z}]=0$. Under this assumption, then, design-based covariances
of $\breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p})$ are calculated in
the same way that design-based covariances of
$\acute{\psi}(\vec{v}_{b}; \theta, \mathbf{p})$ are calculated from
\eqref{eq:31}.  (To implement this design-based estimator, we can
define a function operation on \texttt{teeMod} objects that replaces
the residuals $\tilde{y}_{i}$ with block-centered residuals  $\tilde{y}_{i} -
\hat{\alpha}(\mathbf{v}_{b}; \hat\theta)$, where $b \ni i$, and then
proceeds as the calculation of design-based vcov with right-side absorption.)


\section{Replacement intercepts for \texttt{teeMod} objects}%
\label{sec:repl-interc-textttt}
Section~\ref{sec:psit-rho_0-tau} establishes that for estimation of
main effects, with blocks (if present) accommodated by weights rather than
absorption of one-way fixed effects, the intercept equals the
$\owt[{[0]}]$-weighted control group mean of partial residuals
$y[0] - g(\vec{x}\hat\beta)$. Display~\eqref{eq:upsilondef} and subsequent discussion of
Section~\ref{sec:accomm-interc-via} indicated that our one-way fixed effects absorption will
also report a weighted control group
mean of the same quantities.  When creating \texttt{teeMod} objects in
general, we seek to ensure the presence
of intercept coefficients bearing interpretation as control-group
means of $[y[0] - g(\vec{x}\hat\beta)]$.
This will call for some light editing of \texttt{lm()}'s
coefficient names; the addition of one or more new coefficients, to
record control group means of $g(\vec{x}\hat\beta)$; and wholly new
calculations for one-way fixed effect absorption scenarios.

\subsection{Intercept-related aspects of  \texttt{teeMod}s in general}%
\label{sec:interc-aspects-gen-teeMods}
\subsubsection{\texttt{(Intercept)}s with main effect and continuous
  moderator}
With a main effect only, our \texttt{(Intercept)} coefficient
represents a control group mean. Without absorption, we get this by
default from \texttt{lm()}; neither the
coefficient nor its name calls for adjustment.  (With absorption, we'll
report the $\hat{\tilde{\alpha}}_{0}$ determined by
the top entry in \eqref{eq:upsilondef}'s specification of
$\acute{\psi}(\cdot)$, also interpretable as a weighted control group mean; see Sections~\ref{sec:accomm-interc-via} above
and \ref{sec:interc-relat-aspects} below.)

With a continuous moderator \texttt{m}/$m$, our calls to \texttt{lm()} or
\texttt{lm.wfit()} return both intercept and moderator main
effect coefficients, ``\texttt{(Intercept)}'' and ``\texttt{m}.''
Without absorption, \texttt{(Intercept)} is the $\owt[{[0]}]$-weighted control group mean of $y[0] - g(\vec{x}\hat\beta) -
\hat{\lambda}m$. In the scenario with absorption, we'll also report
``\texttt{(Intercept)}'' and ``\texttt{m}'' coefficients, with the
\texttt{m} coefficient taken from the same \texttt{lm()} fit providing
effect estimates but with a separate calculation, described in Section~\ref{sec:interc-relat-aspects} below,
providing the value of the \texttt{(Intercept)} coefficient. 

\subsubsection{No \texttt{(Intercept)} with a subgrouping
  moderator%
  \protect\footnote{A proposal. Implementation can be prosponed while
  it's under consideration and the changes indicated elsewhere in this
  section are pursued.}%
}
With a categorical moderator \texttt{sg}, \texttt{lmitt.formula()}'s call to
\texttt{lm()} reports coefficients from \texttt{y
  \textasciitilde\ assigned():sg + sg}: $k$ treatment-control contrasts,
\texttt{z.:sg1}, \ldots, \texttt{z.:sg<k>}; a
coefficient labeled ``\texttt{(Intercept)}'' and $k-1$
coefficients \texttt{sg2}, \ldots, \texttt{sg<k>}.  In the scenario
without absorption, the \texttt{(Intercept)} that such a call to \texttt{lm()}
returns is a weighted mean of
controls with \texttt{sg == 1}, while the similarly weighted control mean
of subgroup 2 would be given \texttt{(Intercept)
+ sg2} (and similarly for subgroups 3, \ldots, $k$).
To improve clarity we rotate and relabel within \texttt{lmitt.formula}:
\begin{itemize}
\item for each subgroup variable category \texttt{<s>} other than the
  reference category (eg \texttt{1}), replace the \texttt{lm}-returned value at
  position \texttt{sg<s>} by its sum with the value at
  \texttt{(Intercept)}; 
\item re-label \texttt{(Intercept)} as
  ``\texttt{sg<refcategory>}'' (in the running
  example, \texttt{sg1}).
\end{itemize}

These modifications would have to be reflected also in
%\texttt{estfun.teeMod()} and 
\texttt{bread.teeMod()}.
% In \texttt{estfun.teeMod()}, the values of
% \texttt{.get\_a21()} and\texttt{.get\_tilde\_a21()} have to be
% post-multiplied by the Jacobian...
For \texttt{bread.teeMod()}, the values of \texttt{.get\_a22\_inverse()}
and \texttt{.get\_tilde\_a22\_inverse()} need to be pre-multiplied by
the inverse Jacobian %\marginpar{Check me!}
% --This checks out. Let
% $\tilde{theta}$ be the (old) parametrization with intercept and $\theta$
% the new parametrization without intercept, and let 
% $f: \theta \mapsto \tilde{\theta}$. The Jacobian of
% $\Psi_{i}(f(\theta)$ is
% $J_{\Psi_{i}}(\tilde\theta)J_{f}(\theta)$;
% inversion switches the ordering of the inverted Jacobians.
of the linear transformation
taking the new parameters \texttt{sg1}, \texttt{sg2}, \ldots,
\texttt{sgk}, \texttt{z.:sg1}, \texttt{z.:sg2}, \ldots,
\texttt{z.:sgk}) back to the old parameters (\texttt{(Intercept)}, \texttt{sg2}, \ldots,
\texttt{sgk}, \texttt{z.:sg1}, \texttt{z.:sg2}, \ldots,
\texttt{z.:sgk}).
This Jacobian and its inverse take the forms
\begin{equation*}
  \begin{blockarray}{rcccccccc}
& \text{sg1}& \text{sg2} & \cdots & \text{sgk} & \text{z.:sg1} & \text{z.:sg2}& \cdots & \text{z.:sgk} \\ 
    \begin{block}{r(cccccccc)}
         \text{Int} &1  &&&&&&&\\
    \text{sg2}&-1  &1&&&&&&\\
    \vdots& \vdots &&\ddots&&&&&\\
    \text{sgk}&-1 &&&1&&&&\\
    \text{z.:sg1}&  &&&&1&&&\\
    \text{z.:sg2}&  &&&&&1&&\\
    \vdots&  &&&&&&\ddots&\\
    \text{z.:sgk}& &&&&&&&1\\
    \end{block}
  \end{blockarray}
\end{equation*}
and, respectively, 
\begin{equation*}
  \begin{blockarray}{rcccccccc}
& \text{sg1}& \text{sg2} & \cdots & \text{sgk} & \text{z.:sg1} & \text{z.:sg2}& \cdots & \text{z.:sgk} \\ 
    \begin{block}{r(cccccccc)}
         \text{Int} &1  &&&&&&&\\
    \text{sg2}&1  &1&&&&&&\\
    \vdots&\vdots  &&\ddots&&&&&\\
    \text{sgk}&1 &&&1&&&&\\
    \text{z.:sg1}&  &&&&1&&&\\
    \text{z.:sg2}&  &&&&&1&&\\
    \vdots&  &&&&&&\ddots &\\
    \text{z.:sgk}& &&&&&&&1\\
    \end{block}
  \end{blockarray},
\end{equation*}
with blanks indicating zeros.

\subsection{Intercept-related aspects of absorption  (\texttt{lmitt(y
    \textasciitilde\ \ldots, absorb=T)})}\label{sec:interc-relat-aspects}
With the usage \texttt{lmitt(y \textasciitilde\ \ldots, absorb=T)},
treatment assignment indicators are block-mean centered prior to
fitting, rendering intercept terms superfluous.  The underlying
\texttt{lm} call may or may not allow for an intercept. If it did,
then the intercept \texttt{lm} would have returned isn't meaningful
and needs to be replaced in accord with \eqref{eq:upsilondef}.  That
is, with a weighted average of control-group partial residuals $y[0] -
g(\vec{x}\hat\beta)$, with weights following the
block fixed effects estimator's implicit weighting scheme.

\subsubsection{Main effect only}
With block fixed effects, estimating equations defining
the main effect estimators $\hat{\tau}_{j}$,
$1\leq j \leq k$,  admit expression in terms of differences of weighted averages
under treatment and control.  For each $j$ there are unit weights $\{w_{\text{BFE}i}:
i\}$ determined as follows. For blocks $b \subseteq \mathcal{Q}$,
empirical solutions $\{\hat{p}_{jb}:b\}$  of $\sum_{i \in \bigcup b}
\absorbInterceptsEF (z_{i}; p_{jb})=0$ (cf. \eqref{eq:upsilondef}) and
for $i \in \bigcup b$,
\begin{equation} \label{eq:28}
w_{\text{BFE}i(j)} = 
  \begin{cases}
(1-\hat{p}_{jb})\owt[]& z_i=j \text{ and } 0 < \sum_{i \in \bigcup b} \owt[] \indicator{z_i=j} < \sum_{i \in \bigcup b} \owt[];\\ 
\hat{p}_{jb} \owt[] & z_i\neq j \text{ and }0 < \sum_{i
  \in \bigcup b} \owt[] \indicator{z_i=j} < \sum_{i \in \bigcup b} \owt[]\\
0 &  \sum_{i \in \bigcup b} \owt[] \indicator{z_i=j} = 0 \text{ or }
  \sum_{i \in \bigcup b} \owt[].
\end{cases}
\end{equation}
With a binary treatment this is
\begin{equation*}
w_{\text{BFE}i} = 
  \begin{cases}
(1-\hat{p}_{b_{i}})\owt[]& z_i=1 \text{ and } 0 < \sum_{i \in \bigcup b} z_i\owt[] < \sum_{i \in \bigcup b} \owt[];\\ 
\hat{p}_{b_{i}} \owt[] & z_i=0 \text{ and }0 < \sum_{i
  \in \bigcup b} z_i\owt[] < \sum_{i \in \bigcup b} \owt[]\\
0 &  \sum_{i \in \bigcup b} z_i\owt[]  = 0 \text{ or }
  \sum_{i \in \bigcup b} \owt[],
\end{cases}
\end{equation*}
with ``$(1-p_{b})$'' and ``$p_{b}$'' corresponding to``$p_{0b}$''
and ``$p_{1b}$,'' respectively, as defined by \eqref{eq:upsilondef}.
These ``block fixed effects weights'' then resemble overlap weights,
$z_{i}(1-\pi_{i}) + (1-z_{i})\pi_{i}$ for $i\in \bigcup b$. (They coincide with
overlap weighting in the special case of RCTs with binary treatment,
units of analysis that coincide with units of assignment,
and prior case weights $\{w_{i}:i\}$ not present or not varying within blocks.)


With a binary or dichotomized treatment, these 
weights' role in determining effect estimates admits a
succinct description: $\hat{\tau}$ is simply the difference of block fixed
effects-weighted averages of treatment and control groups.  So in this
case we'll report in the
$\tilde{\alpha}_{0}$ position the $w_{\text{BFE}}$-weighted
control group average of $y[0] - g(\vec{x}\hat\beta)$ (or of $y[0]$ if
there was no prior covariance adjustment model); this
$\hat{\tilde{\alpha}}_{0}$ solves the empirical estimating equation $\sum_{i
  \in \bigcup\mathcal{Q}}\acute{\psi}_{i1}(\alpha)=0$, where
$\acute{\psi}_{i1}(\cdot)$ is a first coordinate of
$\acute{\psi}_{i}$ as in \eqref{eq:upsilondef}. As the
averaging is over controls, the relevant weights $w_{\text{BFE}i}$ take
the form $\hat{p}_{b_{i}} \owt[]$, where $b_{i}$ denotes the block $b$
that contains (the cluster containing) $i$.
Reinterpreting \eqref{eq:upsilondef} in light of
this simplification, the
$\tilde{\alpha}_{0}$ column of an object \texttt{estfun.teeMod(DA)} has entries
$$\indicator{z_{i}=0} w_{\text{BFE}i} \tilde{y}_{i}= (1-z_{i})\hat{p}_{b_{i}}\owt[{[0]}] (y - g(\vec{x}\hat{\beta}) -
\hat{\tilde{\alpha}}_{0}),$$
and the $\tilde{\alpha}_{0}$ row of $A_{21}$ is 
\begin{equation}\label{eq:11}
  -n_{\mathcal{Q}}^{-1}\sum_{i\in \bigcup\mathcal{Q} }
  (1-z_{i}){p}_{b_{{i}}}\owt[{[0]}] g'(\vec{x}_{i}\hat{\beta})
  \vec{x}_{i}, 
\end{equation}
with $(1-z_{i})$ replaced by $\operatorname{Pr}_{\mathcal{D}}(Z_{i}=0)
= 1-\pi_{i}$ in the design-based formulation. 
The $\tilde{\alpha}_{0}$ row of $\tilde{A}_{22}$ has
in the $\tilde{\alpha}_{0}$ column
\begin{equation}\label{eq:32}
  -n^{-1}\sum_{i \in \cup \mathcal{Q}}
(1-z_{i}){p}_{b_{{i}}}\owt[{[0]}],
\end{equation}
or $-n^{-1}\sum_{i \in \cup \mathcal{Q}}
(1-\pi_{i}){p}_{b_{{i}}}\owt[{[0]}]$ in the design-based formulation,
and zeroes in remaining columns (as $\tau$ parameters don't
contribute to $\acute{\psi}_{i1}$). For $\hat{\tilde{A}}_{22}$ we resolve the
``$p_{b_{i}}$'' in this expression as $\hat{p}_{b_{i}}$.%
\footnote{In RCTs $p_{b_{i}}$ can be calculated. Another approach would
  be to perform that calculation and use it rather than
  $\hat{p}_{b_{i}}$ for $p_{b_{i}}$; another approach would be to use
  $p_{b_{i}}$ in RCTs and $\hat{p}_{b_{i}}$ in observational studies.}
The $\tilde{\alpha}_{0}$
column of $\tilde{A}_{22}$ has zeroes off of the diagonal as well:
for blocks $b \subseteq \mathcal{Q}$ and the $\tau$-coordinates $t=2,
\ldots, k+1$ of the $k+1$ vector $\acute{\psi}$, \eqref{eq:31} gives
that
\begin{align*}
  \sum_{i \in
  \bigcup b}\frac{\partial}{\partial \tilde{\alpha}_{0}}\acute{\psi}_{i t} &=
                                                                     -[(1-p_{b})\sum_{z_{i}=1}\owt[] - p_{b}\sum_{z_{i}=0}\owt[]]\\
                                                                    &=
                                                                      -\sum
                                                                      [z_{i}(1-p_{b}) +(1-z_{i})(0-p_{b})]\owt[]\\
   &= \sum (\indicator{z_{i}=1} - p_{b})\owt[],
\end{align*}
evaluating in expectation to 0 
because of \eqref{eq:upsilondef}'s
defining property $\sum_{i \in \bigcup \mathcal{Q}}\EE
\absorbInterceptsEF(\vec{V}_{i}; \mathbf{p}) =0$ of $\mathbf{p} =
(p_{jr}: 1\leq j\leq k, 1\leq r\leq s)$. 

To estimate $A_{21}$ and $A_{22}$ under the design-based formulation
we'll continue to use \eqref{eq:11} and \eqref{eq:32} to approximate
the $\tilde{\alpha}_{0}$ row of $A_{21}$ and the $\tilde{\alpha}_{0}$ column of
$\tilde{A}_{22}$, respectively. This avoids having to keep track of
$\owt[{[0]}]$ for observations $i$ for which $z_{i}\neq 0$, and in
observational studies it avoids the 
need to estimate $(\pi_{i}: i)$.  Taken as random variables,
\eqref{eq:11} and \eqref{eq:32} are
both design-unbiased for their corresponding estimands given by
the same expressions with $(1-\pi_{i})$ substituted for $(1-z_{i})$; this
is true in observational studies as well as experiments.

Design-based calculations also call for adjusting objects \texttt{estfun.teeMod(DA)} by subtracting off
\texttt{estfun\_DB\_blockabsorb(DA)} = $\AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tilde{\alpha}_{0}\tau\,\mathbf{p}}'$, per \S~\ref{sec:accomm-interc-via}
  above. The $\tilde{\alpha}_{0}$ row of $A_{\tilde{\alpha}_{0}\tau\,\mathbf{p}}$ has in
  its column $s$ (partial w/r/t $p_{s}$, $s \in \mathcal{Q}$)
  \begin{multline}\label{eq:33}
  \left[n^{-1}\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right](\tilde{\alpha}_{0\mathcal{D}s} -
  \tilde{\alpha}_{0\mathcal{D}}) =\\ n^{-1}\left\{\left[\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right]\tilde{\alpha}_{0\mathcal{D}s} - \left[\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right]\tilde{\alpha}_{0\mathcal{D}}\right\}, 
  \end{multline}
  for $\pi_{i} = \operatorname{Pr}_{\mathcal{D}}(Z_{i}=1)$, where
  $\pi_{i} = p_{b_{i}}$ in an RCT but not necessarily otherwise;
  \begin{equation*}
 \tilde{\alpha}_{0\mathcal{D}r}=
  \frac{\sum_{b_{i} =r}\owt[{[0]}](1-\pi_{i})[y_{i}[0] -
    g(\vec{x}_{i}\beta)]}{\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})}
  \text{ and }
  \tilde{\alpha}_{0\mathcal{D}} = \frac{\sum_{r}p_{r}\big[\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})\big]
    \tilde{\alpha}_{0\mathcal{D}r}}{\sum_{r}p_{r}\big[\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})\big]};
  \end{equation*}
  and to obtain $\hat{A}_{\tilde{\alpha}_{0}\tau\,\mathbf{p}}$ we plug into our
  ${A}_{\tilde{\alpha}_{0}\tau\,\mathbf{p}}$-expression $z_{i}$'s for
  $\pi_{i}$s, ``$b_{i}$'' denoting the block within $\mathcal{Q}$
  that contains $i$ or $i$'s cluster $[i]$. The resulting estimators
  \begin{align*}
  \hat{\tilde{\alpha}}_{0 r}=&
  \frac{\sum_{i: b_{i}=r, z_{i}=0}\owt[][y_{i}[0] -
                               g(\vec{x}_{i}\beta)]}{\sum_{i:b_{i}=r,
                               z_{i}=0}\owt[]} \text{ and}\\
    \hat{\tilde{\alpha}}_{0\mathcal{D}} =&
  \frac{\sum_{r}p_{r} (\sum_{i: b_{i}=r, z_{i}=0}\owt[]) \hat{\tilde{\alpha}}_{0 r}}{%
\sum_{r}p_{r} (\sum_{i: b_{i}=r, z_{i}=0}\owt[])}
  \end{align*}
  give unbiased estimation of the first term at right of
  \eqref{eq:33}, with the second term at right of
  \eqref{eq:33} the product of an unbiased term ($\sum_{i: b_{i}=r, z_{i}=0}\owt[]$)
  and a consistent ratio estimator ($\hat{\tilde{\alpha}}_{0\mathcal{D}}$).
\subsubsection{Main effect and continuous moderator}
Again let $m$ denote a continuous moderator, $\lambda$ its
coefficient. Recall from Section~\ref{sec:interc-aspects-gen-teeMods} in the no-absorption case, the \texttt{(Intercept)}/$\alpha_{0}$
coefficient works out to the $\owt[{[0]}]$-weighted control group mean of $y[0] - g(\vec{x}\hat\beta) -
\hat{\lambda}m$. In parallel with that case, under absorption we need
to report as $\tilde{\alpha}_{0}$ the  $w_{\text{BFE}}$-weighted control group mean of $y[0] - g(\vec{x}\hat\beta) -
\hat{\lambda}m$. Equivalently, the $\acute{\psi}(\cdot)$ of
\eqref{eq:upsilondef} is replaced by
\begin{align}
    \absorbInterceptsEF(\vec{v}; \nu, \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{rrr}
      (m_{i} - \nu_{1}) \indicator{b_{i}=1} & \ldots & 
(m_{i} - \nu_{s}) \indicator{b_{i}=s} \\
      (\indicator{z_{i}=0}-p_{01})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s})\indicator{b_{i}=s}
      \\
      (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  \right)%
  \right) ;\nonumber \\
    \acute{\psi}(\vec{v}; \tilde{\alpha}_{0}, \alpha_{0}, \tau, \mathbf{p}, \beta) &=
    \indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber
  \\
&\left(
  \begin{aligned}                                               
    \indicator{z_{i}=0} \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta) - {\lambda}m-\tilde{\alpha}_{0}\big]
    &(
    1 - \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}) \\
    y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta) - {\lambda}m-\alpha_{0}-\tau_{z_{i}}& \\
\big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}- {\lambda}m-\tau_{z_{i}}\big]
    &(m_{i} - \sum_{r=1}^{s}\nu_{r}\indicator{b_{i}=r})
    \\
   \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}- {\lambda}m-\tau_{z_{i}}\big]
    &(
    \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{0}- {\lambda}m -
    \tau_{z_{i}}\big]
    &(\indicator{z_{i}=k}
    - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\
  \end{aligned}
\right) .\label{eq:30}
\end{align}
All but the $\tilde{\alpha}_{0}$ column of this $\acute{\Psi}$ estfun will be
recovered by \texttt{estfun.lm} as applied to the \texttt{lm} fitted
after absorption.  This $\tilde{\alpha}_{0}$ column can be recovered as a
$w_{\text{BFE}}$-weighted average of control group residuals, just as
with the main effect only. That is, the 
$\tilde{\alpha}_{0}$ column of an object \texttt{estfun.teeMod(DA)} has entries
$$\indicator{z_{i}=0} w_{\text{BFE}i} \tilde{y}_{i}= (1-z_{i})
\hat{p}_{b_{i}}\owt[{[0]}] (y - g(\vec{x}\hat{\beta})
-{\lambda}m -
\hat{\tilde{\alpha}}_{0}).$$
Again the $\tilde{\alpha}_{0}$ row of $A_{21}$ and $(\tilde{\alpha}_{0}, \tilde{\alpha}_{0})$
entry of $\tilde{A}_{22}$ are given by \eqref{eq:11} and
\eqref{eq:32}, respectively, with zeroes in off-diagonal positions of
$\tilde{A}_{22}$'s $\tilde{\alpha}_{0}$ row and column.
Design-based calculations from
\texttt{estfun\_DB\_blockabsorb()} need to be filled out to reflect
the replacement of $A_{\mathbf{p}\mathbf{p}}$ by $A_{\nu \mathbf{p}\,
  \nu\mathbf{p}}$ and of $A_{\tau\,\mathbf{p}}$ (or more accurately\footnote{%
The model artifacts of the block-absorbed \texttt{lm} will include
appropriate $\lambda$- as well as $\tau$-related
material. Only the $\tilde{\alpha}_{0}$-related material needs to be generated
separately.
}, $A_{\lambda\tau\,\mathbf{p}}$) by $A_{\tilde{\alpha}_{0}\lambda \tau\, \nu\mathbf{p}}$.

\subsubsection{Subgroup effects}
When estimating subgroup effects with absorption of blocks,
the equivalent weighting scheme is determined by applying
\eqref{eq:28} separately within subgroups.  First, for each block $s$
and subgrouping level $\ell$, the empirical solution
$\hat{p}_{js\ell}$  of $\sum_{i \in s, \ell}
\absorbInterceptsEF (z_{i}; p_{js\ell})=0$ is found.
(There may be $s$ and $\ell$ for which $s \cap \ell = \emptyset$. In
these cases we can set $\hat{p}_{js\ell}=0$; setting it to any value
would give the same result.)  Then we can figure a ``main effect'' for
subgroup level $\ell$ as the average over control group, subgroup
$\ell$ members, calculated with weights $\{w_{\text{BFE} \ell i}: i
\in \ell, z_{i}=0\}$, where for $i \in s$ 
\begin{equation*}
  w_{\text{BFE} \ell i} =
  \begin{cases}
    \hat{p}_{js\ell} \owt[] & i \in \ell, z_i =0 \text{ and } 0 < \sum_{i
  \in s\cap \ell} \owt[] (1-z_{i})< \sum_{i \in s\cap \ell} \owt[]\\
0 &  i \in \ell, \sum_{i \in s\cap \ell} \owt[] z_{i} = 0 \text{ or }
  \sum_{i \in s\cap \ell} \owt[].
  \end{cases}
\end{equation*}
The salient generalization of \eqref{eq:upsilondef} is now
\begin{align}
  \absorbInterceptsEF(\vec{v}; \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{ccc}
      (\indicator{z_{i}=0}-p_{01\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}
      \\
      (\indicator{z_{i}=1}-p_{11\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=1}-p_{1s\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}\\                                 (\indicator{z_{i}=0}-p_{01\ell_{2}})\indicator{b_{i}=1, m_{i}=\ell_{2}}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s\ell_{2}})\indicator{b_{i}=s, m_{i}=\ell_{2}}
      \\
      \vdots & \vdots & \vdots \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1\ell_{g}})\indicator{b_{i}=1, m_{i}=\ell_{g}}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks\ell_{g}})\indicator{b_{i}=s, m_{i}=\ell_{g}}\\          
    \end{array}
  \right)%
  \right) ;\nonumber \\
  \acute{\psi}(\vec{v}; \tilde{\alpha}_{0}, \alpha_{0}, \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber \\
&\left(
  \begin{aligned}                                               
    \indicator{z_{i}=0, m_{i}=\ell_{1}} \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\tilde{\alpha}_{\ell_{1}}\big]
    &(
  1 -
   \sum_{r=1}^{s}p_{0r\ell_{1}}\indicator{b_{i}=r}) \\
   \vdots\\
    \indicator{z_{i}=0, m_{i}=\ell_{g}} \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\tilde{\alpha}_{\ell_{g}}\big]
    &(
   1 -
   \sum_{r=1}^{s}p_{0r\ell_{g}}\indicator{b_{i}=r}) \\
   y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{\ell_{1}}-\tau_{z_{i}}&\\
    \vdots&\\
       y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{\ell_{g}}-\tau_{z_{i}}&\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{m_{i}}-\tau_{z_{i}}\big]
    &(
    \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1rm_{i}}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{m_{i}}-
    \tau_{z_{i}}\big]
    &(\indicator{z_{i}=k}
    - \sum_{r=1}^{s}p_{krm_{i}}\indicator{b_{i}=r})\\
  \end{aligned}
\right), 
\end{align}
with corresponding elaborations of $A_{21}$, $\tilde{A}_{22}$,  $A_{\mathbf{p}\mathbf{p}}$ and
$A_{\tau \mathbf{p}}$. 
\section{Of possible future relevance}

As of this writing, we do not plan to implement moderator effect estimation by way of moderator absorption.  Should these plans change, Section~\ref{sec:accomm-moder-vari} may be of use.
\subsection{Accommodating moderator variables via absorption}
\label{sec:accomm-moder-vari}


For a continuous moderator $m$, absorption of the moderator into the assignment variable
corresponds to solving the system $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p},{\lambda})$ for
$\lambda$, where $\mathbf{p}$ solves $0 =
\sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$,
$\absorbInterceptsEF(\cdot)$ as above,
\begin{equation}
  \label{eq:18}
    \absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})=
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (
         % \indicator{z_{i}=0}-p_{0b_{i}}-\lambda_{0}
         % \tilde{m}_{i}
         % )m_{i} \\
         (
         \indicator{z_{i}=1}-p_{1b_{i}}-\lambda_{1}
         \tilde{m}_{i}
         )m_{i} \\
         \vdots\\
         (\indicator{z_{i}=k}-p_{kb_{i}}
         - \lambda_{k} \tilde{m}_{i})m_{i}\\
       \end{array}
  \right)%
\end{equation}
and $\tilde{m}$ is $m$ with block centering. Equivalently, $\lambda$
solves $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p},{\lambda})$ with
$\absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})$ given by
\begin{equation*}
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (\indicator{z_{i}=0}-p_{0b_{i}}-\lambda_{0}
         % \tilde{m}_{i})\tilde{m}_{i} \\
         (\indicator{z_{i}=1}-p_{1b_{i}}-\lambda_{1}
         \tilde{m}_{i})\tilde{m}_{i} \\
         \vdots\\
         (\indicator{z_{i}=k}-p_{kb_{i}}
         - \lambda_{k} \tilde{m}_{i})\tilde{m}_{i}\\
       \end{array}
     \right).
\end{equation*}
If there were no blocks,
or no absorption of block intercepts, then $\tilde{m} = m -
\bar{m}_{w}$,  where $\bar{m}_{w}$ is $m$'s weighted mean, and $p_{jk}
\equiv p_{j}$, with $\hat{p}_{j}$ equal to a weighted mean of
$\indicator{z_{i}=j}$.

For a subgrouping variable $m$ with $t>1$ levels, and corresponding
indicator variables $m_{\ell} =\indicator{m_{i}=\ell}$, $\ell=1,
\ldots, t$, and block-centered versions $\tilde{m}_{\ell}$  moderator absorption into the assignment variable
corresponds to solving $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p}, {\lambda})$ for $\lambda$, where
\begin{align}
    \absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})
&=
                                                 \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}  \left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   % (\indicator{z_{i}=0}-p_{0b_{i}}-\tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=0}-p_{0s}-\tilde{m}_{ti}\lambda_{t})m_{ti}
                                                   % \\
                                                   (\indicator{z_{i}=1}-p_{1b_{i}}-\tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=1}-p_{1s}-\tilde{m}_{ti}\lambda_{t})m_{ti}
                                                   \\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{z_{i}=k}-p_{kb_{i}}
                                                   - \tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=k}-p_{ks}-\tilde{m}_{ti}\lambda_{t})m_{ti}\\                                                 \end{array}
  \right)%
  \right) .  \label{eq:17}
\end{align}

In the model-based formulation neither of these demands any special
accounting in covariance estimation, since both $A$ and the moderating
variable are held fixed by conditioning. We get a legit covariance
without doing anything special, using an $\acute{\psi}$ given by
\begin{multline*}
  \acute{\psi}(\vec{v}; \tau, \mathbf{p}, \beta) =
    \indicator{i \in \bigcup \mathcal{Q}}\owt[] \times \\
\left(
  \begin{array}{r}
    % \big[y_{i}[z_{i}]
    % -
    % g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}\big]
    % (
    % \indicator{z_{i}=0} -
    % \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}
    % - \tilde{m}_{i}\lambda)\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}\big]
    (
    \indicator{z_{i}=1}-
    \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r}
    - \tilde{m}_{i}\lambda)\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) -\alpha_{0} -
    \tau_{z_{i}}\big]
    (
    \indicator{z_{i}=k}
    -
    \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r}
    -
    \tilde{m}_{i}\lambda)\\
  \end{array}
\right) .
\end{multline*}
Because the covariance estimates will
involve residuals that have not been decorrelated from the moderating
variable, they may err on the side of conservatism relative to
covariance estimates issuing from a fit that addressed the moderator
not by absorption but by adding it to the outcome regression
equation. We might consider alternate elaborations of the estimating
function that introduce moderator main effects to the outcome equation.

For design-based calculations, \texttt{estfun\_DB\_blockabsorb(DA)}
 should return
 \begin{equation*}
   A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' +
  \AbsorbModeratorEF{}A_{\lambda \lambda}^{-1}\hat{A}_{\tau \lambda}'.
 \end{equation*}
 In this way, the overall estimating function
 \texttt{estfun.teeMod(DA)} $-$ \texttt{estfun\_DB\_blockabsorb(DA)}
 becomes
 \begin{equation}
   \label{eq:19}
  \acute{\Psi} -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}' - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' -
  \AbsorbModeratorEF{}A_{\lambda \lambda}^{-1}\hat{A}_{\tau \lambda}'
 \end{equation}
with $A_{\lambda \lambda}$ and $A_{\tau \lambda}$ defined in the natural
way given the $\absorbModeratorEF()$ definitions stated just above.


\nobibliography{estfun_DA}
\end{document}


\subsubsection{Alt def of M's}
\[
  \begin{array}{cc}
    M_{11}  = n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\vec{V}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\vec{V}_{i};
                                                                  \beta
                                                                  ), \sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi'(\vec{V}_{i}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = n^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi(\vec{V}_{i};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
With these definitions,
\[
  \operatorname{Cov}(\hat\tau) \approx n^{-1}A_{22}^{-1}\{M_{22} - [M_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}M_{21}] + (n/n_C)A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-t}
\]


We can express HC0 type estimators with matrix rather than summation notation, using $F$ for the \{0,1\}-valued $n
\times m$ matrix with an indicator column for each cluster.
\[
  \begin{array}{cccc}
  \hat{M}_{11} &=n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} &= n_{C}^{-1}(F'\Phi)(F'\Psi)\\
    \hat{M}_{21} &=\hat{M}_{12}' & \hat{M}_{22}&= n^{-1}(F'\Psi)(F'\Psi)\\
    \end{array}
\]




\section{Communicated in ``Josh/Xinhe Meeting Notes," Nov 30 2022
  post-mtg note}
There are random estimating functions $n_{\mathcal{C}}^{-1}\sum_{i\in\mathcal{C}}\phi(\vec{V}_{i}; \beta)$  and
       $n_{\mathcal{Q}}^{-1}\sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta)$,
       $\psi(\vec{V}_{i}; \tau, \alpha, \beta) =
       \owt[][y_{i} - \tau(z_{i} - \bar{a}) -
       g(\vec{x}_{i}\beta)](z_{i} - \bar{a})$ where $\bar{a}$ solves
       \begin{equation}
         \label{eq:4}
         0=\sum_{i \in \mathcal{Q}}w_{i \mathcal{Q}}(z_{i} - \bar{a}),
       \end{equation}
       inducing corresponding estimating equations
\[ \begin{array}{c} 0 \\ 0
   \end{array} = \left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{v}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{v}_{i}; \tau, \alpha, \beta )
     \end{array}
\right).
\]

We seek to represent $\sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta)$ as a
       linear combination of
       \begin{align}
         \label{eq:1}
         \sum_{i \in \mathcal{Q}}\psi_{0i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                             \rho_{0}](1-z_{i})\\
         \label{eq:2}
         \sum_{i \in \mathcal{Q}}\psi_{1i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                              \rho_{1}]z_{i}\\
         \label{eq:3}
         &\rho_{1} - \rho_{0} - \tau.
       \end{align}

       If my algebra is OK, we have
       \begin{equation*}
         \sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta) =
       \sum_{i \in \mathcal{Q}}\psi_{0i} -  \bar{a}\sum_{i \in \mathcal{Q}}(\psi_{0i} + \psi_{1i})
     \end{equation*}
     by some algebra including \eqref{eq:4}.  I had to call time
     before seeing about whether and how to accommodate scenarios with
     strata being absorbed into the treatment variable; there are at
     least a few additional wrinkles lurking there.


M-estimation's characteristic covariance approximation is
\[ \operatorname{Cov}([\hat\beta',\hat\tau']) \approx
  A^{-1} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta )
     \end{array}
\right)A^{-1}
\]

The right-hand side covariance isn't quite a constant multiple of $B$ as in previous comments here (in particular with scaling factor $n_{\mathcal{Q}\mathcal{C}}^{-1}$ on off-diagonals).  Rather, with $B$ as defined there,
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}B_{11}& \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{12}\\
    \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{21} &
                                                                              n_{\mathcal{Q}}^{-1}B_{22}
  \end{array}
\right).
\]

From this I get
\[
  \operatorname{Cov}(\hat\tau) \approx n_Q^{-1}A_{22}^{-1}\{B_{22} - (n_{CQ}/n_C)[B_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}B_{21}] + (n_Q/n_C)A_{21}A_{11}^{-1}B_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]
(with $A$s and $B$s as defined in earlier comments in the thread).
