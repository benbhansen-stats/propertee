\documentclass{article}

\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}%for \llbracket
\usepackage{mathtools}% for DeclarePairedDelimiter
\usepackage{blkarray}
\usepackage{soul}
\usepackage{colonequals}
\newcommand{\defeq}{\ensuremath{\colonequals}}
\newcommand{\eqdef}{\ensuremath{\equalscolon}}

\newcommand{\EE}{\operatorname{E}}
\DeclarePairedDelimiter{\indicator}{\llbracket}{\rrbracket}
\newcommand{\owt}[1][{[z_{i}]}]{\ensuremath{\check{w}_{i#1}}}
\newcommand{\absorbInterceptsEF}{\upsilon}
\newcommand{\AbsorbInterceptsEF}{\Upsilon}
\newcommand{\absorbModeratorEF}{\grave{\upsilon}}
\newcommand{\AbsorbModeratorEF}{\grave{\Upsilon}}

\usepackage{natbib}
\usepackage{bibentry}
\bibliographystyle{plain}

\usepackage{hyperref}


\author{BBH}
\title{\texttt{sandwich::estfun()} method for DA objects}
\begin{document}
\nobibliography*
\maketitle
\section{Separate and combined estimating functions for covariance and comparison fits}

\subsection{Setting}

Let $\mathcal{C}$ and $\mathcal{Q}$ denote the covariance and
quasiexperimental samples, respectively, expressed at the level of
unit of assignment: if assignment is at the level of the classroom,
then these are collections of classes not collections of students, nor
of student-year observations.  In turn, identify each unit of assignment
with the set of elemental observations they contain: a classroom is
identified with the
set of students, or student-years, associated with that classroom;
if there is no clustering, then we think of $\mathcal{C}$ and
$\mathcal{Q}$ as consisting of singleton sets, each containing a
single element.  (Additional structure: any clusters $c, c' \in \mathcal{C}
\cup \mathcal{Q}$ that are distinct, $c  \neq c'$, are nonoverlapping, $c \cap c' = \emptyset$.) This permits us to write $\mathcal{Q}$ for the
quasiexperimental sample as expressed in terms of clusters, and $\bigcup
\mathcal{Q}$ for the collection of all elements belonging to the
quasiexperimental sample.

Denote by $n_{\mathcal{C}}= \# \left( \bigcup \mathcal{C}\right)$,
$n_{\mathcal{Q}}= \# \left( \bigcup \mathcal{Q}\right)$ and
$n = \# \left[\left( \bigcup \mathcal{C}\right) \cup \left(\bigcup
    \mathcal{Q}\right)\right]$ the numbers of elements in the 3
overlapping samples. There are random estimating functions
$\Phi(\mathbf{V}; \beta ) = [\indicator{ i \in
  \left(\bigcup\mathcal{C}\right)}\phi(\vec{V}_{i}; \beta) : i]'$ that define the covariance
regression, padding with rows of zeroes in
the complement of the covariance sample so
as to have dimension $n \times p$ rather than $n_{\mathcal{C}}\times
p$; and estimating functions
$\grave{\Psi}(\mathbf{V}; \rho, \beta) = [\psi(\vec{V}_{i}; \rho,
\beta): i]'$, padded with zeroes on the complement of $\bigcup
\mathcal{Q}$ in order also to have row extent $n$, to define the
treatment condition comparison.  For now, let
\begin{equation}
         \label{eq:5}
         \grave{\psi}(\vec{V}_{i}; \rho, \beta) =\\
         \left( \begin{array}{c}
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[0]}] [Y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \rho_{0}]\indicator{Z_{i}=0}\\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[1]}] [Y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \rho_{1}]\indicator{Z_{i}=1}\\
                  \vdots \\
           \indicator{i \in \bigcup \mathcal{Q}}\owt[{[k]}] [Y_{i}[k] - g(\vec{x}_{i}\beta)-
                  \rho_{k}]\indicator{Z_{i}=k}\\
                \end{array}\right),
\end{equation}
where
\begin{itemize}
\item $Y_{i}[j]$ represents $i$'s potential outcome following
  assignment to condition $i$;
\item $Z_{i}$ is the treatment assignment of $i$'s cluster $[i]$;
\item $\vec{V}_{i} = (Y_{i}[0],\ldots, Y_{i}[k], \vec{X}_{i}, M_{i}, W_{i}, Z_{i})$, where $\vec{X}_{i}$, $M_{i}$ and $W_{i}$ are covariates, an optional moderator variable and a
  frequency or importance weight associating with $i$ (1 if no such weight was
  given);
\item $\owt[{[j]}]$ is either $w_i$ or,
   if requested by the user, $w_{i}$'s product with an
  odds- or inverse probability-based weight attaching to $i$'s
  assignment unit $[i]$ under assignment $j$.
\end{itemize}
(I say ``for now'' because \eqref{eq:14} will present a
$\psi(\cdot)$ with equivalent solutions that is better aligned with
\texttt{lm()}'s default treatment effect parameterizations, and
\eqref{eq:9} in \S~\ref{sec:accomm-interc-via} below will generalize $\psi()$'s definition
to add block-specific intercepts.  The symbols $\vec{x}_{i}$, $m_{i}$
and $w_{i}$ are given in lowercase as we in general condition upon
$\{(\vec{X}_{i}, M_{i}, W_{i}): i\}$.)
The interest parameter is $(\rho_{0}, \tau)$, where $\tau \defeq (\rho_{1}, \ldots, \rho_{k}) - \rho_{0}$. In the model-based interpretation, occurrences of ``$Z_{i}$'' in \eqref{eq:5} may be replaced with ``$z_{i}$,'' while under design-based we can write ``$y_{i}[j]$'' instead of ``$Y_{i}[j]$''; see Section~\ref{sec:des-vs-mod-based} below.

\subsection{Goal} \label{sec:goal}
We aim to define \texttt{estfun.teeMod()} associating
with a direct adjustment model a matrix of evaluated estimating function
contributions of form
\begin{equation} \label{eq:22}
  \tilde{\Psi} = c_{2}\Psi - c_{1}\Phi A_{11}^{-1}A_{21}^{t},
\end{equation}
and a corresponding \texttt{bread.teeMod()} which together will enable \texttt{sandwich::vcov()} to provide valid standard errors.
Note that this is $n$ by
$k+1$, $n$ being the size of the combined $\mathcal{C}$ and
$\mathcal{Q}$ sample, with columns corresponding to $\rho$.%
(Assuming we're estimating a single marginal treatment effect, as
opposed to moderation effect[s], either without blocking or with
neither explicit nor implicit block fixed effects; otherwise the
number of columns could be $k$\footnote{For a main effect
DA model fitted with absorption of the block intercept or
intercepts; see \S~\ref{sec:accomm-interc-via} below.} or something
larger than $k+1$\footnote{In the case of subgroup effects and/or unabsorbed block fixed effects.},
depending. To obviate the ambiguity we'll say
\begin{equation*}
  \tilde{k} \defeq \mathtt{ncol}(\tilde{\Psi}).)
\end{equation*}

Values of the constants $c_{1}$ and $c_{2}$ are to be determined.
I conjecture these decisions can be made in such away that
\begin{enumerate}
\item Using this as a basis for a meat matrix while using
  $A_{22}^{-1}$  ``bread matrix'' lands us in the same place as the
  inversion formula as expressed with A's and B's on p.373 of Carroll
  et al. 2006.
\item Suppose we've got multiple DA models that have been fit over the
  same CA model, and we wish to compute joint covariances for their
  distinct parameters
  (\href{https://github.com/benbhansen-stats/propertee/issues/79}{\#79
    on GitHub}). We get appropriate joint bread matrices by
  diagonally combining bread matrices of the DA models, and we get
  appropriate meat matrices by cbind-ing the estfun's.
\item For ``natural'' choices of $c_{1}$ and $c_{2}$, in the
  presence of clustering the corresponding bread matrices are
  reasonable and interpretable, at least in cases of HC0 and HC1
  degree of freedom adjustments.
\end{enumerate}

\subsection{Intercepts and intercept supplements}
\subsubsection{$\psi(\vec{V}_{i};
  \alpha_{0}, \tau, \beta)$, an intercept-form equivalent of $\grave{\psi}(\vec{V}_{i};
  \rho, \beta)$}
\label{sec:psit-rho_0-tau}

Write $\tau_{j}=\rho_{j}-\rho_{0}$ and introduce the intercept parameter ``$\alpha$,''  for the time being one-dimensional with $\alpha_{0}=\rho_{0}$.   The estimating equation $\sum_{i}\grave{\psi}(\vec{v}_{i}; \rho, \beta)=0$,
$\grave{\psi}(\cdot; \cdot)$ as in \eqref{eq:5} above, has the same solutions as $\sum_{i}\psi(\vec{v}_{i};
  \alpha_{0}, \tau, \beta)=0$, with $\psi(\cdot; \cdot)$ as in \eqref{eq:14} immediately below:
\begin{equation} \label{eq:14}
         \psi(\vec{v}_{i};
         \alpha_{0}, \tau, \beta) =
         \indicator{i \in \bigcup \mathcal{Q}}
         \left( \begin{aligned}
           \owt[{[z_{i}]}] &\big[y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\\
           \owt[{[1]}]\indicator{z_{i}=1} & \big[y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{1}\big]\\
                  \vdots \\
           \owt[{[k]}]\indicator{z_{i}=k} & \big[y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_{0} - \tau_{k}\big]\\
                \end{aligned}\right),
\end{equation}
the omission of a ``$\indicator{z_{i}=0}$'' factor from the topmost
expression being intentional and deliberate.
Abbreviating ``$\owt{}$'' as ``$\owt[]$'' and ``$y_{i}[z_{i}]$'' as
``$y_{i}$,'' we have
\begin{equation}\label{eq:27}
         \psi(\vec{v}_{i};
         \alpha_{0}, \tau, \beta) =
         \indicator{i \in \bigcup \mathcal{Q}}
  \owt[] \Big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0} -
                  \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\Big]
                  \begin{pmatrix} 1 \\ \indicator{z_{i}=1}\\ \vdots
                    \\ \indicator{z_{i}=k}\end{pmatrix} .
\end{equation}
Since the solution in
$(\alpha_{0}, \tau_{1}, \ldots, \tau_{k})$ of
$\sum_{i}\psi(\vec{v}_{i}; \alpha_{0}, \tau, \beta) =0$ satisfies
\begin{equation} \label{eq:16}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[][y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\tau_{j}]\indicator{z_{i}=j}, \text{ each } j=1, \ldots, k,
\end{equation}
this solution also satisfies
\begin{equation*}
       0 = \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0}-\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big]\indicator{z_{i}
                  \neq 0}.
\end{equation*}
The complement of condition $\indicator{z\neq 0}$ being
$\indicator{z=0}$, under which
$\sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}=0$, it follows from \eqref{eq:16} that
\begin{align*}
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[{[0]}]\big[y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{0}\big]\indicator{z_{i}
             =0}\\
           & \Updownarrow \\
         0 &= \sum_{i \in \bigcup \mathcal{Q}}\owt[]\big[y_{i} - g(\vec{x}_{i}\beta)-
                  \alpha_{0} - \sum_{j=1}^{k}\tau_{j}\indicator{z_{i}=j}\big].
\end{align*}
Representations \eqref{eq:14} and \eqref{eq:27} better represent what
\texttt{lm()} does, while \eqref{eq:5} centers the interpretation of $\rho_{0}$/$\alpha_{0}$ as a
marginal mean of the outcome, or partial residual outcome, under control.

\subsubsection{$\xi(\vec{V}_i; \alpha_{y0}, \alpha_{g0}, \beta)$ and intercept supplements}\label{sec:xivecv_i-alph-alph}
Intercept supplements $\alpha_{y0}$ solving $\sum_{i} \EE \xi(\vec{V}_i; \alpha_{y0}, \alpha_{g0}, \beta) =0$, for $\xi(\cdot)$ as given by
\begin{equation}\label{eq:34}
  \xi(\vec{V}_i; \alpha_{yg0},  \beta) =
  \indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]
  \begin{pmatrix}
    y_{i}-\alpha_{y0}\\
    g(\vec{x}_{i}\beta) - \alpha_{g0}
  \end{pmatrix}
,
\end{equation}
will often be of greater interest than will $\alpha_{0}$ solving $\sum_{i} \EE \psi(\vec{V}_i; \alpha_{0}, \tau, \beta) =0$, as $\alpha_{y0}$ and  $\alpha_{0}$ represent marginal means of $y$ and $y-g(\vec{x}\beta)$, respectively.  So $\hat{\alpha}_{y0}$ will reported alongside of $\hat{\tau}$, with $\hat{\alpha}_{0}$ and $\hat{\alpha}_{g0}$ also carried in \texttt{teeMod} objects and the associated estfun's carrying $\xi$ material alongside of $\psi$-content.

It is redundant to report $\alpha_{g0}$ as well as $\alpha_{0}$ since $\alpha_{0} \equiv \alpha_{y0} - \alpha_{g0}$, but we do this because when absorbing block fixed effects (Sec.~\ref{sec:accomm-interc-via}) we will continue to return $\hat{\alpha}_{0}$, $\hat{\alpha}_{y0}$ and $\hat{\alpha}_{g0}$ despite the corresponding $\alpha_{0}$'s lacking a substantive interpretation and no longer being equal to the difference of ${\alpha}_{y0}$ and ${\alpha}_{g0}$.

With a continuous moderator $m$,
\begin{equation}
\label{eq:37}
    \xi(\vec{V}_i; \lambda_{yg0}, \alpha_{yg0}, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]
  \begin{pmatrix}
    y_{i}-m_{i}\lambda_{y0} - \alpha_{y0}\\
    (y_{i}-m_{i}\lambda_{y0} - \alpha_{y0})m_i\\    
    g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}\\
    [g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}]m_{i}\\
  \end{pmatrix};
\end{equation}
with a categorical moderator $m$ possessing levels $\ell_{1}, \ldots,
\ell_{g}$,
\begin{equation}
  \label{eq:36}
    \xi(\vec{V}_i; \alpha_{yg0},  \beta) =
  \indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]
  \begin{pmatrix}
    \indicator{ m_{i}=\ell_{1}}(y_{i}-\alpha_{y\ell_{1}0})\\
    \vdots\\
    \indicator{ m_{i}=\ell_{g}}(y_{i}-\alpha_{y\ell_{g}0})\\
    \indicator{ m_{i}=\ell_{1}}[g(\vec{x}_{i}\beta) -
    \alpha_{g\ell_{1}0}]\\
    \vdots\\
    \indicator{ m_{i}=\ell_{g}}[g(\vec{x}_{i}\beta) -    
    \alpha_{g\ell_{g}0}]\\  \end{pmatrix}.
\end{equation}

\subsection{The covariance we're trying to
  produce}\label{sec:covar-were-trying}

Let $\hat\beta$, $\hat{\alpha}$, $\hat\tau$, $\hat{\alpha}_{y}$ and $\hat{\alpha}_{g}$ solve
\[\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{v}_{i}; \beta )
  =0,\quad
\sum_{j\in \bigcup \mathcal{Q}}\psi(\vec{v}_{j}; \tau,
\alpha, \beta )  =0 \quad \text{and } \sum_{j\in \bigcup \mathcal{Q}}\xi(\vec{v}_{j}; \beta, \alpha_{y}, \alpha_{g})=0
  \]
  in $\beta$, $\alpha$, $\tau$, $\alpha_{y}$ and $\alpha_{g}$, whereas $\beta_{(n)}$, $\alpha_{(n)}$ and $\tau_{(n)}$, $\alpha_{(n)y}$ and $\alpha_{(n)g}$ solve or nearly solve\footnote{%
In actuality we only need the estimator sequence to be consistent for the deterministic sequences $(\beta_{(n)}:n)$, $(\alpha_{(n)}:n)$ and $(\tau_{(n)}:n)$, i.e. $|\hat\beta - \beta_{(n)}|_{2}$, $|\hat{\alpha} - \alpha_{(n)}|_{2}$ and $|\hat\tau - \tau_{(n)}|_{2}$ are all $o_{P}(1)$.%
}
\[\sum_{i\in \bigcup \mathcal{C}}\EE\phi(\vec{V}_{i}; \beta )
  =0,\quad
\sum_{j\in \bigcup \mathcal{Q}}\EE\psi(\vec{V}_{j}; \tau,
\alpha, \beta )  =0 \quad\text{and }
\sum_{j\in \bigcup \mathcal{Q}}\EE\xi(\vec{V}_{j}; \beta, \alpha_{y}, \alpha_{g})=0.
  \]
In these expectations and those that follow, $\vec{x}_{i}$ and $w_{i}$
are taken as fixed but $Z_{i}$ and/or potential outcomes $(Y_{i}[z]:
z=0, \ldots, k)$ are random: in terms of sigma fields defined in \S~\ref{sec:des-vs-mod-based} below, this section's expectations condition on $\mathcal{X}$ and $\mathcal{Z}$.  These sigma fields are contained within the larger sigma fields of both model- and design-based conditioning.  Write
\begin{align}
\nonumber
  A(\beta, \alpha, \tau) &=
\begin{pmatrix}                           
        n_{\mathcal{C}}^{-1}\sum_{i}\EE [\nabla_{\beta}\phi(\vec{V}_{i};
        \beta )]&0\\
        n_{\mathcal{Q}}^{-1}\sum_{j}\EE\begin{bmatrix}
    \nabla_{\beta}\psi(\vec{V}_{j};\tau, \alpha, \beta )\\
    \nabla_{\beta}\xi(\vec{V}_{j};\alpha_{y},\alpha_{g}, \beta )\\
  \end{bmatrix} &
n_{\mathcal{Q}}^{-1}\sum_{j}\EE\begin{bmatrix}
                    \nabla_{\alpha, \tau}\psi(\vec{V}_{j}; \tau, \alpha, \beta ) & 0\\
                    0 & \nabla_{\alpha_{y}, \alpha_{g}}\xi(\vec{V}_{j};\alpha_{y},\alpha_{g}, \beta)\\
\end{bmatrix}\\
                  \end{pmatrix}\\
    \label{eq:13}
    &\eqdef \left(
      \begin{array}{cc}
        A_{11}(\beta)& 0\\
        A_{21}(\beta) & A_{22}(\beta, \alpha, \tau)
      \end{array}
\right),
  \end{align}
with sums in $i$ and $j$ ranging over $\bigcup \mathcal{C}$ and $\bigcup\mathcal{Q}$, respectively.
The structure of $\psi(\cdot)$ and $\xi(\cdot)$ as given in \eqref{eq:14} and \eqref{eq:34} ensures
that $A_{21}(\beta, \alpha, \tau, \alpha_{y}, \alpha_{g})$ is a function only of $\beta$, and that $A_{22}(\beta, \alpha, \tau, \alpha_{y}, \alpha_{g})$ is constant in its argument $(\beta, \alpha, \tau, \alpha_{y}, \alpha_{g})$.\footnote{We write $A_{22}$  as a function of $(\beta, \alpha, \tau)$ to facilitate a possible future generalization to cases where the stage 2 $\psi(\cdot)$ model is of the glm form.}  Assuming $A_{11}(\beta_{(n)})$ and $A_{22}(\beta, \alpha, \tau)$ to be invertible, define \textit{linearizations} (of
$\hat\beta$ and $(\hat{\alpha}_{0}, \hat\tau)$) as follows:
  \begin{align}
    \begin{pmatrix}\tilde{\beta}_{(n)}\\\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \\ \tilde{\alpha}_{yg(n)}\end{pmatrix}
    \defeq& \begin{pmatrix}\beta_{(n)} \\\alpha_{(n)}\\
                    \tau_{(n)}\\ \alpha_{yg(n)} \end{pmatrix}
    + A^{-1}(\beta_{(n)}, \alpha_{(n)}, \tau_{(n)})\begin{pmatrix}
      n_{\mathcal{C}}^{-1}\sum_{i%\in \bigcup \mathcal{C}
      }\phi(\vec{V}_{i}; \beta_{(n)})\\
      n_{\mathcal{Q}}^{-1}\sum_{j%\in \bigcup \mathcal{Q}
                 }\begin{bmatrix}\psi(\vec{V}_{j};
                   \alpha_{(n)}\tau_{(n)},\beta_{(n)} )\\
                   \xi(\vec{V}_{j}; \beta_{(n)},\alpha_{yg(n)})\end{bmatrix}
      \\
    \end{pmatrix},\, \text{i.e.} \nonumber \\
   \nonumber \\
       \tilde{\beta}_{(n)}
    \defeq& \beta_{(n)} + A_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})
  \, \text{and} \nonumber\\
    \begin{pmatrix}\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)}\\ \tilde{\alpha}_{yg(n)} \end{pmatrix}
    &=\begin{pmatrix}\alpha_{(n)}\\
                         \tau_{(n)}\\ \alpha_{yg(n)} \\ \end{pmatrix} +
    A_{22}^{-1}\times \label{eq:20}\\
    & \left\{
                 n_{\mathcal{Q}}^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\begin{bmatrix} \psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} )\\ \xi(\vec{V}_{j}; \beta_{(n)}, \alpha_{yg(n)}) \end{bmatrix} - n_{\mathcal{C}}^{-1}A_{21}(\beta_{(n)}) A_{11}^{-1}(\beta_{(n)})\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\right\},\nonumber
\end{align}
where we've used the block inversion relationship
\begin{equation*}
  \label{eq:23}
  \left(\begin{array}{cc}A & 0 \\ B & C \end{array} \right)^{-1} =
  \left(\begin{array}{cc}A^{-1} & 0 \\ 0 & C^{-1} \end{array} \right)
  \left(\begin{array}{cc} I & 0 \\ -BA^{-1} & I \end{array} \right)
\end{equation*}
(a special case of factorization and inversion based on Schur complements).

The constants $n_{\mathcal{C}}$ and
$n_{\mathcal{Q}}$ in the above are somewhat
arbitrary.  Provided they're positive, replacing them throughout with any other
positive constants defines the same linearizations (although there may
be numerical differences).
Using the numbers of elements (not
clusters) in the covariance and in the comparison samples,
respectively, aligns the expressions with the computing procedures
generating $\hat\beta$, $\hat{\alpha}$ and $\hat\tau$ from specific
datasets, but our strategy for invoking the \texttt{sandwich} package
in standard error computations calls for  $\psi()$ \texttt{estfun}'s
of dimension $n$ rather than $n_{\mathcal{Q}}$.%
\footnote{%
  The constant that \texttt{sandwich}
  functions will select for the covariance model is $n_{\mathcal{C}}$, given its sample
  size.  It's incumbent on us to supply \texttt{sandwich::estfun()}
  and/or \texttt{sandwich::bread()} methods for
  \texttt{teeMod} objects with compatible implicit scaling
  constants. \texttt{sandwich::bread()} and
  \texttt{sandwich::breadCL()} take the scaling constant to be the
  reciprocal of the number of rows in whatever
  \texttt{sandwich::estfun()} returns; it will fall to us to ensure
  our $\hat{A}_{21}$ and $\hat{A}_{22}$ observe compatible scalings.}%
 This leads us to define
\begin{align}\label{eq:29}
  \tilde{A}_{21} &= n^{-1} \sum_{j\in \bigcup
        \mathcal{Q}}\EE\begin{bmatrix} \nabla_{\beta}\psi(\vec{V}_{j};
          \tau, \alpha, \beta )\\
          \nabla_{\beta}\xi(\vec{V}_{j}; \beta, \alpha_{yg})\\ \end{bmatrix}
                          = \frac{n_{\mathcal{Q}}}{n} {A}_{21},\\
  \tilde{A}_{22} &= n^{-1} \sum_{j\in \bigcup
        \mathcal{Q}}\EE\begin{bmatrix} \nabla_{\alpha, \tau}\psi(\vec{V}_{j};
          \tau, \alpha, \beta ) & 0 \\
        0 & \nabla_{\alpha_{yg}}\xi(\vec{V}_{j}; \beta, \alpha_{yg})\end{bmatrix} = \frac{n_{\mathcal{Q}}}{n}
                                 {A}_{22} \text{ and}\nonumber\\
  \tilde{A} &= \left(
              \begin{array}{cc}
                A_{11} & 0 \\
                \tilde{A}_{21} & \tilde{A}_{22}\\
              \end{array}
\right).\nonumber
\end{align}
Substitution into expression \eqref{eq:20} for
$\tilde{\alpha}_{(n)}$ and $\tilde{\beta}_{(n)}$ gives
\begin{align}
      \left(\begin{array}{c}\tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)         &=\left(\begin{array}{c}\alpha_{(n)}\\
                         \tau_{(n)} \end{array}\right) +
    \tilde{A}_{22}^{-1}(\alpha_{(n)}, \tau_{(n)})\times \label{eq:21}\\
    & \left[
                 n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} ) -
      \tilde{A}_{21}(\beta_{(n)})
      {A}_{11}^{-1}(\beta_{(n)})n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\right] \nonumber
\end{align}
or equivalently
\begin{equation*}
\left(\begin{array}{c}\tilde{\beta}_{(n)}\\ \tilde{\alpha}_{(n)}\\ \tilde{\tau}_{(n)} \end{array}\right)
    = \left(\begin{array}{c}\beta_{(n)} \\\alpha_{(n)}\\
                    \tau_{(n)} \end{array}\right)
    + \tilde{A}^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})\left(
    \begin{array}{c}
      n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
      \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)})\\
      n^{-1}\sum_{j\in \bigcup
                 \mathcal{Q}}\psi(\vec{V}_{j};
                 \tau_{(n)},\beta_{(n)} )
    \end{array}
\right).
\end{equation*}
Assume that: the linearization errors $|\hat\beta -\tilde{\beta}_{(n)}|_{2}$,
$|\hat{\alpha} - \tilde{\alpha}_{(n)}|_{2}$, $|\hat\tau -\tilde{\tau}_{(n)}|_{2}$ and $|\hat{\alpha}_{yg} - \tilde{\alpha}_{(n)yg}|_{2}$ tend in probability to 0;
$A_{11}(\beta_{(n)})$ and $A_{22}(\beta_{(n)}, \alpha_{(n)}, \tau_{(n)})$ are nonsingular
with eigenvalues bounded away from 0; and $A_{11}(\cdot)$, $A_{21}(\cdot)$ and $A_{22}(\cdot)$ are suitably
regular\footnote{E.g, they admit of Lipschitz constants: see \S~9.8,
  \bibentry{keener2010TheorStats}, while bearing in mind issues raised
  by \bibentry{feng2014exact}.}.
Then we have the covariance approximation characteristic of M-estimation,
\begin{align*} \operatorname{Cov}([\hat\beta',\hat{\alpha}', \hat\tau']) \approx&
  \operatorname{Cov}([\tilde{\beta}_{(n)}',\hat{\alpha}_{(n)}',\tilde{\tau}_{(n)}', \tilde{\alpha}_{yg}])\\
  &=
    \tilde{A}^{-1}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)}) \times \\
  &\hspace{1.5em} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta_{(n)} )\\
       n^{-1}\sum_{j=1}^{n}\begin{bmatrix}\psi(\vec{V}_{j}; \tau_{(n)}, \alpha_{(n)}, \beta_{(n)} )\\ \xi(\vec{V}_{j}; \beta_{(n)}, \alpha_{yg(n)}) \end{bmatrix}
     \end{array}
    \right)
  \tilde{A}^{-t}(\alpha_{(n)}, \beta_{(n)}, \tau_{(n)})
\end{align*}
(recalling that we have so defined $\psi(\cdot)$ that $\psi(\vec{V}_{j}; \tau, \alpha, \beta) = 0$ when
$j\not\in \bigcup \mathcal{Q}$).


To help mark the difference from the package's first implementations of these estimating equation stacks (which took pains to avoid scaling constants),
meat matrices will be denoted with the letter ``M,'' not ``B.''
We have
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n^{-1}\sum_{j=1}^{n}\psi(\vec{V}_{j}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}M_{11}& n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{12}\\
    n_{\mathcal{C}}^{-1/2}n^{-1/2} M_{21} & n^{-1}M_{22}
  \end{array}
\right),
\]
in which
\[
  \begin{array}{cc}
    M_{11}  = \operatorname{Cov}[n_{\mathcal{C}}^{-1/2}\sum_{i} \phi(\vec{V}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  \operatorname{Cov}\left\{n_{\mathcal{C}}^{-1/2}\sum_{i}\phi(\vec{V}_{i};
                                                                  \beta
                                                                  ), n^{-1/2}\sum_{j}\begin{bmatrix}\psi(\vec{V}_{j}; \tau, \alpha, \beta ) \\ \xi(\vec{V}_{j}; \beta, \alpha_{yg})\end{bmatrix}'\right\}\\
    M_{21}=M_{12}' & M_{22} = \operatorname{Cov}\left\{n^{-1/2}\sum_{j}
                     \begin{bmatrix}\psi(\vec{V}_{j};
                     \tau, \alpha, \beta)\\ \xi(\vec{V}_{j}; \beta, \alpha_{yg})\end{bmatrix}\right\} ,
    \end{array}
  \]
where again sums in $i$ and $j$ range over $\bigcup \mathcal{C}$ and $\bigcup\mathcal{Q}$, respectively.
Note that $M_{11}$ is scaled just as in estimates of $\operatorname{Cov}(\hat\beta)$, while $M_{22}$'s and
$M_{12}$/$M_{21}$'s scaling constants --- $n^{-1}$ rather than
$n_{\mathcal{Q}}^{-1}$ in $M_{22}$'s
case and $(n_{\mathcal{C}}n)^{-1/2}$ not $(n_{\mathcal{C}} n_{\mathcal{Q}})^{-1/2}$ in $M_{12}$/$M_{21}$'s --- are progressively more weird.

With these definitions,
\begin{align}
  \operatorname{Cov}\begin{pmatrix}\hat\alpha \\ \hat\tau \\ \hat{\alpha}_{yg}\end{pmatrix} \approx& \tilde{A}_{22}^{-1}\{n^{-1} M_{22} -
                                 n_{\mathcal{C}}^{-1/2}n^{-1/2}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-1}\tilde{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}\tilde{A}_{21}^{t}\}\tilde{A}_{22}^{-t}\nonumber
  \\
                               &= n^{-1}\tilde{A}_{22}^{-1}\{
                                 M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}\tilde{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}\tilde{A}_{21}^{t}\}\tilde{A}_{22}^{-t}
                                . \label{eq:6}
\end{align}
An advantage of the expression with leading factor $n^{-1}$ is that
that way it parallels the calculation of
\texttt{sandwich::sandwich()} and, I think, other \texttt{sandwich}
functions. (Assuming that we write
\texttt{estfun.teeMod()}  to return a matrix of row extent
$n$.)
% A potential disadvantage is that it makes $A_{21}$ and
% $A_{22}$ not means of estimating equation contributions but rather
% means down-scaled by $n_{\mathcal{Q}}/n$, a potentially small factor;
% more on this presently.

  \subsection{Plain-vanilla estimation of $A$s and $M$s}
\subsubsection {Aside: ``Model-based'' vs. ``Design-based''}\label{sec:des-vs-mod-based}

In the model-based perspective, expected value and covariance are
conditional on
\[ \mathcal{M} = \sigma\left(\left[(X_{i}, W_{i}, M_{i}, Z_{i}: i \in c): c \in
      \mathcal{C}\cup \mathcal{Q} \right]\right)\]


In the design-based perspective, $\mathcal{Q}$ is partitioned into
blocks $b$, and expected value and covariance are
conditional on
\[\mathcal{X} = \sigma(\{(X_{i}, W_{i}, M_{i}): i \in c; c\}),\]
\[\mathcal{Y} = \sigma(\{(Y_{i}[1], \ldots Y_{i}[k]): i \in c; c\})\]
and
\[\mathcal{Z} = \sigma(\{\sum_{c \in b}\, \![Z_{c}=j\!] : j \in \{0, \ldots, k\}; b\} ),\]
where $z_{c}$ denotes the common treatment assignment of all $i\in
c$. (So estimating equations ${\Psi}_{i}$s are independent across but not within
clusters $c \ni i$.)

To incorporate $c\in \mathcal{C}\setminus\mathcal{Q}$ into this
definition, let each such cluster $c$ constitute a block unto itself,
$\{c\}$. This has the consequence that observations $i
\in \bigcup\left(\mathcal{C}\setminus\mathcal{Q}\right)$ do not
contribute to $M_{11}$, $M_{12}$ or $M_{21}$, at least in the
design-based perspective.

  \subsubsection{Conceptual vs practical bread estimates} \label{sec:conc-vs-pract}
  \paragraph{Model-based $\hat{A}$'s as estimates of design-based $A$ matrices.}%
In general $\EE
[\nabla_{\beta}\psi(\vec{V}_{i};  \beta, \alpha_{0},\tau ) \mid \mathcal{M}] \equiv
\nabla_{\beta}\psi(\vec{V}_{i};  \beta, \alpha_{0}, \tau )$; for many types of
covariance model one also has $\EE
[\nabla_{\beta}\phi(\vec{V}_{i};  \beta ) \mid \mathcal{M}] \equiv
\nabla_{\beta}\phi(\vec{V}_{i};  \beta )$. As a result, under the
model-based perspective
$A_{11}(\beta)$ and perhaps $A_{21}(\beta)$ are essentially observed,
i.e.  $\hat{A}_{11}(\beta) \equiv A_{11}(\beta)$ and (perhaps)
$\hat{A}_{21}(\beta) \equiv A_{21}(\beta)$.   These equivalences don't
hold under the design-based perspective, but under that perspective
the same matrices do serve as estimates of $A_{11}(\beta)$ and
$A_{21}(\beta)$ as given in \eqref{eq:13}.

\paragraph{Numerical stability by combining ${A}_{21}$ and
  $\tilde{A}_{22}$, rather than either
  ${A}_{21}$ and ${A}_{22}$ or $\tilde{A}_{21}$ and $\tilde{A}_{22}$.}
Recall from \eqref{eq:29} that $A_{2*}$ and $\tilde{A}_{2*}$ are
scaled sums of expected $\psi$-gradients differing only in their
scaling factors of $n_{\mathcal{Q}}^{-1}$ and $n^{-1}$,
respectively. The $n^{-1}$-scaling embedded
in $\tilde{A}_{21}$ and $\tilde{A}_{22}$ may invite numerical
instability.

  In setups with
  $n_{\mathcal{C}} \gg n_{\mathcal{Q}}$, $n^{-1}$-scaling may lead to underflows,
  collapsing of distinct values into numeric zeroes, if performed
  before taking sums; even if we don't scale before summing,
  $n_{\mathcal{Q}}^{-1}$ scaling of these matrices could faciliated improved
  use of numerically optimized mean and weighted mean functions in
  these calculations.  (By application of such functions only to the
  $n_{\mathcal{Q}}$ contributions to $\hat{A}_{2*}$  from $i \in \bigcup \mathcal{Q}$, not the broader set of $n = \#\left[\bigcup (\mathcal{C}\cup
    \mathcal{Q}) \right]$, which includes $n - n_{\mathcal{Q}}$ structural
  zeroes.)

  In parallel with \eqref{eq:6} we
  have $\operatorname{Cov}\left(\begin{smallmatrix}\hat\alpha \\ \hat\tau \\ \hat{\alpha}_{yg}\end{smallmatrix}\right) \approx$
  \begin{align*}
  &A_{22}^{-1}\{\frac{n}{n_{\mathcal{Q}}^{2}} M_{22} -
                                 \frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}n_{\mathcal{Q}}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 n_C^{-1}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-t}\nonumber
  \\
                               =& n^{-1}A_{22}^{-1} \times\\
                                 &\left\{\left(\frac{n}{n_{\mathcal{Q}}}\right)^{2}M_{22} -
                                 \frac{n^{3/2}}{n_{\mathcal{C}}^{1/2}n_{\mathcal{Q}}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\right\}\\
    &
                                 \times A_{22}^{-t}\\
                               =& n^{-1}A_{22}^{-1} \times\\
                                 &\left\{\left(\frac{n}{n_{\mathcal{Q}}}\right)^{2}M_{22} -
                                 \left(\frac{n}{n_{\mathcal{Q}}}\right)\left(\frac{n^{1/2}}{n_{\mathcal{C}}^{1/2}}\right)[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n}{n_C}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\right\}\\
    &
                                 \times A_{22}^{-t}
                                .
\end{align*}
In computations that assemble the three inner summands separately, we may combine $\psi$-\texttt{estfun}'s of dimension
$n$ with $\hat{A}_{21}$ and $\hat{A}_{22}$ that have been scaled by
$n_{\mathcal{Q}}$, as opposed to $n$.  However, for Section~\ref{sec:goal}'s purpose of
associating \texttt{teeMod} objects witth estimating functions of \eqref{eq:22}'s form $c_{2}\Psi - c_{1}\Phi
A_{11}^{-1}A_{21}^{t}$, this formula invites unhelpful
complication. It seems to call for $c_{2}=({n}/{n_{\mathcal{Q}}})$
and as well as a $c_{1}=({n}/{n_C})^{1/2}$.  An $A$/$\tilde{A}$
compromise will allow us to keep $c_{2}=1$.

To this end, substituting $(n_{\mathcal{Q}}/n)A_{21}$ for $\tilde{A}_{21}$ in \eqref{eq:6} while leaving
$\tilde{A}_{22}$ in place, $\operatorname{Cov}\left(\begin{smallmatrix}\hat\alpha \\ \hat\tau \\ \hat{\alpha}_{yg}\end{smallmatrix}\right) \approx$
  \begin{align*}
  &\tilde{A}_{22}^{-1}\{\frac{1}{n} M_{22} -
                                 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}^{1/2}n^{3/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{n_{\mathcal{C}}n^{2}}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}\tilde{A}_{22}^{-t}\nonumber
    \\
  &= \frac{1}{n}\tilde{A}_{22}^{-1}\{M_{22} -
                                 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}^{1/2}n^{1/2}}[M_{21}A_{11}^{-t}A_{21}^t
                                 + A_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{n_{\mathcal{C}}n}A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}\tilde{A}_{22}^{-t} .\nonumber
  \end{align*}
 We engender corresponding estimates of $\operatorname{Cov}(\hat\tau)$ by setting \eqref{eq:22}'s
 $(c_{2}, c_{1})$ to $(1, n_{\mathcal{Q}}n_{\mathcal{C}}^{-1})$, that is having
 \texttt{estfun.DA()} return $[\Psi\, \Xi] - 
 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
 \hat{A}_{11}^{-1}\hat{A}_{21}^{t}$, or simply $[\Psi\, \Xi]$ if $n_{\mathcal{C}}=0$, while having \texttt{bread.DA()}
 return $\hat{\tilde{A}}_{22}^{-1}$.


  \subsubsection{Clustered estimation of meat matrices}
For model-based estimation with units of observation and of assignment being
the same, $M$'s are estimated as follows.
\begin{equation}\label{eq:7}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} \Phi'\Phi & \hat{M}_{12} = (n_{\mathcal{C}}^{-1/2}\Phi)'(n^{-1/2} [\Psi\, \Xi])\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}[\Psi\, \Xi]'[\Psi\, \Xi].\\
    \end{array}
  \end{equation}
  In the more general case with potential clustering of observation
units, by units of assignment and maybe more, we can express
basic estimators with matrix (rather than summation) notation as
follows. Let
$F$ be the one-hot encoding of cluster membership, that is the \{0,1\}-valued $n \times m$ matrix with an indicator
column for each cluster. The HC0 estimators of meat matrix components
are then
\begin{equation} \label{eq:8}
  \begin{array}{cc}
  \hat{M}_{11} =n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} = [n_{\mathcal{C}}^{-1/2} (F'\Phi)]'[n^{-1/2} (F'[\Psi\, \Xi])]\\
    \hat{M}_{21} =\hat{M}_{12}' & \hat{M}_{22}= n^{-1}(F'[\Psi\, \Xi])'(F'[\Psi\, \Xi])\\
    \end{array}.
  \end{equation}

\subsection{Realization of goal} \label{sec:realization-goal}
Leaving the clusterwise aggregation to \texttt{sandwich}, as
appropriate, it seems that if we define
\texttt{sandwich::estfun.teeMod()} to return an $n \times (\tilde{k}+2)$ matrix
\begin{equation} \label{eq:15}
  [\Psi\, \Xi] -
  \frac{n}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{\tilde{A}}_{21}' = [\Psi\, \Xi] -
 \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
 \hat{A}_{11}^{-1}\hat{A}_{21}^{\prime},
\end{equation}
while \texttt{sandwich::bread()} as applied to
\texttt{teeMod} objects returns $\hat{\tilde{A}}_{22}^{-1} = nn_{\mathcal{Q}}^{-1}\hat{A}_{22}^{-1}$, then the effect of this will be for \texttt{sandwich::sandwich()}
and \texttt{sandwich::vcovCL()} to return \eqref{eq:6} with
$\hat{M}$'s,  as
given by \eqref{eq:7} or \eqref{eq:8} respectively, substituted for
$M$'s, and with $\hat{A}$'s for $A$'s.

(In \eqref{eq:15}
we use
\begin{align*}
  \hat{A}_{11}(\beta) &\defeq  n_{\mathcal{C}}^{-1}\sum_{i\in \bigcup
        \mathcal{C}}\nabla_{\beta}\phi(\vec{v}_{i};
        \beta ),\\
  \hat{\tilde{A}}_{21}(\beta) &\defeq n^{-1}\sum_{j\in \bigcup
        \mathcal{Q}}\nabla_{\beta}\begin{pmatrix}\psi(\vec{v}_{j};
        \tau, \alpha_{0}, \beta )\\ \xi(\vec{v}_{j};\beta, \alpha_{yg}\\ \end{pmatrix},\\
  \hat{A}_{11}&\defeq \hat{A}_{11}(\hat\beta)\, \text{and}\, \hat{\tilde{A}}_{21}\defeq \hat{\tilde{A}}_{21}(\hat\beta).
\end{align*}
Under the model based
formulation, $\hat{\tilde{A}}_{21}(\cdot) \equiv \tilde{A}_{21}(\cdot)$; when the
first-stage model is a glm, also $\hat{A}_{11}(\cdot) \equiv
\tilde{A}_{11}(\cdot)$.  However, in design based formulations neither
equivalence holds in general.)

Alternately put,
defining $(\tilde{k}+2)$ by 1 random matrices
\begin{equation}\label{eq:10}
    \psi_\text{DA}(\vec{V}; \tau, \alpha_{0},
    \beta) = \begin{pmatrix}\psi (\vec{V}; \tau,\alpha_{0},
    \beta) \\ \xi(\vec{V};\beta, \alpha_{yg}) \end{pmatrix} -
    \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}
    \hat{{A}}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{V}_{i};
    \beta),
\end{equation}
assuming the first-stage fit to be a glm, in the model-based formulation we have
\begin{multline}
  \operatorname{Cov}\left(n^{-1/2}\sum_{i=1}^{n} \psi_\text{DA}(\vec{V}_{i}; \tau_{(n)},\alpha_{(n)},
    \beta_{(n)})\right) = \\
  M_{22} -
                                 \frac{n_{\mathcal{Q}}}{(n n_{\mathcal{C}})^{1/2}}[M_{21}A_{11}^{-t}\tilde{A}_{21}^t
                                 + \tilde{A}_{21}A_{11}^{-1}M_{12}] +
                                 \frac{n_{\mathcal{Q}}^{2}}{nn_C}{A}_{21}A_{11}^{-1}M_{11}A_{11}^{-t}{A}_{21}^{t}, \label{eq:12}
                               \end{multline}
regardless of what adjustments for
clustering or heteroskedasticity we might seek to apply.


In the design-based formulation such equality would not generally
hold, as in that setting
$\hat{\tilde{A}}_{21}(\beta_{(n)})$ and $\hat{A}_{11}(\beta_{(n)})$ needn't
coincide with $\tilde{A}_{21}(\beta_{(n)})$ and ${A}_{11}(\beta_{(n)})$, respectively.  In both design- and model-based settings, absorption (e.g. of intercepts) introduces additional wrinkles.

This generalizes readily beyond comparison regressions of the type encoded in \eqref{eq:14}. Should $\tilde{\psi}(\cdot; \cdot)$ encode such a regression with a subgroup or continuous moderator variable, or even with the offset $g(\vec{x}\beta)$ replaced with independent variables $\lambda_{0}g(\vec{x}\beta) + \sum_{j=1}^{k}\lambda_{j} g(\vec{x}\beta)\indicator{z_{i}=j}$, then we only have to generate a corresponding ${A}_{21}(\cdot)$ to use
\begin{equation*}\label{eq:24}
      \tilde{\psi}_\text{DA}(\vec{V}; \tau, \alpha_{0},
    \beta) = \tilde{\psi} (\vec{V}; \tau,\alpha_{0},
    \beta) -
    \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}
    \hat{\mathrm{A}}_{21}(\beta) \hat{A}_{11}^{-1}(\beta)\phi(\vec{V}_{i};
    \beta)
\end{equation*}
in combination with $\hat{\tilde{A}}_{22}^{-1}$ as bread matrix. As long as the comparison regression value inherits from \texttt{lm} (or \texttt{glm}), \texttt{sandwich::bread()} as applied to it will with little or no modification return
$\hat{\mathrm{A}}_{22}^{-1}$, which we can readily rescale to obtain $\hat{\tilde{\mathrm{A}}}_{22}^{-1}$; \texttt{sandwich::vcov()} and \texttt{sandwich::vcovCL()} should do the right thing.


\section{Accommodating intercept(s) via absorption}\label{sec:accomm-interc-via}
For treatment effect estimators that partial out block fixed effects,
but do so without saddling the comparison or Q-regression with a new
parameter for each block, we use the
Frisch-Waugh-Lovell (FWL) Theorem to get rid of
it: use preliminary $\owt[]$-weighted least-squares regressions of
treatment and of moderator variables, if present, on block indicator
variables, to create block-mean centered versions of these intended
regressors; optionally recenter the outcome variable by subtracting
off some measure of central tendency; and finally regress intended outcome variables (with or without
prior centering) on the transformed
regressors.\footnote{This is the variant of the FWL theorem calling
  for regression of the unchanged independent variable on residualized
or partialed out dependent variables.  See e.g. Section 8.4.1,
``Regressing the partialled-out X on the full Y,'' of
\href{https://bookdown.org/ts_robinson1994/10_fundamental_theorems_for_econometrics/frisch.html}{Robinson's
  blog writeup of Wooldridge's 10 biggest principles of econometrics},
which in turn references Angrist \& Pitschke (2009).}
This ``absorbed'' regression involving transformed regressors is also weighted by $\owt[]$,
and can be fitted with or without an intercept.\footnote{Setting aside
numerical differences, the presence or absence of an intercept won't
matter.}  
In the process, we implicitly introduce at least $k$ new propensity score parameters
for each block in order to absorb block effects into the
independent variable(s) representing interest parameters.
These are ``slack'' parameters that
in actuality are known, in RCTs and in observational studies analyzed
according to the mimetic analysis template;  the fact that information about them
need not accumulate as the sample size grows does not in itself
threaten the asymptotics. Estimates of them are determined in a prior and separate regression
fit, and have an instrumental role.
For standard error estimation we fold them into the estimator stack in the same way that a prior
covariance model is folded in.

The canonical reference implementation would have introduced $[\alpha_{1}, \ldots, \alpha_{s}]$
as block-specific intercept parameters, turning
estimating functions \eqref{eq:14} into
\begin{multline}
  \label{eq:9}
  \psi(\vec{v}; \tau, \alpha, \beta) =
  \indicator{i \in \bigcup \mathcal{Q}} \owt[]\times \\
  \left(
    \begin{array}{c}
           [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} -
      \tau_{z_{i}}]\indicator{b_{i}=1}\\
      \vdots\\
          {} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_{s} -
      \tau_{z_{i}}]\indicator{b_{i}=s}\\
         {}  [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \sum_{r=1}^s\alpha_r\indicator{b_i=r}-\tau_{1}]\indicator{z_{i}=1}\\
                  \vdots \\
          {} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                 \sum_{r=1}^s\alpha_r\indicator{b_i=r} - \tau_{k}]\indicator{z_{i}=k}\\
    \end{array}
\right);
\end{multline}
we won't be introducing these parameters.
Recall that $\tau_{0} = \rho_{0}-\rho_{0}=0$.
In contrast to the situation with a grand-mean
intercept but no block effects (as reflected in \eqref{eq:14}), \eqref{eq:9}'s block-specific intercepts $\alpha_s$ do not ordinarily correspond to block means under control.\footnote{%
The argument of \S~\ref{sec:psit-rho_0-tau} no longer applies
to show the system of equations
$0=\sum_{i}\psi(\vec{v}_{i}; \tau, \alpha, \beta)$
equivalent to $0=\sum_{i}\grave{\psi}(\vec{v}_{i}; \tau,
\alpha, \beta)$, where $\grave{\psi}(\vec{v}; \tau,
\alpha, \beta)$ has uppermost $s$ entries
\begin{equation*}
    \left(
    \begin{array}{c}
           [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{1} ]\indicator{b_{i}=1} \indicator{z_{i}=0}\\
      \vdots\\
          {} [y_{i}[0] - g(\vec{x}_{i}\beta)-
                  \alpha_{s}]\indicator{b_{i}=s} \indicator{z_{i}=0}\\
    \end{array}
    \right),
  \end{equation*}
in parallel with \eqref{eq:5}, but bottom $k$ entries the same as ${\psi}(\vec{v}_{i}; \tau,
\alpha, \beta)$'s. That is, it may not be the case that the $\alpha$-solutions of
$0=\sum_{i}\psi(\vec{v}_{i}; \tau, \alpha, \beta)$ are
block-specific control group means.  Accordingly its $\tau$-solutions
do not necessarily bear interpretation as contrasts of Hajek estimators.
}

\subsection{Right-side-only absorption}\label{sec:right-side-only}
As applied to a \texttt{teeMod} object with
\texttt{absorbed\_intercepts} set to \texttt{T}, we need \texttt{estfun()} to switch out the system of equations
$0=\sum_{i}\operatorname{vec}[\psi(\vec{v}_{i}; \tau, \alpha, \beta); \xi(\vec{v}_{i}; \alpha_{y}, \alpha_{g}, \beta)]$
for the pair of systems $0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$ and
$0= \sum_{i}\operatorname{vec}[\acute{\psi}(\vec{v}_{i}; \tau, \alpha_{0}, \mathbf{p}, \beta); \acute{\xi}(\vec{v}_{i}; \alpha_{y}, \alpha_{g}, \beta, \mathbf{p})]$, where
\begin{align}
  \absorbInterceptsEF(\vec{v}_{i}; \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{ccc}
      (\indicator{z_{i}=0}-p_{01})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s})\indicator{b_{i}=s}
      \\
      (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  \right)%
  \right) ;\nonumber \\
  \acute{\psi}(\vec{v}_{i}; \alpha_{0}, \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber \\
&\left(
  \begin{aligned}
\big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}\big]& \\
   \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}\big]
    &(
    \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - {\alpha}_{0}-
    \tau_{z_{i}}\big]
    &(\indicator{z_{i}=k}
    - \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r})\\
  \end{aligned}
  \right) ; \label{eq:upsilondef}\\
  \acute{\xi}(\vec{v}_{i}; \alpha_{y}, \alpha_{g}, \beta, \mathbf{p}) &=
\indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]  \begin{pmatrix}
        y_{i}[0] - \alpha_{y0} \\
g(\vec{x}_{i}\beta) - \alpha_{g0}
  \end{pmatrix}
  (1 - \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}).\label{eq:35}
\end{align}
Here $\alpha_{0}$ is a single intercept
that does not arise
in the canonical reference implementation \eqref{eq:9} without absorption, which has $\alpha_{1},
\ldots, \alpha_{s}$ but not $\alpha_{0}$,
but is presumed to be included in the
regression of the outcome on block-absorbed treatment indicators.
Supplemental intercept parameters $\alpha_{y0}$ and $\alpha_{g0}$ are estimated separately, as are the corresponding
columns of $\acute{\Xi} = [\acute{\xi}(\vec{v}_{i}; {\alpha}_{y0}, {\alpha}_{g0}, \beta, {\mathbf{p}}, ) :i]'$ and rows of $\hat{A}_{21}$ and $\hat{\tilde{A}}_{22}$; do not ordinarily satisfy the relationship $\alpha_{0} = \alpha_{y0} - \alpha_{g0}$ that obtained in the case without absorption (Sec.~\ref{sec:xivecv_i-alph-alph}); but are
interpretable as weighted averages of control group
observations, as described in Section~\ref{sec:interc-relat-aspects}
below (which also describes $A$ matrix entries corresponding to it).
With a subgrouping or continuous moderator, $\acute{\xi}(\cdot)$
becomes
\begin{equation*}
\indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]
  \begin{pmatrix}
    \indicator{m_{i}=\ell_{1}}(y_{i}-\alpha_{y\ell_{1}0})  (1 - \sum_{r=1}^{s}p_{0r\ell_{1}}\indicator{b_{i}=r})\\
    \vdots\\
    \indicator{m_{i}=\ell_{g}}(y_{i}-\alpha_{y\ell_{g}0})  (1 - \sum_{r=1}^{s}p_{0r\ell_{g}}\indicator{b_{i}=r})\\
    \indicator{m_{i}=\ell_{1}}[g(\vec{x}_{i}\beta) -
    \alpha_{g\ell_{1}0}](1 - \sum_{r=1}^{s}p_{0r\ell_{1}}\indicator{b_{i}=r})\\
    \vdots\\
    \indicator{m_{i}=\ell_{g}}[g(\vec{x}_{i}\beta) -
    \alpha_{g\ell_{g}0}](1 - \sum_{r=1}^{s}p_{0r\ell_{g}}\indicator{b_{i}=r})\\
  \end{pmatrix}  
\end{equation*}
or
\begin{equation*}
  \indicator{i \in \bigcup \mathcal{Q}, z_{i}=0}\owt[{[0]}]
  \begin{pmatrix}
    y_{i}-m_{i}\lambda_{y0} - \alpha_{y0}\\
    (y_{i}-m_{i}\lambda_{y0} - \alpha_{y0})m_i\\    
    g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}\\
    [g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}]m_{i}\\
  \end{pmatrix}   (1 - \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}),
\end{equation*}
respectively.  These are the same as the $\xi(\cdot)$ estimating
equations of \eqref{eq:36} or \eqref{eq:37}, respectively, with the
sole difference of the $\acute{\xi}$ expressions' final blockwise
probability of assignment to treatment factor,  $(1 -
\sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r})$. 

By simplifying sums of \eqref{eq:upsilondef},  block $b$'s contributions
$\sum_{i \in (\bigcup b)} \acute{\psi}(\vec{v}_{i};
\alpha_{0} \tau,
\mathbf{p}, \beta)$ and
$\sum_{i \in (\bigcup b)} \acute{\xi}(\vec{v}_{i};
\alpha_{y0}, \alpha_{g0}, \mathbf{p}, \beta)$ can be seen to be 
\begin{multline}

  \sum_{i \in (\bigcup b)}\acute{\psi}_{i}=  \big(\sum_{i} \owt[]\big)
        \left(
          \begin{array}{c}
            \frac{\sum_{i}\owt[]\tilde{y}_{i}}{\sum_{i} \owt[]}\\
         {} (1-{p}_{1b})
            \hat{p}_{1b}\frac{\sum_{{z_i=1}}\owt[]\tilde{y}_{i}}{\sum_{{z_i=1}}\owt[]}  -
      p_{1b}(1-\hat{p}_{1b})\frac{\sum_{{z_i\neq
            1}}\owt[]\tilde{y}_{i}}{\sum_{{z_i\neq 1}}\owt[]}\\
      \vdots\\
         {}
            (1-{p}_{kb})\hat{p}_{kb}\frac{\sum_{{z_i=k}}\owt[]\tilde{y}_{i}}{\sum_{{z_i=k}}\owt[]}  -
      p_{kb}(1-\hat{p}_{kb})\frac{\sum_{{z_i\neq
            k}}\owt[]\tilde{y}_{i}}{\sum_{{z_i\neq k}}\owt[]} \\
          \end{array}
        \right)\\
= 
        \left(
          \begin{array}{c}
            \sum_{i}\owt[]\tilde{y}_{i}\\
         {} (1-{p}_{1b})
            {\sum_{{z_i=1}}\owt[]\tilde{y}_{i}}  -
      p_{1b}{\sum_{{z_i\neq
            1}}\owt[]\tilde{y}_{i}}\\
      \vdots\\
         {}
            (1-{p}_{kb}){\sum_{{z_i=k}}\owt[]\tilde{y}_{i}}  -
      p_{kb}{\sum_{{z_i\neq
            k}}\owt[]\tilde{y}_{i}} \\
          \end{array}
        \right)\\
  \label{eq:31}
\end{multline}
and
\begin{multline}\label{eq:34}
\sum_{i \in (\bigcup b)}\acute{\xi}_{i}=  \big(\sum_{i} \owt[]\big)
\begin{pmatrix}
              (1-{p}_{0b})
            \hat{p}_{0b}\left(\frac{\sum_{{z_i=0}}\owt[][y_{i}[0] -\alpha_{y0}] }{\sum_{{z_i=0}}\owt[]}
            \right)\\
              (1-{p}_{0b})
            \hat{p}_{0b}\left(\frac{\sum_{{z_i=0}}\owt[][g(\vec{x}_{i}\beta)-\alpha_{g0}] }{\sum_{{z_i=0}}\owt[]}
            \right)\\
\end{pmatrix}\\
=
\begin{pmatrix}
              (1-{p}_{0b})
            {\sum_{{z_i=0}}\owt[] [y_{i}[0] -\alpha_{y0}]}
            \\
              (1-{p}_{0b})
            {\sum_{{z_i=0}}\owt[] [g(\vec{x}_{i}\beta)-\alpha_{g0}]}
  \\
\end{pmatrix},
\end{multline}
where the sums at right are restricted to $i \in \bigcup b$ and $\tilde{y}_{i}$ is the residual
$y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-{\alpha}_{0}-\tau_{z_{i}}$.

(If \texttt{absorb\_intercepts=FALSE}, then we'll revert to \eqref{eq:14}, i.e.
\begin{equation*}
         \psi(\vec{v}_{i};
  \alpha_{0}, \tau, \beta) = \indicator{i \in \bigcup \mathcal{Q}}\owt[]
         \left( \begin{array}{l}
           y_{i}[z_{i}] - g(\vec{x}_{i}\beta)-
                  \alpha_0 - \tau_{z_{i}}\\
           {}[y_{i}[1] - g(\vec{x}_{i}\beta)-
                  \alpha_0-\tau_{1}]\cdot \indicator{z_{i}=1}\\
                  \vdots \\
           {}[y_{i}[k] - g(\vec{x}_{i}\beta)-
                 \alpha_0 - \tau_{k}]\cdot \indicator{z_{i}=k}\\%
                \end{array}\right),
\end{equation*}
as supplemented by \eqref{eq:34}, skipping the introduction of slack parameters $\mathbf{p}$.)

Returning to \eqref{eq:upsilondef}, in the model based formulation $[Z_{i}: i]$ is held fixed, so
solutions $\hat{\mathbf{p}}$ of
$0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$ have
covariance 0.  Writing $\acute{\Psi} =
[\acute{\psi}(\vec{v}_{i}; \hat{\mathbf{p}}, \hat{\beta},  \hat{\alpha}_{0}, \hat{\tau}) : i]'$ and $\acute{\Xi} = [\acute{\xi}(\vec{v}_{i}; \hat{\mathbf{p}}, \hat{\alpha}_{y0}, \hat{\alpha}_{g0}, \hat\beta) :i]'$ , one
obtains appropriate\footnote{%
A difference between this and the matrices that \texttt{estfun.lm()}
would have produced for either the regression with block fixed effects
or a regression absorbing block means from the dependent as well as
the independent variables is that according to \eqref{eq:upsilondef},
the residuals in $\acute{\Psi}$ are not stripped of stratum
effects.  This is done because in asymptotics with the number of
blocks growing in tandem with the number of observations,
\eqref{eq:9}'s stratum
effects $\alpha_{1}, \ldots, \alpha_{s}$ need not be consistently
estimated, so that making the variance estimator depend on them would
appear to compromise consistency of variance estimation.  The
inclusion of an overall intercept $\alpha_{0}$ in the absorbed regression
\eqref{eq:upsilondef} mitigates the inflation of our
residuals relative to those of ``naive'' covariance estimators
with dependence on $\hat{\alpha}_{1}, \ldots, \hat{\alpha}_{s}$.

It must be acknowledged that $\acute{\Psi}$ as defined here shares
in the same naivete: it depends on $\hat{\mathbf{p}}$, which can be
inconsistent for ${\mathbf{p}}$ in increasing-numbers-of-blocks
regimes just as $\hat{\alpha}_{1}$ may be inconsistent for $\alpha_{1}$. We
would address this by defining $\acute{\Psi} =
[\acute{\psi}(\vec{v}_{i}; {\mathbf{p}}, \hat{\beta},
\hat{\alpha}_{0}, \hat{\tau}) : i]'$, using
the \texttt{StudySpecification} and an assumed treatment allocation
model to infer rather than estimate $\mathbf{p}$.  For now we are
holding off on introducing this second difference between our
estimator and conventional estimates.  Observe that in the special
case of no within-block variation in cluster size, $\hat{\mathbf{p}} =
\mathbf{p}$ with probability one, and our use of $\hat{\mathbf{p}}$ is
not a threat.\label{fn:p-est-vs-inf}}
  sandwich estimates by substitution of $\acute{\Psi}$ and $\acute{\Xi}$ for
$\Psi$ and $\Xi$ in \eqref{eq:15}.  That is, for a
\texttt{teeMod} object \texttt{DA} we should have
\begin{equation*}\label{eq:26}
\mathtt{estfun.teeMod(DA)} =
  \begin{cases}
  [\acute{\Psi}\, \acute{\Xi}] -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'& \mathtt{DA} \text{ absorbs
    intercepts}\\
 [{\Psi}\, {\Xi}] - 
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  & \text{ otherwise}.
\end{cases}
\end{equation*}
For estimators $\hat{A}_{2*}$ of $A_{21}$ and $A_{22}$, we apply the plug-in
principle to incidental parameters $\mathbf{p}$ as well as to $\beta$,
$\tau$ and the alphas. This decision to treat $p$s in the same way as
other parameters (which may be revisited; see note~\ref{fn:p-est-vs-inf}) allows
us to obtain $\hat{A}_{22}$ through conventional \texttt{bread.lm}
calculations. 

In the design-based formulation, randomness of $Z$ can propagate to
sampling variability of $\hat{\mathbf{p}}$,  so we must attend to
variance contributions from $[\absorbInterceptsEF(z_{i}, \mathbf{p}): i]$.
The $\mathbf{p}$ being another nuisance parameter estimated in a
preliminary regression, we can get variance propagation for it by
another elaboration of estimating functions along the lines of
\eqref{eq:15}. We facilitate doing
this by creating a
helper \texttt{estfun\_DB\_blockabsorb()}, applying to
\texttt{teeMod} objects \texttt{DB} and returning
\begin{equation*}
\begin{cases}
  \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}
  \begin{bmatrix}\hat{A}_{\tau\,\mathbf{p}}\\ \hat{A}_{\alpha_{yg}\mathbf{p}}\end{bmatrix}' &
  \mathtt{DA} \text{ absorbs intercepts}\\
0 & \text{ otherwise}\\
\end{cases}
\end{equation*}
where
$\AbsorbInterceptsEF= [\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p}): i]'$,
\marginpar{``$n^{-1}$'' $\mapsto$ ``$n_{\mathcal{Q}}^{-1}$''?}
$A_{\mathbf{p}, \mathbf{p}} = n^{-1}\sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E} [\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p})]$, $A_{\tau, \mathbf{p}} = n^{-1} \sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E}[\nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha, \tau)]$ and
$A_{\alpha_{yg}, \mathbf{p}} = n^{-1} \sum_{i\in \bigcup
  \mathcal{Q}}\mathbb{E}[\nabla_{\mathbf{p}}\acute{\xi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha_{yg})]$.  For design-based variance-covariance
estimation one uses as estimating function
\begin{multline*}\label{eq:25}
  \mathtt{estfun.teeMod(DA)} -
  \mathtt{estfun\_DB\_blockabsorb(DA)} =\\
\begin{cases}
\acute{\Psi} -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'  - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\begin{bmatrix}\hat{A}_{\tau\,\mathbf{p}}\\ \hat{A}_{\alpha_{yg}, \mathbf{p}}\end{bmatrix}' & \mathtt{DA} \text{ absorbs intercepts}\\
 \Psi -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}'
 & \text{ otherwise}.\\
\end{cases}
\end{multline*}

In these displays, $A_{\mathbf{p}, \mathbf{p}}$ is
an average of diagonal matrices $\nabla_{\mathbf{p}}\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p})$ with $-1$s for diagonal entries $(b_{i}-1)(k+1)+1, \ldots,
b_{i}(k+1) +1$ and zeros elsewhere, while $A_{\tau,
  \mathbf{p}}$ and $A_{\alpha_{yg},
  \mathbf{p}}$ are scaled sums of $\mathbb{E} \nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha, \tau)$ and $\mathbb{E} \nabla_{\mathbf{p}}\acute{\xi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha_{yg})$ terms, $i \in \bigcup \mathcal{Q}$. The average $A_{\tau,
  \mathbf{p}}$ of expected contributions $\mathbb{E}[ \nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta, \alpha, \tau) \mid \mathcal{X}, \mathcal{Z}]$ is estimable by corresponding means of
random variables $\nabla_{\mathbf{p}}\acute{\psi}(\vec{V}_{i};
\mathbf{p}, \beta,  \alpha, \tau)$, and likewise $A_{\alpha_{yg},
  \mathbf{p}}$ is estimated as a mean of r.v.'s
$\nabla_{\mathbf{p}}\acute{\xi}(\vec{V}_{i}; \mathbf{p}, \beta,  \alpha_{yg})$. For estimators
$\hat{A}_{\tau, \mathbf{p}}$ and $\hat{A}_{\alpha_{yg},
  \mathbf{p}}$, these averages of random variables are
estimated by the plug-in principle, substituting $\hat{\mathbf{p}}$,
$\hat\beta$, $\hat{\alpha}$, $\hat{\tau}$ and $\hat{\alpha}_{yg}$
for $\mathbf{p}$, $\beta$, $\alpha$, 
$\tau$ and $\alpha_{yg}$ as necessary.%
\footnote{%
Another approach would use $\hat\beta$, $\hat{\alpha}$,
$\hat{\tau}$ and $\hat{\alpha}_{yg}$ but not $\hat{\mathbf{p}}$.  In this approach, should 
``${p}_{ij}$'' be needed we would infer its value from the
\texttt{StudySpecification} and the equation $\sum_{i} \EE [\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p}) \mid \mathcal{X}, \mathcal{Z}]=0$, rather than substituting
$\hat{p}_{b_{i}}$ as empirically defined via $\sum_{i} \absorbInterceptsEF(\vec{v}_{i};
\hat{\mathbf{p}})=0$.}
Observation $i$'s contribution $\nabla_{\mathbf{p}}\acute{\psi}(\vec{v}_{i};
\mathbf{p}, \beta, \alpha, \tau)$ is the product of the residual $\tilde{y}_{i}=y_{i} - g(\vec{x}_{i}\beta) -
\tau_{z_{i}}$ with a $(k+1) \times (k+1)s$ block matrix
consisting of a $(k+1)\times (k+1)$ diagonal of form
\begin{equation*}
  \begin{pmatrix}
    0& 0& \cdots & 0\\
    0&-1& \cdots&0\\
    \vdots&\vdots&\vdots& 0\\
    0&0&\cdots&-1\\
  \end{pmatrix}
\end{equation*}
at the
$b_{i}$th block and the 0 matrix for the $r$th $(k+1) \times (k+1)$ block whenever
$r\neq b_{i}$, while its $\nabla_{\mathbf{p}}\acute{\xi}(\vec{v}_{i};
\mathbf{p}, \beta, \alpha_{yg})$ contribution multiplies the same
residual with the $1 \times s$ matrix having $-1$ at positions 1,
$k+2$, \ldots, $1+(k+1)s$ and zeroes elsewhere. Blockwise
contributions to $A_{\alpha_{yg},  \mathbf{p}}$ are discussed further
in Section~\ref{sec:interc-relat-aspects}, where they are detailed in \eqref{eq:33}.

\subsection{Both-sides absorption} \label{sec:both-sides-absorpt}

As ordinarily applied, the FWL Theorem ``partials out'' fixed effects
from both dependent and independent variables, but so far we've only
done so with independent variables.  Which of these we do makes no
difference for point estimates, but standard errors are another
matter.  Here we describe a design-based variance estimator making use
of both-sides absorption plus additional assumptions.

To ensure
applicability with small and moderately sized strata, we must do this without
introducing parameter for each block. Instead, define the
block residual mean function
\begin{equation*}
  \hat{\alpha}(\mathbf{v}_{b}; \theta) \defeq \left\{\sum_{i \in
    (\bigcup b)}\owt{} [y_{i}[z_{i}] - g(\vec{x}_{i}\beta) -
  \alpha_{0} - \tau_{z_{i}}]\right\}\Big/ \left(\sum_{i \in (\bigcup b)}w_{i}\right),
\end{equation*}
where $\mathbf{v}_{b} = \Big\{\vec{v}_{i} : i \in \big(\bigcup b \big)\Big\}$ is the matrix of
data-and-potential-outcomes vectors for block $b \subseteq \mathcal{Q}$ and $\theta = (\tau, \alpha_{0}, \beta)$.  Now consider the pair
of systems $0 = \sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$
and
$0= \sum_{i}\breve{\psi}(\vec{v}_{i}; \tau, \alpha_{0}, \mathbf{p},
\beta)$, where $\absorbInterceptsEF(\cdot)$ is as above and for blocks
$b \subseteq \mathcal{Q}$,
\begin{multline}
  % \absorbInterceptsEF(\vec{v}; \mathbf{p}) &=
  %                                            \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  % &\operatorname{vec}\left(%
  %   \left(
  %   \begin{array}{ccc}
  %     (\indicator{z_{i}=0}-p_{01})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=0}-p_{0s})\indicator{b_{i}=s}
  %     \\
  %     (\indicator{z_{i}=1}-p_{11})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=1}-p_{1s})\indicator{b_{i}=s}
  %     \\
  %     \vdots & \vdots & \vdots \\
  %     (\indicator{z_{i}=k}-p_{k1})\indicator{b_{i}=1}
  %     &\ldots
  %     &
  %       (\indicator{z_{i}=k}-p_{ks})\indicator{b_{i}=s}\\                                      \end{array}
  % \right)%
  % \right) ;\nonumber \\
  \breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p}) =
\sum_{i \in (\bigcup b)}\owt[] \times  \\
\left(
  \begin{aligned}                                               
    y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-{\alpha}_{0} - \tau_{z_{i}}&\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}} - &\hat{\alpha}(\mathbf{v}_{b}; \theta)\big]
    (
    \indicator{z_{i}=1} - p_{1b})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{0}-
    \tau_{z_{i}} - &\hat{\alpha}(\mathbf{v}_{b}; \theta)\big]
    (\indicator{z_{i}=k} - p_{kb})\\
  \end{aligned}
\right) . \label{eq:upsilon-rsa-def}
\end{multline}
Denoting as $(\hat{p}_{0b}, \ldots, \hat{p}_{kb})$ the solutions in
$({p}_{0b}, \ldots, {p}_{kb})$ of
$0= \sum \{\absorbInterceptsEF(\vec{v}_{i}; {p}_{0b}, \ldots,
{p}_{kb}) : i \in (\bigcup b)\}$, and recalling that  $\tilde{y}_{i}$
is the residual
$y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}$, 
\eqref{eq:upsilon-rsa-def} gives, with sums understood to range over
$(\bigcup b)$:
\begin{multline*}
    \breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p}) = \Big(\sum_{i} \owt[]\Big) \times\\
        \left(
          \begin{array}{c}
            \frac{\sum \owt[] \tilde{y}_{i}}{\sum \owt[]} \\
         {} (1-{p}_{1b})
            \hat{p}_{1b}\frac{\sum_{{z_i=1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i=1}}\owt[]}  -
      p_{1b}(1-\hat{p}_{1b})\frac{\sum_{{z_i\neq
            1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i\neq 1}}\owt[]}\\
      \vdots\\
         {}
            (1-{p}_{kb})\hat{p}_{kb}\frac{\sum_{{z_i=k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i=k}}\owt[]}  -
      p_{kb}(1-\hat{p}_{kb})\frac{\sum_{{z_i\neq
            k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}{\sum_{{z_i\neq k}}\owt[]} \\
          \end{array}
        \right)\\
= 
        \left(
          \begin{array}{c}
            \sum \owt[] \tilde{y}_{i}\\
         {} (1-{p}_{1b})
            {\sum_{{z_i=1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}  -
      p_{1b}{\sum_{{z_i\neq
            1}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}\\
      \vdots\\
         {}
            (1-{p}_{kb}){\sum_{{z_i=k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]}  -
      p_{kb}{\sum_{{z_i\neq
            k}}\owt[][\tilde{y}_{i} -
    \hat{\alpha}(\mathbf{v}_{b}; \theta)]} \\
          \end{array}
        \right).\\
\end{multline*}
Under the assumption that $\tau$ captures person-level treatment
effects, $\operatorname{Var}[ \hat{\alpha}(\mathbf{v}_{b}; \theta) \mid \mathcal{X}, \mathcal{Y}, 
\mathcal{Z}]=0$. Under this assumption, then, design-based covariances
of $\breve{\psi}(\vec{v}_{b}; \theta, \mathbf{p})$ are calculated in
the same way that design-based covariances of
$\acute{\psi}(\vec{v}_{b}; \theta, \mathbf{p})$ are calculated from
\eqref{eq:31}.  (To implement this design-based estimator, we can
define a function operation on \texttt{teeMod} objects that replaces
the residuals $\tilde{y}_{i}$ with block-centered residuals  $\tilde{y}_{i} -
\hat{\alpha}(\mathbf{v}_{b}; \hat\theta)$, where $b \ni i$, and then
proceeds as the calculation of design-based vcov with right-side absorption.)


\section{Intercept supplements for \texttt{teeMod} objects}%
\label{sec:repl-interc-textttt} 
Section~\ref{sec:psit-rho_0-tau} establishes that for estimation of
main effects, with blocks (if present) accommodated by weights rather than
absorption of one-way fixed effects, the intercept equals the
$\owt[{[0]}]$-weighted control group mean of partial residuals
$y[0] - g(\vec{x}\hat\beta)$. Under absorption, the intercept $\alpha$ determined jointly with $\tau$ lacks this interpretation, but with or without absorption we supplement the $(\alpha, \tau)$ report with summaries of the distributions under control of the response $y$ and offset $g(\vec{x}\hat\beta)$.  When estimating main effects only, this summary is simply a mean; when there is a moderator variable, the summary of control observations is a weighted regression involving the moderator. 
\subsection{Intercept supplement-related aspects of  \texttt{teeMod}s in general}%
\label{sec:interc-aspects-gen-teeMods}
\subsubsection{\texttt{(Intercept)}s with main effect and continuous
  moderator}
With a main effect only, our intercept supplements as determined by \eqref{eq:24}'s $\xi(\cdot)$, under \texttt{absorb=F}, or by \eqref{eq:35}'s $\xi(\cdot)$, under \texttt{absorb=T}, are weighted regressions of $y$ and $g(\vec{x}\hat\beta)$ on an \texttt{(Intercept)}.  These regressions are restricted to controls in the sense of downweighting non-control observations to 0. 

With a continuous moderator \texttt{m}/$m$, the regressions of $y$ and $g(\vec{x}\hat\beta)$ determining intercept supplements are similarly weighted, but have as regressors \texttt{(Intercept)} and \texttt{m} columns.  The intercept $\alpha$ that is co-determined with $\tau$ is without interpretation under \texttt{absorb=T}, but under \texttt{absorb=F} it and a continuous moderator main effect are equal to differences of corresponding \texttt{(Intercept)}- and \texttt{m}- coefficients reported as intercept supplements. 

\subsubsection{ With a subgrouping
  moderator, intercept supplements include no \texttt{(Intercept)}}
With a categorical moderator \texttt{sg}, \texttt{lmitt.formula()}'s call to
\texttt{lm()} reports coefficients from \texttt{y
  \textasciitilde\ assigned():sg + sg}: $k$ treatment-control contrasts,
\texttt{z.:sg1}, \ldots, \texttt{z.:sg<k>}; a
coefficient labeled ``\texttt{(Intercept)}'' and $k-1$
coefficients \texttt{sg2}, \ldots, \texttt{sg<k>}.  In the scenario
without absorption, the \texttt{(Intercept)} that such a call to \texttt{lm()}
returns is a weighted mean of
controls with \texttt{sg == 1}, while the similarly weighted control mean
of subgroup 2 would be given \texttt{(Intercept)
+ sg2} (and similarly for subgroups 3, \ldots, $k$).
To improve clarity, our intercept supplement reports coefficients of regressions of $y$ and $g(\vec{x}\hat\beta)$ on subgroup indicator variables \texttt{sg1}, \ldots, \texttt{sg<k>}, ie the ``one-hot'' encoding of the moderator; thus none get the label ``\texttt{(Intercept)}'', and each is interpretable as a weighted control-group mean within the subgroup. 

This structure and labeling should be reflected in
\texttt{bread.teeMod()}, the values of which have block-diagnonal structure with $\psi$- and $\xi$-blocks. 

\subsection{Intercept supplement-related aspects of absorption  (\texttt{lmitt(y
    \textasciitilde\ \ldots, absorb=T)})}\label{sec:interc-relat-aspects}

\subsubsection{Main effect only}
With block fixed effects, estimating equations $\sum_{i}\acute{\psi}_{i}(\theta)$ defining
the main effect estimators $\hat{\tau}_{j}$,
$1\leq j \leq k$,  admit expression in terms of differences of weighted averages
under treatment and control.  For each $j$ there are unit weights $\{w_{\text{BFE}i}:
i\}$ determined as follows. For blocks $b \subseteq \mathcal{Q}$,
empirical solutions $\{\hat{p}_{jb}:b\}$  of $\sum_{i \in \bigcup b}
\absorbInterceptsEF (z_{i}; p_{jb})=0$ (cf. \eqref{eq:upsilondef}) and
for $i \in \bigcup b$,
\begin{equation} \label{eq:28}
w_{\text{BFE}i(j)} = 
  \begin{cases}
(1-\hat{p}_{jb})\owt[]& z_i=j \text{ and } 0 < \sum_{i \in \bigcup b} \owt[] \indicator{z_i=j} < \sum_{i \in \bigcup b} \owt[];\\ 
\hat{p}_{jb} \owt[] & z_i\neq j \text{ and }0 < \sum_{i
  \in \bigcup b} \owt[] \indicator{z_i=j} < \sum_{i \in \bigcup b} \owt[]\\
0 &  \sum_{i \in \bigcup b} \owt[] \indicator{z_i=j} = 0 \text{ or }
  \sum_{i \in \bigcup b} \owt[].
\end{cases}
\end{equation}
With a binary treatment this is
\begin{equation*}
w_{\text{BFE}i} = 
  \begin{cases}
(1-\hat{p}_{b_{i}})\owt[]& z_i=1 \text{ and } 0 < \sum_{i \in \bigcup b} z_i\owt[] < \sum_{i \in \bigcup b} \owt[];\\ 
\hat{p}_{b_{i}} \owt[] & z_i=0 \text{ and }0 < \sum_{i
  \in \bigcup b} z_i\owt[] < \sum_{i \in \bigcup b} \owt[]\\
0 &  \sum_{i \in \bigcup b} z_i\owt[]  = 0 \text{ or }
  \sum_{i \in \bigcup b} \owt[],
\end{cases}
\end{equation*}
with ``$(1-p_{b})$'' and ``$p_{b}$'' corresponding to``$p_{0b}$''
and ``$p_{1b}$,'' respectively, as defined by \eqref{eq:upsilondef}.
These ``block fixed effects weights'' then resemble overlap weights,
$z_{i}(1-\pi_{i}) + (1-z_{i})\pi_{i}$ for $i\in \bigcup b$. (They coincide with
overlap weighting in the special case of RCTs with binary treatment,
units of analysis that coincide with units of assignment,
and prior case weights $\{w_{i}:i\}$ not present or not varying within blocks.)


With a binary or dichotomized treatment, these 
weights' role in determining effect estimates admits a
succinct description: $\hat{\tau}$ is simply the difference of block fixed
effects-weighted averages of treatment and control groups.  So in this
case we report as intercept supplements the coefficients of $w_{\text{BFE}}$-weighted
regressions of $y$ and $g(\vec{x}\hat\beta)$. As the
averaging is over controls, the relevant weights $w_{\text{BFE}i}$ take
the form $\hat{p}_{b_{i}} \owt[]$, where $b_{i}$ denotes the block $b$
that contains (the cluster containing) $i$.
Rows of $A_{21}$ corresponding to the supplemental $y$ regression are 0, and $A_{21}$ rows for the supplemental $g(\cdot)$ regression are, in the main effect only case, 
\begin{equation}\label{eq:11}
  n_{\mathcal{Q}}^{-1}\sum_{i\in \bigcup\mathcal{Q} }
  (1-z_{i}){p}_{b_{{i}}}\owt[{[0]}] g'(\vec{x}_{i}{\beta})
  \vec{x}_{i}, 
\end{equation}
with $(1-z_{i})$ replaced by $\operatorname{Pr}(Z_{i}=0 \mid \mathcal{X}, \mathcal{Z})
= 1-\pi_{i}$ in the design-based version of $A_{21}$. 
The $\alpha_{y0}$ row of $\tilde{A}_{22}$ has
in the $\alpha_{y0}$ column
\begin{equation}\label{eq:32}
  -n^{-1}\sum_{i \in \cup \mathcal{Q}}
(1-z_{i}){p}_{b_{{i}}}\owt[{[0]}],
\end{equation}
or $-n^{-1}\sum_{i \in \cup \mathcal{Q}}
(1-\pi_{i}){p}_{b_{{i}}}\owt[{[0]}]$ in the design-based formulation,
and zeroes in remaining columns, as $\tau$ parameters don't
contribute to $\acute{\psi}_{i1}$; similarly the $\alpha_{g0}$ row has \eqref{eq:32} at the column corresponding to $\alpha_{g0}$ and zeroes elsewhere. Estimators $\hat{A}_{21}$ and
$\hat{\tilde{A}}_{22}$ replace $p_{b_{i}}$ and $\beta$ with
$\hat{p}_{b_{i}}$ and $\hat{\beta}$ in these expressions.%
\footnote{An alternate approach would be to plug in $\hat\beta$ for
  $\beta$ but not $\hat{\mathbf{p}}$ for $\mathbf{p}$, instead use
the \texttt{StudySpecification} to infer the values of 
$p_{b_{i}}$ solving $\sum_{i} \EE [\absorbInterceptsEF(\vec{V}_{i};
\mathbf{p}) \mid \mathcal{X}, \mathcal{Z}]=0$, rather than substituting
$\hat{p}_{b_{i}}$ as empirically defined via  $\sum_{i} \absorbInterceptsEF(\vec{v}_{i};
\hat{\mathbf{p}})=0$.  This would require an assumption
of block random assignment of clusters, but would circumvents the problem
of these $\hat{p}_{b}$'s not being consistently estimated under
asymptotics in which blocks increase in number while their sizes stay
fixed. See also footnote~\ref{fn:p-est-vs-inf}.}
Columns of $\tilde{A}_{22}$ corresponding to derivatives with respect to these intercept supplement parameters also have zeroes on the off-diagonal. 

To estimate $A_{21}$ and $A_{22}$ under the design-based formulation
we'll continue to use \eqref{eq:11} and \eqref{eq:32} to approximate
the $\alpha_{g0}$ row of $A_{21}$ and the  $(\alpha_{y0}, \alpha_{y0})$ and $(\alpha_{g0}, \alpha_{g0})$ positions of $A_{22}$. This avoids having to keep track of
$\owt[{[0]}]$ for observations $i$ for which $z_{i}\neq 0$, and in
observational studies it avoids the 
need to estimate $(\pi_{i}: i)$.  Taken as random variables,
\eqref{eq:11} and \eqref{eq:32} are
both design-unbiased for their corresponding estimands given by
the same expressions with $(1-\pi_{i})$ substituted for $(1-z_{i})$; this
is true in observational studies as well as experiments.

Design-based calculations also call for adjusting objects \texttt{estfun.teeMod(DA)} by subtracting off
\texttt{estfun\_DB\_blockabsorb(DA)} = $\AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\left[\begin{smallmatrix}\hat{A}_{\tau\,\mathbf{p}}\\ \hat{A}_{\alpha_{yg}\mathbf{p}}\end{smallmatrix}\right]'$, per \S~\ref{sec:accomm-interc-via}
  above. In light of \eqref{eq:34}, 
  $A_{\alpha_{yg}\mathbf{p}}$ has in
  its column $s$ (partial w/r/t $p_{s}$, $s$ a block of $\mathcal{Q}$)
  \begin{multline}\label{eq:33}
  \left[n^{-1}\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right](\begin{bmatrix} \alpha_{y0\mathcal{Z}s}\\ \alpha_{g0\mathcal{Z}s}  \end{bmatrix} -
  \begin{bmatrix} \alpha_{y0\mathcal{Z}}\\ \alpha_{g0\mathcal{Z}}  \end{bmatrix}) =\\ n^{-1}\left\{\left[\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right]\begin{bmatrix} \alpha_{y0\mathcal{Z}s}\\ \alpha_{g0\mathcal{Z}s}  \end{bmatrix} - \left[\sum_{i: b_{i}=s}\owt[{[0]}]
    (1-\pi_{i})\right]\begin{bmatrix} \alpha_{y0\mathcal{Z}}\\ \alpha_{g0\mathcal{Z}}  \end{bmatrix}\right\}, 
  \end{multline}
  for $\pi_{i} = \operatorname{Pr}(Z_{i}=1 \mid \mathcal{X}, \mathcal{Z})$, where
  $\pi_{i} = p_{b_{i}}$ in an RCT but not necessarily otherwise;
  \begin{equation*}
        \begin{bmatrix} \alpha_{y0\mathcal{Z}r}\\ \alpha_{g0\mathcal{Z}r}  \end{bmatrix}=
  \frac{\sum_{b_{i} =r}\owt[{[0]}](1-\pi_{i})\begin{bmatrix}y_{i}[0]\\
    g(\vec{x}_{i}\beta)\end{bmatrix}}{\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})}
  \text{ and }
  \begin{bmatrix} \alpha_{y0\mathcal{Z}}\\ \alpha_{g0\mathcal{Z}}  \end{bmatrix} = \frac{\sum_{r}p_{r}\big[\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})\big]
    \begin{bmatrix} \alpha_{y0\mathcal{Z}r}\\ \alpha_{g0\mathcal{Z}r}\end{bmatrix}}{\sum_{r}p_{r}\big[\sum_{b_{i}=r}\owt[{[0]}](1-\pi_{i})\big]};
  \end{equation*}
  and to obtain $\hat{A}_{\alpha_{yg}\mathbf{p}}$ we plug into our
  ${A}_{\alpha_{yg},\mathbf{p}}$-expression $z_{i}$'s for
  $\pi_{i}$s. The resulting estimators
  \begin{align*}
  \begin{bmatrix} \hat{\alpha}_{y0\mathcal{Z}r}\\ \hat{\alpha}_{g0\mathcal{Z}r}
  \end{bmatrix}
    =&
  \frac{\sum_{i: b_{i}=r, z_{i}=0}\owt[]\begin{bmatrix}y_{i}[0]\\
                               g(\vec{x}_{i}\beta)\end{bmatrix}}{\sum_{i:b_{i}=r,
                               z_{i}=0}\owt[]} \text{ and}\\
    \begin{bmatrix} \hat{\alpha}_{y0\mathcal{Z}}\\ \hat{\alpha}_{g0\mathcal{Z}}
    \end{bmatrix} =&
%%% (taking out special casing by RCT/obs study type, which would
%%% have warranted consistency in the RCT case even w/o special
%%% conditions on block structure)
%%%                     \begin{cases}
  % \frac{\sum_{r}p_{r}\cdot (\sum_{i: b_{i}=r, z_{i}=0}\owt[]) \begin{bmatrix} \hat{\alpha}_{y0\mathcal{Z}r}\\ \hat{\alpha}_{g0\mathcal{Z}r}
  % \end{bmatrix}}{%
  %   \sum_{r}p_{r} \cdot (\sum_{i: b_{i}=r, z_{i}=0}\owt[])}
  %    &\text{ (for
  % RCTs)}\\
  \frac{\sum_{r}\hat{p}_{r}\cdot (\sum_{i: b_{i}=r, z_{i}=0}\owt[]) \begin{bmatrix} \hat{\alpha}_{y0\mathcal{Z}r}\\ \hat{\alpha}_{g0\mathcal{Z}r}
  \end{bmatrix}}{%
    \sum_{r}\hat{p}_{r} \cdot (\sum_{i: b_{i}=r, z_{i}=0}\owt[])}
%      & \text{ (for
% obs studies)}\\
% \end{cases}
  \end{align*}
  give unbiased estimation of the first term at right of
  \eqref{eq:33}, with the second term at right of
  \eqref{eq:33} the product of an unbiased term, $\sum_{i: b_{i}=r,
    Z_{i}=0}\owt[]$ (unbiased for $\sum_{i:
    b_{i}=s}\owt[{[0]}](1-\pi_{i})$), 
  and near-ratio estimators, $\hat{\alpha}_{y0\mathcal{Z}}$ and
  $\hat{\alpha}_{g0\mathcal{Z}}$, that
    are consistent as blocks either uniformly increase without bound in size,
or uniformly diminish without bound in $\owt[]$-variation.
%%% NB: Had we used p_{r}'s not \hat{p}_{r}'s for RCTs, then in that case
%%% the \hat{\alpha}_{yg0\mathcal{Z}}'s would be ratio estimators not
%%% just near-ratio estimators, and we would be assured of
%%% consistency in those cases, even w/o the special conditions on
%%% block growth. 

  \subsubsection{Main effect and continuous moderator}
As in the main effect only case, $\acute{\xi}(\cdot)$ encodes
regressions of $y$ and $g(\vec{x}\beta)$ with weights
$w_{\text{BFE}i}\indicator{z_i=0}$.  Within the control group, this is
again the same as $\hat{p}_{b_i}\owt[]$, $b_i$ denoting the block
containing $i$. But here the regressions are on an
intercept and the continuous moderator $m$. Accordingly
$\acute{\xi}(\cdot)$ is now a 4-vector, with intercept and $m$ terms
for the $y$ and for the $g$ regresssions.  There is no longer
equivalence to $\owt[]\indicator{z_{i}=0}$-weighted regression of $y$
and $g$ on the block-absorbed transformation of the
\texttt{[(Intercept), m]} model matrix.

Again rows of $A_{21}$ corresponding to the supplemental $y$
regression are 0. The lower rectangular block of $A_{21}$ rows,
corresponding to the supplemental $g(\cdot)$ regression's $\xi_{i}$
entries, is
\begin{equation*}
    n_{\mathcal{Q}}^{-1}\sum_{i\in \bigcup\mathcal{Q} }
  (1-\pi_{i}){p}_{b_{{i}}}\owt[{[0]}] g'(\vec{x}_{i}{\beta})
  \vec{x}_{i} \begin{bmatrix}    1 \\ m_{i}  \end{bmatrix}, 
\end{equation*}
again with $(1-z_{i})$ replaced by $\operatorname{Pr}(Z_{i}=0 \mid \mathcal{X}, \mathcal{Z})
= 1-\pi_{i}$ in the design-based version. The square submatrix of
$\tilde{A}_{22}$ corresponding to $\xi_{i}$ entries for the
supplemental $y$ regression and corresponding parameters
$(\alpha_{y0}, \alpha_{y m})$ is
\begin{equation*}
    -n^{-1}\sum_{i \in \cup \mathcal{Q}}
(1-z_{i}){p}_{b_{{i}}}\owt[{[0]}],
\begin{bmatrix}
  1 & m_{i}\\
  m_{i} & m_{i}^{2}\\
\end{bmatrix},
\end{equation*}
with $(1-z_{i})$'s replaced with $(1-\pi_{i})$'s in the design-based
formulation.  The square submatrix of
$\tilde{A}_{22}$ corresponding to $\xi_{i}$ entries for the
supplemental $g(\vec{x}\beta)$ regression and corresponding parameters
$(\alpha_{g0}, \alpha_{g m})$ is precisely the same, as in the main
effects only case; also as in that case, the broader square submatrix of
$\tilde{A}_{22}$ corresponding to $\xi_{i}$ entries for both
supplemental regressions is 0 outside of these two blocks.

Again $A_{21}$ and $\tilde{A}_{22}$ are estimated the same way under
design- and model-based setups, with the same rationale. Design-based calculations still call for adjusting objects \texttt{estfun.teeMod(DA)} by subtracting off
\texttt{estfun\_DB\_blockabsorb(DA)} = $\AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\left[\begin{smallmatrix}\hat{A}_{\tau\,\mathbf{p}}\\
      \hat{A}_{\alpha_{yg}\mathbf{p}}\end{smallmatrix}\right]'$.
  Following \eqref{eq:33} in restricting to the binary treatment case,
  for ease of notation, the
  $r$ column of ${A}_{\alpha_{yg}\mathbf{p}}$ represents partials with
  respect to $p_{r}$ (i.e. $1-p_{0r}$ in the multi-level treatment case), $r$ a block of $\mathcal{Q}$, and contains
  \begin{equation*}
    n^{-1}\sum_{i\in \bigcup \mathcal{Q}: b_i=r}(1-\pi_{i})
    \owt[{[0]}]
  \begin{pmatrix}
    y_{i}-m_{i}\lambda_{y0} - \alpha_{y0}\\
    (y_{i}-m_{i}\lambda_{y0} - \alpha_{y0})m_i\\    
    g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}\\
    [g(\vec{x}_{i}\beta) - m_{i}\lambda_{g0}- \alpha_{g0}]m_{i}\\
  \end{pmatrix}.\end{equation*}
  Here as with \eqref{eq:33}, an estimator
  $\hat{A}_{\alpha_{yg}\mathbf{p}}$ is obtained by substituting
  ``$(1-z_{i})$'' for ``$(1-\pi_{i})$'' in this expression. 

\subsubsection{Subgroup effects}
When estimating subgroup effects with absorption of blocks,
the equivalent weighting scheme is determined by applying
\eqref{eq:28} separately within subgroups.  (This differs from
continuous moderation, where the presence of the moderator does not
bear on the weighting scheme.) First, for each block $s$
and subgrouping level $\ell$, the empirical solution
$\hat{p}_{js\ell}$  of $\sum_{i \in s, \ell}
\absorbInterceptsEF (z_{i}; p_{js\ell})=0$ is found.
(There may be $s$ and $\ell$ for which $s \cap \ell = \emptyset$. In
these cases we can set $\hat{p}_{js\ell}=0$; setting it to any value
would give the same result.)  Then we can figure a ``main effect'' for
subgroup level $\ell$ as the average over control group, subgroup
$\ell$ members, calculated with weights $\{w_{\text{BFE} \ell i}: i
\in \ell, z_{i}=0\}$, where for $i \in s$ 
\begin{equation*}
  w_{\text{BFE} \ell i} =
  \begin{cases}
    \hat{p}_{js\ell} \owt[] & i \in \ell, z_i =0 \text{ and } 0 < \sum_{i
  \in s\cap \ell} \owt[] (1-z_{i})< \sum_{i \in s\cap \ell} \owt[]\\
0 &  i \in \ell, \sum_{i \in s\cap \ell} \owt[] z_{i} = 0 \text{ or }
  \sum_{i \in s\cap \ell} \owt[].
  \end{cases}
\end{equation*}
The salient generalization of \eqref{eq:upsilondef} is now
\begin{align}
  \absorbInterceptsEF(\vec{v}; \mathbf{p}) &=
                                             \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}\left(%
    \left(
    \begin{array}{ccc}
      (\indicator{z_{i}=0}-p_{01\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}
      \\
      (\indicator{z_{i}=1}-p_{11\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=1}-p_{1s\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}
      \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1\ell_{1}})\indicator{b_{i}=1, m_{i}=\ell_{1}}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks\ell_{1}})\indicator{b_{i}=s, m_{i}=\ell_{1}}\\                                 (\indicator{z_{i}=0}-p_{01\ell_{2}})\indicator{b_{i}=1, m_{i}=\ell_{2}}
      &\ldots
      &
        (\indicator{z_{i}=0}-p_{0s\ell_{2}})\indicator{b_{i}=s, m_{i}=\ell_{2}}
      \\
      \vdots & \vdots & \vdots \\
      \vdots & \vdots & \vdots \\
      (\indicator{z_{i}=k}-p_{k1\ell_{g}})\indicator{b_{i}=1, m_{i}=\ell_{g}}
      &\ldots
      &
        (\indicator{z_{i}=k}-p_{ks\ell_{g}})\indicator{b_{i}=s, m_{i}=\ell_{g}}\\          
    \end{array}
  \right)%
  \right) ;\nonumber \\
  \acute{\psi}(\vec{v}; \alpha_{\ell}, \tau, \mathbf{p}, \beta) &=
\indicator{i \in \bigcup \mathcal{Q}}\owt[] \nonumber \\
&\left(
  \begin{aligned}                                               
   y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{\ell_{1}}-\tau_{z_{i}}&\\
    \vdots&\\
       y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{\ell_{g}}-\tau_{z_{i}}&\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{m_{i}}-\tau_{z_{i}}\big]
    &(
    \indicator{z_{i}=1} - \sum_{r=1}^{s}p_{1rm_{i}}\indicator{b_{i}=r})\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) - \alpha_{m_{i}}-
    \tau_{z_{i}}\big]
    &(\indicator{z_{i}=k}
    - \sum_{r=1}^{s}p_{krm_{i}}\indicator{b_{i}=r})\\
  \end{aligned}
\right), 
\end{align}
with corresponding elaborations of $A_{21}$, $\tilde{A}_{22}$,
$A_{\mathbf{p}\mathbf{p}}$, 
$A_{\alpha\tau \mathbf{p}}$ and $A_{\alpha_{yg}\mathbf{p}}$. 

That is: $A_{21}$'s rows for the supplemental $y$ regression are again
0, with the complementary lower block of rows for the $g(\cdot)$
regression's $\xi_{i}$ entries is 
\begin{equation*}
      n_{\mathcal{Q}}^{-1}\sum_{i\in \bigcup\mathcal{Q} }
  (1-\pi_{i}){p}_{b_{{i}}}\owt[{[0]}] g'(\vec{x}_{i}{\beta})
  \vec{x}_{i}
  \begin{pmatrix}
    \indicator{m_{i}=\ell_{1}}\\ \vdots \\ \indicator{m_{i}=\ell_{g}} \\
  \end{pmatrix};
\end{equation*}
$\tilde{A}_{22}$' square submatrix for $\xi$-equations and parameters
is diagonal with $j$th entry
\begin{equation*}
  -n^{-1}\sum_{i\in \bigcup\mathcal{Q} }
  (1-\pi_{i}){p}_{b_{{i}}}\owt[{[0]}]\indicator{m_{i}=j};
\end{equation*}
the column of $A_{\alpha_{yg}\mathbf{p}}$ corresponding to the $r$
block of $\mathcal{Q}$ and subgroup level $j$ is
\begin{equation*}
  n^{-1}\sum_{i\in \bigcup\mathcal{Q}: b_{i}=r }
  (1-\pi_{i}) \owt[{[0]}]
  \begin{pmatrix}
    0\\ \vdots\\ 0 \\
\indicator{m_{i}=\ell_{j}}(y_{i}-\alpha_{y\ell_{j}0})\\
  0\\ \vdots\\ 0 \\ 
\indicator{m_{i}=\ell_{j}}[g(\vec{x}_{i}\beta) -
    \alpha_{g\ell_{j}0}]\\
 0\\ \vdots\\ 0 \\ 
\end{pmatrix}.
\end{equation*}

\section{Of possible future relevance}

As of this writing, we do not plan to implement moderator effect estimation by way of moderator absorption.  Should these plans change, Section~\ref{sec:accomm-moder-vari} may be of use.
\subsection{Accommodating moderator variables via absorption}
\label{sec:accomm-moder-vari}


For a continuous moderator $m$, absorption of the moderator into the assignment variable
corresponds to solving the system $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p},{\lambda})$ for
$\lambda$, where $\mathbf{p}$ solves $0 =
\sum_{i}\absorbInterceptsEF(\vec{v}_{i}; \mathbf{p})$,
$\absorbInterceptsEF(\cdot)$ as above,
\begin{equation}
  \label{eq:18}
    \absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})=
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (
         % \indicator{z_{i}=0}-p_{0b_{i}}-\lambda_{0}
         % \tilde{m}_{i}
         % )m_{i} \\
         (
         \indicator{z_{i}=1}-p_{1b_{i}}-\lambda_{1}
         \tilde{m}_{i}
         )m_{i} \\
         \vdots\\
         (\indicator{z_{i}=k}-p_{kb_{i}}
         - \lambda_{k} \tilde{m}_{i})m_{i}\\
       \end{array}
  \right)%
\end{equation}
and $\tilde{m}$ is $m$ with block centering. Equivalently, $\lambda$
solves $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p},{\lambda})$ with
$\absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})$ given by
\begin{equation*}
    \indicator{i \in \bigcup \mathcal{Q}} \owt[]
     \left(
       \begin{array}{c}
         % (\indicator{z_{i}=0}-p_{0b_{i}}-\lambda_{0}
         % \tilde{m}_{i})\tilde{m}_{i} \\
         (\indicator{z_{i}=1}-p_{1b_{i}}-\lambda_{1}
         \tilde{m}_{i})\tilde{m}_{i} \\
         \vdots\\
         (\indicator{z_{i}=k}-p_{kb_{i}}
         - \lambda_{k} \tilde{m}_{i})\tilde{m}_{i}\\
       \end{array}
     \right).
\end{equation*}
If there were no blocks,
or no absorption of block intercepts, then $\tilde{m} = m -
\bar{m}_{w}$,  where $\bar{m}_{w}$ is $m$'s weighted mean, and $p_{jk}
\equiv p_{j}$, with $\hat{p}_{j}$ equal to a weighted mean of
$\indicator{z_{i}=j}$.

For a subgrouping variable $m$ with $t>1$ levels, and corresponding
indicator variables $m_{\ell} =\indicator{m_{i}=\ell}$, $\ell=1,
\ldots, t$, and block-centered versions $\tilde{m}_{\ell}$  moderator absorption into the assignment variable
corresponds to solving $0 = \sum
\absorbModeratorEF(\vec{v}_{i}; \mathbf{p}, {\lambda})$ for $\lambda$, where
\begin{align}
    \absorbModeratorEF(\vec{v}_{i}; \mathbf{\lambda})
&=
                                                 \indicator{i \in \bigcup \mathcal{Q}} \owt[] \times \nonumber \\
  &\operatorname{vec}  \left(%
                                                 \left(
                                                 \begin{array}{ccc}
                                                   % (\indicator{z_{i}=0}-p_{0b_{i}}-\tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=0}-p_{0s}-\tilde{m}_{ti}\lambda_{t})m_{ti}
                                                   % \\
                                                   (\indicator{z_{i}=1}-p_{1b_{i}}-\tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=1}-p_{1s}-\tilde{m}_{ti}\lambda_{t})m_{ti}
                                                   \\
                                                   \vdots & \vdots &
                                                                     \vdots
                                                   \\
                                                   (\indicator{z_{i}=k}-p_{kb_{i}}
                                                   - \tilde{m}_{1i}\lambda_{1})m_{1i}&\ldots&(\indicator{z_{i}=k}-p_{ks}-\tilde{m}_{ti}\lambda_{t})m_{ti}\\                                                 \end{array}
  \right)%
  \right) .  \label{eq:17}
\end{align}

In the model-based formulation neither of these demands any special
accounting in covariance estimation, since both $A$ and the moderating
variable are held fixed by conditioning. We get a legit covariance
without doing anything special, using an $\acute{\psi}$ given by
\begin{multline*}
  \acute{\psi}(\vec{v}; \tau, \mathbf{p}, \beta) =
    \indicator{i \in \bigcup \mathcal{Q}}\owt[] \times \\
\left(
  \begin{array}{r}
    % \big[y_{i}[z_{i}]
    % -
    % g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}\big]
    % (
    % \indicator{z_{i}=0} -
    % \sum_{r=1}^{s}p_{0r}\indicator{b_{i}=r}
    % - \tilde{m}_{i}\lambda)\\
    \big[y_{i}[z_{i}]
    -
    g(\vec{x}_{i}\beta)-\alpha_{0}-\tau_{z_{i}}\big]
    (
    \indicator{z_{i}=1}-
    \sum_{r=1}^{s}p_{1r}\indicator{b_{i}=r}
    - \tilde{m}_{i}\lambda)\\
    \vdots \\
    {}\big[y_{i}[z_{i}]
    - g(\vec{x}_{i}\beta) -\alpha_{0} -
    \tau_{z_{i}}\big]
    (
    \indicator{z_{i}=k}
    -
    \sum_{r=1}^{s}p_{kr}\indicator{b_{i}=r}
    -
    \tilde{m}_{i}\lambda)\\
  \end{array}
\right) .
\end{multline*}
Because the covariance estimates will
involve residuals that have not been decorrelated from the moderating
variable, they may err on the side of conservatism relative to
covariance estimates issuing from a fit that addressed the moderator
not by absorption but by adding it to the outcome regression
equation. We might consider alternate elaborations of the estimating
function that introduce moderator main effects to the outcome equation.

For design-based calculations, \texttt{estfun\_DB\_blockabsorb(DA)}
 should return
 \begin{equation*}
   A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' +
  \AbsorbModeratorEF{}A_{\lambda \lambda}^{-1}\hat{A}_{\tau \lambda}'.
 \end{equation*}
 In this way, the overall estimating function
 \texttt{estfun.teeMod(DA)} $-$ \texttt{estfun\_DB\_blockabsorb(DA)}
 becomes
 \begin{equation}
   \label{eq:19}
  \acute{\Psi} -
  \frac{n_{\mathcal{Q}}}{n_{\mathcal{C}}}\Phi
  \hat{A}_{11}^{-t}\hat{A}_{21}' - \AbsorbInterceptsEF{}
  A_{\mathbf{p}\,\mathbf{p}}^{-1}\hat{A}_{\tau\,\mathbf{p}}' -
  \AbsorbModeratorEF{}A_{\lambda \lambda}^{-1}\hat{A}_{\tau \lambda}'
 \end{equation}
with $A_{\lambda \lambda}$ and $A_{\tau \lambda}$ defined in the natural
way given the $\absorbModeratorEF()$ definitions stated just above.


\nobibliography{estfun_DA}
\end{document}


\subsubsection{Alt def of M's}
\[
  \begin{array}{cc}
    M_{11}  = n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}} \phi(\vec{V}_{i}; \beta )] &
                                                                  M_{12}=
                                                                  n_{\mathcal{C}}^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
             \mathcal{C}}\phi(\vec{V}_{i};
                                                                  \beta
                                                                  ), \sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi'(\vec{V}_{i}; \tau, \alpha, \beta )]\\
    M_{21}=M_{12}' & M_{22} = n^{-1}\operatorname{Cov}[\sum_{i\in \bigcup
                                                                  \mathcal{Q}}\psi(\vec{V}_{i};
                     \tau, \alpha, \beta )] .
    \end{array}
\]
With these definitions,
\[
  \operatorname{Cov}(\hat\tau) \approx n^{-1}A_{22}^{-1}\{M_{22} - [M_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}M_{21}] + (n/n_C)A_{21}A_{11}^{-1}M_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-t}
\]


We can express HC0 type estimators with matrix rather than summation notation, using $F$ for the \{0,1\}-valued $n
\times m$ matrix with an indicator column for each cluster.
\[
  \begin{array}{cccc}
  \hat{M}_{11} &=n_{\mathcal{C}}^{-1} (F'\Phi)'(F'\Phi) & \hat{M}_{12} &= n_{C}^{-1}(F'\Phi)(F'\Psi)\\
    \hat{M}_{21} &=\hat{M}_{12}' & \hat{M}_{22}&= n^{-1}(F'\Psi)(F'\Psi)\\
    \end{array}
\]




\section{Communicated in ``Josh/Xinhe Meeting Notes," Nov 30 2022
  post-mtg note}
There are random estimating functions $n_{\mathcal{C}}^{-1}\sum_{i\in\mathcal{C}}\phi(\vec{V}_{i}; \beta)$  and
       $n_{\mathcal{Q}}^{-1}\sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta)$,
       $\psi(\vec{V}_{i}; \tau, \alpha, \beta) =
       \owt[][y_{i} - \tau(z_{i} - \bar{a}) -
       g(\vec{x}_{i}\beta)](z_{i} - \bar{a})$ where $\bar{a}$ solves
       \begin{equation}
         \label{eq:4}
         0=\sum_{i \in \mathcal{Q}}w_{i \mathcal{Q}}(z_{i} - \bar{a}),
       \end{equation}
       inducing corresponding estimating equations
\[ \begin{array}{c} 0 \\ 0
   \end{array} = \left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{v}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{v}_{i}; \tau, \alpha, \beta )
     \end{array}
\right).
\]

We seek to represent $\sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta)$ as a
       linear combination of
       \begin{align}
         \label{eq:1}
         \sum_{i \in \mathcal{Q}}\psi_{0i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                             \rho_{0}](1-z_{i})\\
         \label{eq:2}
         \sum_{i \in \mathcal{Q}}\psi_{1i} =& \sum_{i \in
                                             \mathcal{Q}}\owt[][y_{i}
                                             - g(\vec{x}_{i}\beta) -
                                              \rho_{1}]z_{i}\\
         \label{eq:3}
         &\rho_{1} - \rho_{0} - \tau.
       \end{align}

       If my algebra is OK, we have
       \begin{equation*}
         \sum_{i\in
         \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta) =
       \sum_{i \in \mathcal{Q}}\psi_{0i} -  \bar{a}\sum_{i \in \mathcal{Q}}(\psi_{0i} + \psi_{1i})
     \end{equation*}
     by some algebra including \eqref{eq:4}.  I had to call time
     before seeing about whether and how to accommodate scenarios with
     strata being absorbed into the treatment variable; there are at
     least a few additional wrinkles lurking there.


M-estimation's characteristic covariance approximation is
\[ \operatorname{Cov}([\hat\beta',\hat\tau']) \approx
  A^{-1} \operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta )
     \end{array}
\right)A^{-1}
\]

The right-hand side covariance isn't quite a constant multiple of $B$ as in previous comments here (in particular with scaling factor $n_{\mathcal{Q}\mathcal{C}}^{-1}$ on off-diagonals).  Rather, with $B$ as defined there,
\[
\operatorname{Cov}\left(
     \begin{array}{c}
       n_{\mathcal{C}}^{-1}\sum_{i\in \mathcal{C}}\phi(\vec{V}_{i}; \beta )\\
       n_{\mathcal{Q}}^{-1}\sum_{i\in \mathcal{Q}}\psi(\vec{V}_{i}; \tau, \alpha, \beta )
     \end{array}
\right) = \left(
  \begin{array}{cc}
    n_{\mathcal{C}}^{-1}B_{11}& \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{12}\\
    \frac{n_{\mathcal{C}\mathcal{Q}}}{n_{\mathcal{C}}n_{\mathcal{Q}}}B_{21} &
                                                                              n_{\mathcal{Q}}^{-1}B_{22}
  \end{array}
\right).
\]

From this I get
\[
  \operatorname{Cov}(\hat\tau) \approx n_Q^{-1}A_{22}^{-1}\{B_{22} - (n_{CQ}/n_C)[B_{12}A_{11}^{-t}A_{21}^t + A_{21}A_{11}^{-1}B_{21}] + (n_Q/n_C)A_{21}A_{11}^{-1}B_{11}A_{11}^{-t}A_{21}^{t}\}A_{22}^{-1}
\]
(with $A$s and $B$s as defined in earlier comments in the thread).
